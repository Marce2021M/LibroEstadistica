[
["programación-en-r.html", "A Programación en R A.1 Algunas ventajas de R y cosas no tan padres A.2 Bienvenidx a R, Taking Off Again (sí, así se llama esta versión) A.3 Instalando cosas A.4 Instalación de RStudio A.5 Primeros pasos en R usando RStudio A.6 Cálculos numéricos A.7 Variables A.8 Observaciones sobre la aritmética de punto flotante A.9 Leer y almacenar variables en R A.10 Instalación de paquetes A.11 Comentarios adicionales sobre el formato", " A Programación en R Figure A.1: R es un programa chido de estadística. FIN. Una de las primeras cosas que necesitamos saber es que R (por más que sus más ávidos defensores digan lo contrario) no es para todo. Si tú ya conoces otro lenguaje (sea Stata, Excel, SAS, Python, Matlab, Julia, etc) sabrás utilizar muchas de sus opciones. Estoy seguro que, de conocer uno de estos, te será muchísimo más fácil seguir sacando promedios en tu lenguaje favorito que en R, realizar regresiones lineales es probablemente más sencillo en Stata mientras que las gráficas de barras para mí son más simples en Excel, Python excede en aplicaciones de inteligencia artificial mientras que Matlab es más veloz que R, Julia tiene muchas cosas de ecuaciones diferenciales que nadie más. Lo que probablemente no sea más sencillo de hacer en otro lenguaje es realizar análisis estadístico, gráficas de todo tipo y modelos de simulación. Para eso, R es, indiscutiblemente, una de las mejores opciones para quienes no conocen de programación1. Finalmente, uno de los consejos más importantes que te puedo dar es que este curso no te va a servir si no practicas. Igual que como pasa con los idiomas uno no aprende R en una semana sin practicarlo después. Mi sugerencia es que, a la vez que sigues estas notas comiences a trabajar un proyecto tuyo específico junto con el buscador de Internet de tu preferencia a la mano y empieces a usar R en él. Practica2. A.1 Algunas ventajas de R y cosas no tan padres A.1.1 Puntos a favor de R Todo el mundo lo usa. Quizá éste es el punto más a favor. Si mucha gente lo conoce y lo utiliza, hay más opciones de ayuda. Los sitios de StackOverflow en inglés y en español son excelentes para pedir apoyo en R; los grupos de usuarios de Google son otra fuente muy buena. Entre más gente usa el programa; es más fácil obtener ayuda porque seguro alguien más tuvo hace ya tiempo el mismo problema que tú. Todas las personas que trabajan en estadística publican sus métodos y su código en R (eso, claro, cuando publican sus métodos). Es raro encontrar un nuevo método estadístico en el mundo y que no se pueda usar, de alguna forma, en R. Dentro de los lenguajes de programación R es de los más sencillos. Quienes lo hicieron realmente se preocuparon por su público (de no especialistas) y en general desarrollan para él. R es gratis. Y en esta época de austeridad, cualquier ahorro es bueno. Que sea gratis no significa que no esté respaldado: existen versiones de R respaldadas por grandes compañías como Microsoft Todo lo que se hace en R es público. R no tiene métodos secretos ni es una caja negra. Todo lo que hace cada una de las funciones de R, cualquiera lo puede revisar, por completo. En R puedes hacer libros o notas ¡como este! donde guardes todo tu trabajo, reportes automatizados e incluso documentos interactivos para facilitar el análisis de datos. R puede hacer gráficas bonitas: Por supuesto, no todo es miel sobre hojuelas con R. Particularmente, algunos de los problemas con el lenguaje: Figure A.2: La curva de aprendizaje de R es más empinada pero después de un rato vale la pena La curva de aprendizaje es mucho más empinada que para otros programas estadísticos (como Stata, SAS o SPSS) ¡particularmente si es tu primera vez programando! La mayor parte de las personas que trabajan en R no son programadores de verdad. Gran parte del código que te puedes encontrar en el mundo real está escrito con prisa para salir del aprieto sin mucha planeación, con pocos comentarios, falta de control de versiones y pocas herramientas de revisión. ¡Internet está lleno de creaturas espantosas escritas en R! Figure A.3: R puede ser muy lento pero eso te da oportunidad de hacer otras cosas ;) . R de ninguna manera es veloz por lo que algunos programas (lo veremos en simulación) pueden ser extremada (y dolorosamente) lentos. A.2 Bienvenidx a R, Taking Off Again (sí, así se llama esta versión) R es un lenguaje de cómputo y un programa estadístico libre, gratuito, de programación funcional (¿qué es eso?), orientado a objetos (what??) que mutó a partir de otros dos lenguajes conocidos como Scheme y S3. El primero de estos fue desarrollado en el MIT por Sussman y Steele mientras que el segundo surgió en los laboratorios Bell4 creado por Becker, Wilks y Chambers. R nació en junio de 1995 a partir del trabajo de Ross Ihaka y Robert Gentleman5. Desde su creación, la mayor parte del desarrollo de R ha sido trabajo completamente voluntario de la Fundación R, del equipo de R Core y de miles de usuarios que han creado funciones específicas para R conocidas como paquetes (packages). Actualmente el repositorio más importante de R, CRAN, contiene más de 16000 paquetes con distintas funciones para hacer ¡lo que quieras! Como todo el trabajo en R es voluntario hace falta: Una homologación en los métodos. Puedes encontrar varias funciones que supuestamente hacen exactamente lo mismo (como es el caso de emojifont, fontemoji y emoGG para graficar usando emojis). Estandarizar la notación. Algunos paquetes como aquellos del tidyverse (veremos más adeltna) utilizan pipes (%&gt;%); estos sólo funcionan en el tidyverse pero no fuera del mismo. Sin embargo, también es una gran ventaja que sean los usuarios de R quienes guían su desarrollo. El lenguaje va mutando según peticiones de las personas que lo usan. Si hay algo que te gustaría R tuviera y aún no existe ¡lo puedes proponer! A.3 Instalando cosas A.3.1 Instalación de R Figure A.4: Oficialmente, la página de R es de las páginas más feas del mundo. ¡No te dejes llevar por las apariencias! A lo largo de estas notas estaré trabajando con: R version 4.0.2 (2020-06-22) Taking Off Again. La más reciente versión de R la puedes encontrar en CRAN. Para ello ve al sitio y selecciona tu plataforma. Nota usuarios de Mac En algunas Mac, al abir R, aparece el siguiente mensaje de advertencia: During startup - Warning messages: 1: Setting LC_CTYPE failed [...] para solucionarlo ve a Aplicaciones y abre Terminal. Copia y pega en ella el siguiente texto: defaults write org.R-project.R force.LANG en_US.UTF-8 Da enter, cierra la Terminal y reinicia R. En el caso de Windows da clic en Download R for Windows y luego en install R for the first time. Finalmente, ejecuta el instalable que aparece al dar click en Download R 4.0.2 for Windows . Para este curso pudiera ser que requirieras las herramientas de desarrollador Rtools. En el caso de Mac selecciona Download R for (Mac) OS X y luego elige R-4.0.2.pkg. En Mac puede que necesites instalar adicionalmente XQuartz (según tu versión de Mac). Si tu Mac es una versión suficientemente antigua, sigue las instrucciones específicas de CRAN. En el caso de Linux al elegir Download R for Linux tendrás la opción de buscar tu distribución específica. Al elegirla, aparecerán instrucciones para tu terminal de comandos; síguelas. En el caso de Linux, según los paquetes de R que elijamos instalar en la computadora requerirás instalar paquetería adicional para tu distribución de Linux. R te informará de la paquetería necesaria conforme la requiera. Si tienes problemas para instalar puedes usar RStudio Cloud. A.4 Instalación de RStudio Figure A.5: RStudio es una empresa que se dedica a hacer cosas para R. RStudio es una interfaz gráfica (IDE) para R. Puedes pensar a R como el Bloc de Notas y a RStudio como Word. El Bloc tiene todas las capacidades que necesitas para poder escribir; empero, es muchísimo mejor trabajar tus papers en Word. De la misma manera, R tiene todas las capacidades para hacer estadística pero un formato horrible y RStudio se ha convertido en la más popular forma de usar R. Por supuesto que no es la única; algunas alternativas son Atom con ide-r, Eclipse con StatET y RKWard. En general es posible seguir estas notas sin que tengas RStudio pero, si es tu primera vez programando, no lo recomiendo. Si ya tienes experiencia con lenguajes como Python, Javascript, Java ó alguno de los mil C que existen, no tendrás ningún problema usando el editor de tu preferencia. Para descargar RStudio ve a su página y da clic en Download RStudio. Baja tu pantalla hasta donde dice Installers for Supported Platforms y elige tu plataforma: Windows, Mac OS X ó tu sabor de Linux preferido. Una vez descargado el archivo, ábrelo y sigue las instrucciones que aparecen en pantalla. A.5 Primeros pasos en R usando RStudio Una vez hayas instalado R y RStudio, abre RStudio6. Te enfrentarás a una pantalla similar a esta: Figure A.6: La primera vez que abres RStudio Si tu RStudio tiene sólo 3 páneles, como en mi caso, ve a la esquina superior izquierda (signo de hoja+) y elige un nuevo R Script Figure A.7: Elige hoja+ para crear un nuevo archivo Tendrás, entonces, 4 páneles como se ve a continuación: Figure A.8: RStudio &lt;3 El primer panel (esquina inferior izquierda) es la Consola. Aquí es donde se ejecutan las acciones. Prueba escribir 2 + 3 en él y presiona enter. Aparece el resultado de la suma. Definitivamente, R es la calculadora que más trabajo cuesta instalar. Figure A.9: La consola de R es la calculadora más difícil de instalar que existe. El segundo panel (esquina superior izquierda) es el panel con el Script. Aquí se escribe el programa pero no se ejecuta. Prueba escribir 10 + 9. ¿Ves que no pasa nada? Lo que acabas de hacer es crear un programa que, cuando se ejecute, hará la suma de 10 + 9. ¡Qué programa más aburrido! Sin embargo, no todo está perdido: presiona CTRL+Enter (Cmd+Enter en Mac) al final de la línea o bien da clic en Run y verás que, en la consola, aparece la instrucción y el resultado de la misma. El Script es una excelente fuente para tener un historial de lo que estás haciendo. Figure A.10: El Script sirve para salvar las instrucciones en el orden en que las vas a ejecutar. El tercer panel contiene el ambiente. Aquí aparecerán las variables que vayamos creando. Por ahora, para poner un ejemplo, importaremos el archivo Example1.csv (con valores simulados) disponible en Github dando clic en Import Dataset y From Text (base). Selecciona el archivo y elige las opciones en la ventana de previsualización que hagan que se vea bien. Nota que una vez realizada la importación aparece en el panel derecho Example1. Al dar clic podrás ver la base de datos. Las bases de datos y variables que utilices durante tus análisis aparecerán en esa sección. Figure A.11: El Ambiente muestra las variables (incluyendo bases de datos) que estás utilizando en este momento. A diferencia de otros programas estadísticos (o sea Stata) en R es posible tener múltiples bases de datos abiertas a la vez. Para entender mejor lo que ocurre en el último de los páneles, lo mejor es trabajar con nuestra base. Escribe en la consola plot(Example1) . En el cuarto pánel aparecerá una gráfica. El cuarto de los páneles para nosotros tendrá esa utilidad: mostrará las gráficas que hagamos así como la ayuda. Para ver la ayuda para las instrucciones de R puedes escribir ?. Prueba teclear ?plot en la consola. El signo de interrogación es un help() que muestra las instrucciones para usar una función. Figure A.12: La gráfica que aparece de hacer un plot de la base de datos de ejemplo. Figure A.13: El cuarto panel muestra respectivamente las gráficas y la ayuda. Mi sugerencia personal es que escribas todo lo que haces en el Script y que sólo utilices la consola para verificar valores. De esta manera podrás almacenar todas las instrucciones ejecutadas y volver a ellas cuando se requieran. Por último te sugiero utilizar # gatos para comentar tu código. Así, el código anterior lo podrías ver en la consola como: Comenta. Comenta. Comenta, por favor. Tu ser del futuro que regrese a sus archivos de R un mes después de haberlos hecho te lo agradecerá (y tu profe también). Finalmente y como aclaración para estas notas, el código de R aparece como: Mientras que los resultados de evaluar en R se ven con #: [1] 5 Así, la evaluación con su resultado se ve de la siguiente forma: [1] 5 A.6 Cálculos numéricos R sirve como calculadora para las operaciones usuales. En él puedes hacer sumas, [1] 43 Figure A.14: Ada Lovelace (1815-1852), la primera en diseñar un algoritmo computacional ¡y sin tener computadoras! restas, [1] -1 multiplicaciones, [1] 56 divisiones, [1] 2 sacar logaritmos naturales \\(\\ln\\), [1] 4.60517 o bien logaritmos en cualquier base,7 [1] 2 también puedes elevar a una potencia (por ejemplo hacer \\(6^3\\)), [1] 216 calcular la exponencial \\(e\\), [1] 2.718282 o bien exponenciar cualquier variable \\(e^{-3}\\), [1] 0.04978707 también puedes usar el número \\(\\pi\\). [1] 3.141593 No olvides que R usa el orden de las operaciones de matemáticas. Siempre es de izquierda a derecha con las siguientes excepciones: Primero se evalúa lo que está entre paréntesis. En segundo lugar se calculan potencias. Lo tercero en evaluarse son multiplicaciones y divisiones. Finalmente, se realizan sumas y restas. Por ejemplo, en la siguiente ecuación \\[ 2 - 2 \\cdot \\frac{(3^4 - 9)}{(5 + 4)} \\] se resuelven primero los paréntesis \\((3^4 - 9) = 81 - 9 = 72\\) y \\((5 + 4) = 9\\); luego se resuelve la división: \\(\\frac{72}{9}=8\\), se multiplica por el \\(2\\): \\(2 \\cdot 8 = 16\\) y finalmente se hace la resta: \\(2-16 = -14\\). A.6.1 Ejercicio Determina, sin evaluar, los resultados de los siguientes segmentos de código: Evalúa para comprobar tu respuesta. A.6.2 Ejercicio Calcula el área y el perímetro de un círculo de radio 5. Recuerda que la fórmula del área es \\(\\pi \\cdot r^2\\) donde \\(r\\) es el radio; mientras que la del perímetro es: \\(\\pi \\cdot d\\) donde \\(d\\) es el díametro (= dos veces el radio). A.6.3 Respuestas Área = 78.5398163397448 Perímetro = 31.4159265358979 A.7 Variables R es un programa orientado a objetos; esto quiere decir que R almacena la información en un conjunto de variables que pueden tener diferentes clases y opera con ellos según su clase. Por ejemplo, un conjunto de caracteres, entre comillas, es un Character (R lo piensa como texto) [1] “Hola” Un número (por ejemplo 2 tiene clase numeric)8. Hay que tener mucho cuidado con combinar floats con Strings: [1] 6 Figure A.15: El algoritmo diseñado por Ada Lovelace. ## Error in 2 + &quot;4&quot;: non-numeric argument to binary operator Si lo piensas, este último error ¡tiene todo el sentido! no puedes sumar un número a un texto. ¿O qué significaría 'Felices' * 4 ? La magia de R comienza con que puedes almacenar valores en variables. Por ejemplo, podemos asignar un valor a una variable: Hay dos formas de asignar valores, una es con la flecha de asignación \\(\\leftarrow\\) y otra con el signo de igual: Nota que, cuando realizamos operaciones, la asignación es la última que se realiza: Los valores que fueron asignados en las variables, R los recuerda y es posible calcular con ellos: [1] 16 [1] 8 Podemos preguntarnos por el valor de las variables numéricas mediante los operadores == (sí, son dos iguales), != (que es un \\(\\neq\\)) &gt;, &gt;=, &lt;= y &lt;: [1] FALSE El operador de asignación también se puede utilizar al revés \\(2 \\rightarrow x\\) pero no lo hagas, por favor. Nota que no estamos asignando el valor de x: [1] 10 Podemos preguntarnos por diferencia: [1] TRUE Así como por mayores, menores incluyendo posibles igualdades (i.e. los casos \\(\\geq\\) y \\(\\leq\\)) [1] TRUE [1] TRUE [1] FALSE [1] TRUE En todos los casos los resultados han sido TRUE ó FALSE. La clase de variables que toma valores TRUE ó FALSE se conoce como booleana. Hay que tener mucho cuidado con ellas porque, puedes acabar con resultados muy extraños: [1] 101 [1] 0 Aquí puedes encontrar una lista de malas prácticas en computación a evitar. Finalmente, nota que es posible reescribir una variable y cambiar su valor: [1] 10 [1] 0.5 A.7.1 Ejercicios Determina el valor que imprime R en cada caso, sin que corras los siguientes pedazos de código. Después, verifica tu respuesta con R: A.7.2 NIVEL 3 Determina, sin correr el programa, qué regresa la consola en este caso Comprueba con la consola tus resultados; puede que encuentres respuestas poco intuitivas. A.8 Observaciones sobre la aritmética de punto flotante Si hiciste el penúltimo ejercicio (el cual, obviamente hiciste y comprobaste con la consola) podrás haber notado una trampa. Analicemos qué ocurre; quizá hicimos mal la suma [1] 0.3 [1] 0.3 Aparentemente no hay nada malo ¿qué rayos le pasa a R? La respuesta está en la aritmética de punto flotante. Podemos pedirle a R que nos muestre los primeros 100 dígitos de la suma 0.1 + 0.1 + 0.1: Figure A.16: Réplica de la Z3, la primer computadora con punto flotante (1941). [1] 0.3000000000000000444089 El comando options(digits = 22) especifica que R debe imprimir en la consola 22 dígitos. No más. ¡Ahí está el detalle! R no sabe sumar. En general, ningún programa de computadora sabe hacerlo. Veamos otros ejemplos: [1] 3.999999999999999555911 [1] 0.2999999999999999888978 [1] Inf El problema está en cómo las computadoras representan los números. Ellas escriben los números en binario. Por ejemplo, 230 lo representan como 11100110 mientras que el 7 es: 111. El problema de las computadoras radica en que éstas tienen una memoria finita por lo que números muy grandes como: \\(124765731467098372654176\\) la computadora hace lo mejor por representarlos eligiendo el más cercano: [1] 124765731467098377420800 Un error de punto flotante en la vida real ocasionó en los años noventa, la explosión del cohete Ariane 5. Moraleja: hay que tener cuidado y respeto al punto flotante. No olvides cambiar la cantidad de dígitos que deseas que imprima R en su consola de vuelta: El mismo problema ocurre con números decimales cuya representación binaria es periódica; por ejemplo el \\(\\frac{1}{10}\\) en binario se representa como \\(0.0001100110011\\overline{0011}\\dots\\). Como es el cuento de nunca acabar con dicho número, R lo trunca y almacena sólo los primeros dígitos de ahí que, cada vez que escribes 0.1, R en realidad almacene el 0.1000000000000000055511 que es casi lo mismo pero no es estrictamente igual. Hay que tener mucho cuidado con esta inexactitud de las computadoras (inexactitud estudiada por la rama de Análisis Numérico) pues puede generar varios resultados imprevistos. A.8.1 ¿Cómo checar un if? En general lo que hacen las computadoras para comparar valores es que verifican que, en valor absoluto, el error sea pequeño. Recuerda que el valor absoluto de \\(x\\), \\(|x|\\), regresa siempre el positivo: \\[ |4| = 4 \\qquad \\textrm{y} \\qquad |-8| = 8 \\] Para verificar que algo es más o menos \\(0.3\\) suele usarse el valor absoluto9 de la siguiente manera: [1] TRUE donde 1.e-6 es notación corta para 0.000001 (también escrito como \\(1\\times 10^{-6}\\)). La pregunta que nos estamos haciendo es que si el error entre sumar \\(0.1+0.1+0.1\\) y \\(0.3\\) es muy pequeño \\(&lt; 0.000001\\): \\[ | (0.1 + 0.1 + 0.1) - 0.3 | &lt; 0.000001 \\] A.9 Leer y almacenar variables en R Para terminar esta sección, aprenderemos cómo guardar variables en R. Para eso, el concepto de directorio es uno de los más relevantes. En general, en computación, el directorio se refiere a la dirección en tu computadora donde estás trabajando. Por ejemplo, si estás en una carpeta en tu escritorio de nombre “Ejercicios_R” probablemente tu directorio sea ‘~/Desktop/Ejercicios_R/’ (en Mac) o bien ‘~\\Desktop\\Ejercicios_R\\’ en Windows10. La forma de saber tu directorio (en general) es ir a la carpeta que te interesa y con clic derecho ver propiedades (o escribir ls en la terminal Unix). R tiene un directorio default que quién sabe dónde está (depende de tu instalación, generalmente está donde tu Usuario). Usualmente lo mejor es elegir un directorio para cada uno de los proyectos que hagas. Para ello si estás en RStudio puedes utilizar Shift+Ctrl+H (Shift+Cmd+H en Mac) o bien ir a Session &gt; Set Working Directory &gt; Choose Directory y elegir el directorio donde deseas trabajar tu proyecto. Pensando que elegiste el escritorio (Desktop en mi computadora) notarás que en la consola aparece el comando setwd(\"~/Desktop\") (o bien con ‘\\’ si eres Windows). Mi sugerencia es que copies ese comando en tu Script para que, la próxima vez que lo corras ya tengas preestablecido el directorio. Podemos verificar el directorio elegido con getwd(): En general es buena práctica en R establecer, hasta arriba del Script, el comando de directorio. Esto con el propósito de que, cuando compartas un archivo, la persona a quien le fue compartido el archivo pueda rápidamente elegir su propio directorio en su computadora. Probemos guardar unas variables en un archivo dentro de nuestro directorio. Para ello utilizaremos el comando save. Si vas a tu directorio, notarás que el archivo MisVariables.rda acaba de ser creado. De esta forma R puede almacenar objetos creados en R que sólo R puede leer (más adelante veremos cómo exportar bases de datos y gráficas). Observa que en tu ambiente (si estás en RStudio puedes verlas en el panel 3) deben aparecer las variables que hemos usado hasta ahora: [1] “x” “y” “z” “Example1” Podemos probar sumar nuestras variables y todo funciona súper: [1] 300 Limpiemos el ambiente. El comando equivalente al clear all en R es un poco más complicado de memorizar: Ahora, si vuelves a ver el ambiente, éste estará vacío: ¡hemos limpiado el historial! Nota que si intentamos operar con las variables, R ya no las recuerda: ## Error in eval(expr, envir, enclos): object &#39;x&#39; not found Así como hay que lavarse las manos antes de comer, es buen hábito limpiar todas las variables del ambiente de R antes de usarlo. Podemos leer la base de datos usando load: [1] 300 Por último, es necesario resaltar la importancia del directorio. Para ello crea una nueva carpeta en tu escritorio de nombre Mi_curso_de_R. Mueve el archivo \"MisVariables.rda\" dentro de la carpeta. Borra todo e intenta leer de nuevo el archivo: ## Warning in readChar(con, 5L, useBytes = TRUE): cannot open compressed file ## &#39;MisVariables.rda&#39;, probable reason &#39;No such file or directory&#39; ## Error in readChar(con, 5L, useBytes = TRUE): cannot open the connection Este error es porque R sigue pensando que nuestro directorio es el escritorio y está buscando el archivo ahí sin hallarlo. Para encontrarlo hay que cambiar el directorio a través de RStudio (ya sea Ctrl+Shift+H o Session &gt;Set Working Directory &gt; Choose Directory) o bien a través de comandos en R: A.9.1 Ejercicio Responde a las siguientes preguntas: ¿Qué es el directorio y por qué es necesario establecerlo? Si R me da el error 'No such file or directory' ¿qué hice mal? En RStudio, ¿qué hace Session &gt; Restart R? ¿cuál es la diferencia con rm(list = ls())? ¿Qué hace el comando cat(\"\\014\")? (Ojo puede que no haga nada). Si funciona, ¿cuál es la diferencia con rm(list = ls()) y con Restart R? A.10 Instalación de paquetes Un paquete de R es un conjunto de funciones adicionales elaboradas por los usuarios, las cuales permiten hacer cosas adicionales en R. Para instalarlos requieres de una conexión a Internet (o bien puedes instalarlos a partir de un archivo, por ejemplo, mediante una USB). El comando de instalación es install.packages seguido del nombre del paquete. Por ejemplo (y por ocio) descarguemos el paquete beepr para hacer reproducir sonidos en la computadora11. Para ello: [...] * DONE (beepr) The downloaded source packages are in ‘/algun/lugar/downloaded_packages’ Esto significa que el paquete ha sido instalado. Nos interesa usar la función beep que emite un sonido (??beep para ver la ayuda). Si la llamamos así tal cual, nos da error: R es incapaz de hallar la función porque aún no le hemos dicho dónde se encuentra. Para ello podemos llamar al paquete mediante la función library y decirle a R que incluya las funciones que se encuentran dentro de beepr: El comando library le dice a R ¡hey, voy a usar unas funciones que creó alguien más y que están dentro del paquete beepr! De esta manera, al correr beep(3), R ya sabe dónde hallar la función y por eso no arroja error. A.10.1 Ejercicios NIVEL 1 Instala los paquetes tidyverse en R. De tidyverse haz lo necesario para que el siguiente bloque de código te arroje una gráfica: NIVEL 3 Instala el paquete devtools (para hacerlo probablemente necesites instalar más cosas en tu computadora; averigua cuáles) Usa devtools para instalar el paquete emoGG desde Github. Verifica que tu instalación fue correcta haciendo la siguiente gráfica: A.11 Comentarios adicionales sobre el formato Así como en el español existen reglas de gramática para ponernos todos de acuerdo y entendernos entre todos, en R también existen sugerencias a seguir para escribir tu código. Las sugerencias que aquí aparecen fueron adaptadas de las que utiliza el equipo de Google. No escribas líneas de más de 80 caracteres (si se salió de tu pantalla, mejor continúa en el siguiente renglón). Coloca espacios entre operadores +,*,/,-,&lt;-,=, &lt;, &lt;=, &gt;, &gt;=, == y usa paréntesis para agrupar: Intenta alinear la asignación de variables para legibilidad: Utiliza nombres que evoquen la variable que representas No utilices un nombre demasiado similar para cosas diferentes. Comenta: Figure A.17: Trad: Un periodista se acerca a un programador a preguntarle ¿qué hace que un código sea malo? -Sin comentarios. Siempre pon las llamadas a los paquetes y el directorio al inicio de tu archivo para que otro usuario sepa qué necesita. Código limpio y legible: es siempre preferible a código escrito con prisas : Figure A.18: Yo, leyendo mi código no comentado y con mala edición 6 meses después de haberlo hecho. Siempre escribe tu código pensando que alguien más (y ese alguien más puedes ser tú) va a leerlo. ¡No olvides comentar! Modelos de simulación más avanzados suelen hacerse en C, C++ o Fortran por su velocidad; empero, es necesario conocer más de programación.↩︎ La práctica hace al maestro↩︎ De ahí que se llame R porque la R es una mejor letra que la S (todos lo sabemos) -Atte. Rodrigo, el autor de este documento.↩︎ Mejor conocidos ahora como AT&amp;T, la compañía celular que nunca tiene señal.↩︎ Sus nombres empiezan con la letra R ¿coincidencia?↩︎ Si decidiste no instalar RStudio salta al final de esta sección.↩︎ Recuerda que un logaritmo base \\(a\\) te dice a qué potencia \\(b\\) tuve que elevar \\(a\\) para llegar a \\(b\\). Por ejemplo \\(\\log_{10}(100) = 2\\) te dice que para llegar al \\(100\\) tuviste que hacer \\(10^2\\).↩︎ Puede ser float, int, double pero no nos preocuparemos por eso.↩︎ En R el comando abs toma el valor absoluto.↩︎ Windows usa backslash. Y hay toda una historia detrás de ello↩︎ En los siguientes capítulos descargaremos paquetes más interesantes; pero no desprecies la utilidad de beepr yo lo he usado en múltiples ocasiones para que la computadora me avise que ya terminó de correr un código.↩︎ "],
["repaso-de-proba.html", "B Repaso de Proba B.1 Funciones indicadoras B.2 Conteo B.3 Espacios de probabilidad B.4 Probabilidad condicional B.5 Independencia B.6 Variables aleatorias y función de distribución (acumulada) B.7 Funciones de masa de probabilidad B.8 Funciones de densidad B.9 Teorema de cambio de variable unidimensional B.10 Probabilidad Multivariada B.11 Esperanza, varianza y covarianza B.12 Condicionamiento por otra variable aleatoria B.13 Funciones características B.14 Convergencias B.15 Ley de los grandes números B.16 Teorema del límite central", " B Repaso de Proba B.1 Funciones indicadoras Dado un conjunto \\(A\\) definimos la función indicadora de \\(A\\) como sigue: \\[ \\mathbb{I}_A (x)= \\begin{cases} 1 &amp; \\text{ si } x \\in A \\\\ 0 &amp; \\text{ si } x \\not\\in A \\end{cases} \\] La función indicadora cumple las siguientes propiedades: Sean \\(A,B\\) conjuntos; luego: \\(\\mathbb{I}_{A \\cap B}(x) = \\mathbb{I}_{A}(x) \\cdot \\mathbb{I}_{B}(x)\\) \\(\\mathbb{I}_{A \\cup B}(x) = \\mathbb{I}_{A}(x) + \\mathbb{I}_{B}(x) - \\mathbb{I}_{A}(x) \\cdot \\mathbb{I}_{B}(x)\\) \\(\\mathbb{E}_X[\\mathbb{I}_A(X)] = \\mathbb{P}(X\\in A)\\) Demostración: 1. Si \\(x\\in A \\cap B\\) pasa que \\(\\mathbb{I}_{A \\cap B}(x) = 1\\); además, por hipótesis \\(x\\in A\\) y \\(x \\in B\\) lo que implica que \\(\\mathbb{I}_{A}(x) = 1\\) y \\(\\mathbb{I}_{B}(x) = 1\\); en caso contrario \\(\\mathbb{I}_{A \\cap B}(x) = 1\\) y como no está en el conjunto al menos uno \\(\\mathbb{I}_{A}(x)\\) ó \\(\\mathbb{I}_{B}(x)\\) es cero. Esto concluye la prueba. 2. Demostración es similar 3. Para cualquier variable aleatoria \\(X\\), \\(\\mathbb{I}_{A}(X)\\) sólo toma dos valores: \\(0\\) si \\(X\\not\\in A\\) y \\(1\\) si \\(X\\in A\\). Luego: \\[ \\mathbb{E}_X[\\mathbb{I}_A(X)] = 1 \\cdot \\mathbb{P}(X\\in A) + 0 \\cdot \\mathbb{P}(X\\not\\in A) = \\mathbb{P}(X\\in A) \\] B.2 Conteo Intentemos resumir todas las formas de contar que tenemos con un ejemplo de Casella and Berger (2002). En la lotería de Nueva York se eligen \\(6\\) de \\(44\\) números para un ticket. ¿Cuántos boletos de lotería posibles hay? Veamos algunas formas posibles de solución12: Ordenado y sin reemplazo Si sólo importa el orden y una vez que sale un número no se vuelve a meter a los posibles entonces tenemos: \\[ \\frac{44!}{(44-6)!} \\] Ordenado y con reemplazo En cada uno de los \\(6\\) lugares hay \\(44\\) números posibles: \\[ 44^6 \\] Sin orden y sin reemplazo Esto es una combinación por lo que la forma de extraerlo es: \\[ \\binom{44}{6} \\] Sin orden y con reemplazo Para resolver este caso podemos usar la técnica de las barras y los puntos. Coloquemos barras y los huecos entre ellas representan cada uno de los \\(44\\) números. \\[\\begin{equation}\\nonumber |\\underbrace{\\_}_{1}|\\underbrace{\\_}_{2}|\\underbrace{\\_}_{3}|\\cdots |\\underbrace{\\_}_{n}| \\end{equation}\\] Coloquemos puntos (\\(\\circ\\)) donde estén los números seleccionados. Por ejemplo la siguiente representa la combinación \\(113555\\) \\[\\begin{equation}\\nonumber |\\underbrace{\\circ \\circ}_{1}|\\underbrace{\\_}_{2}|\\underbrace{\\circ}_{3}||\\underbrace{\\_}_{4}|\\underbrace{\\circ \\circ \\circ}_{5}|\\cdots |\\underbrace{\\_}_{n}| \\end{equation}\\] Tenemos entonces que el problema se reduce a colocar \\(n - 1= 43\\) barritas (son un total de \\(45\\) pero la primera y la última no deben cambiar de lugar) y \\(k = 6\\) círculos por tanto colocamos \\(49\\) elementos en total. De estos, nos interesa poner \\(6\\) por lo que tenemos: \\[ \\binom{44 + 6 - 1}{6} \\] formas distintas. Esto nos lleva a la tabla siguiente: Para obtener una muestra de tamaño \\(k\\) a partir de un conjunto de tamaño \\(n &gt; 0\\) éstas son las opciones: \\(\\quad \\text{Con Reemplazo}\\) \\(\\quad \\text{Sin Reemplazo}\\) \\(\\quad \\text{Con Orden}\\) \\(\\quad n^k\\) \\(\\quad (n)_k\\) \\(\\quad \\text{Sin Orden}\\) \\(\\quad \\binom{n+k-1}{k}\\) \\(\\quad \\binom{n}{k}\\) B.3 Espacios de probabilidad Los ingredientes para un modelo probabilístico son \\(3\\): Un conjunto \\(\\Omega\\) conocido como espacio muestral el cual es el conjunto de los resultados de interés. Por ejemplo, en el tiro de un dado \\(\\Omega = \\{1,2,3,4,5,6\\}\\), para el lanzamiento de una moneda \\(\\Omega = \\{\\text{Águila},\\text{Sol}\\}\\) o bien en seleccionar un número uniforme entre \\(0\\) y \\(1\\) tenemos que \\(\\Omega = [0,1]\\). Una colección \\(\\mathcal{F}\\) de subconjuntos de \\(\\Omega\\) conocida como sigma-álgebra o bien como espacio de eventos la cual cumple las siguientes características: \\(\\Omega \\in \\mathcal{F}\\) Si \\(A\\in\\mathcal{F}\\) entonces \\(A^C \\in \\mathcal{F}\\) Si \\(A_1, A_2, \\dots\\) es una colección finita ó numerable de elementos de \\(\\mathcal{F}\\) entonces \\(\\bigcup_{n} A_n \\in \\mathcal{F}\\) Generalmente identificamos a la \\(\\mathcal{F}\\) con el potencia para conjuntos \\(\\Omega\\) finitos; para casos infinitos el teorema de Vitali nos dice que las cosas son más complicadas. Una función \\(\\mathbb{P}:\\mathcal{F} \\to [0,1]\\) que cumple que: \\(\\mathbb{P}(\\Omega) = 1\\). \\(\\mathbb{P}(A) \\geq 0\\) para todo \\(A \\in \\mathcal{F}\\). Si \\(A_1, A_2, \\dots\\) es una colección finita ó numerable de conjuntos disjuntos (\\(A_i\\cap A_j = \\emptyset\\) para \\(i \\neq j\\)) entonces \\(\\mathbb{P}(\\bigcup_{n} A_n) = \\sum\\limits_{n}\\mathbb{P}(A_n)\\). Estos últimos tres puntos se conocen como Axiomas de Kolmogorov. Una vez armados con los axiomas podíamos empezar a probar cosas con ellos; por ejemplo: Sea \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) un espacio de probabilidad. Sea \\(A\\) evento de \\(\\mathcal{F}\\). Luego: \\[ \\mathbb{P}(A^C) = 1 - \\mathbb{P}(A). \\] Para verlo, podemos escribir \\(\\Omega = A\\cup A^C\\) de donde se sigue que: \\[ 1 = \\mathbb{P}(\\Omega) = \\mathbb{P}(A \\cup A^C) = \\mathbb{P}(A) + \\mathbb{P}(A^C); \\] si despejamos obtenemos el resultado deseado. También podemos probar, por ejemplo: \\[ \\mathbb{P}(A\\setminus B) = \\mathbb{P}(A) - \\mathbb{P}(A \\cap B) \\] si escribimos \\(A = (A\\setminus B) \\cup (A \\cap B)\\) de donde se sigue que: \\[ \\mathbb{P}(A) = \\mathbb{P}\\big((A\\setminus B) \\cup (A \\cap B) \\big) = \\mathbb{P}(A\\setminus B) + \\mathbb{P} (A \\cap B) \\] y despejamos para tener el resultado deseado. Una última cosa de importancia es tomar \\(A,B\\) eventos de \\(\\mathcal{F}\\). Luego: \\[ \\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B) \\] Para verlo, escribimos \\(A\\cup B\\) como \\(A\\cup B = (A\\setminus B)\\cup (A \\cap B)\\cup (B\\setminus A)\\) luego: \\[\\begin{equation}\\nonumber \\begin{aligned} \\mathbb{P}(A\\cup B) &amp; = \\mathbb{P}(A\\setminus B) + \\mathbb{P} (A \\cap B) + \\mathbb{P}(B\\setminus A) \\\\ &amp; = \\mathbb{P}(A) - \\mathbb{P}(A \\cap B) + \\mathbb{P} (A \\cap B) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B) \\\\ &amp; = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B) \\end{aligned} \\end{equation}\\] B.4 Probabilidad condicional Muchas veces la probabilidad cambia conforme obtenemos información extra. Por ejemplo, si consideramos los tiros de un dado \\(\\Omega = \\{1,2,3,4,5,6\\}\\) y se sabe que cayó par \\(B = \\{2,4,6 \\}\\), la probabilidad de obtener \\(2\\) ó \\(4\\) (el evento) \\(A = \\{ 2, 4\\}\\) cambia de probabilidad: \\[ \\mathbb{P}(A | B) = \\dfrac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)} \\] En particular hay dos teoremas principales con probabilidad condicional: la ley de probabilidad total que te permite reconstruirlas probabilidades originales a partir de las condicionales y el de Bayes. El teorema de Bayes puede deducirse a partir de un simple despeje pues notamos que: \\[ \\mathbb{P}(A | B) = \\dfrac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)} \\] y por otro lado: \\[ \\mathbb{P}(B | A) = \\dfrac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(A)} \\] Si despejamos del segundo, obtenemos: \\[ \\mathbb{P}(B | A)\\mathbb{P}(A) = \\mathbb{P}(A \\cap B) \\] Podemos sustituir la definición de intersección en \\(\\mathbb{P}(A|B)\\) y así obtener: \\[ \\mathbb{P}(A | B) = \\dfrac{\\mathbb{P}(B | A)\\mathbb{P}(A)}{\\mathbb{P}(B)} \\] Por otro lado, dada una partición \\(B_1, B_2, \\dots\\) finita o numerable de \\(\\Omega\\) podemos definir la probabilidad de \\(A\\) en términos de cada uno de los pedazos: \\[ \\mathbb{P}(A) = \\sum\\limits_{k} \\mathbb{P}(A | B_k) \\cdot \\mathbb{P}(B_k) \\] Esta identidad se sigue de que: \\[ \\mathbb{P}(A | B_k) = \\dfrac{\\mathbb{P}(A \\cap B_k)}{\\mathbb{P}(B_k)} \\] de donde podemos sustituir arriba y obtener: \\[ \\mathbb{P}(A) = \\sum\\limits_{k} \\dfrac{\\mathbb{P}(A \\cap B_k)}{\\mathbb{P}(B_k)} \\cdot \\mathbb{P}(B_k) = \\sum\\limits_{k} \\mathbb{P}(A \\cap B_k) = \\mathbb{P}\\big(A \\cap (\\bigcup_k B_k) \\big) = \\mathbb{P}\\big(A \\cap \\Omega \\big) \\] Tenemos entonces el teorema siguiente: Sean $B_1, B_2, $ eventos que forman una partición de \\(\\Omega\\); sea \\(A\\) un evento cualquiera; luego: \\[ \\mathbb{P}(A) = \\sum\\limits_{k} \\mathbb{P}(A | B_k) \\cdot \\mathbb{P}(B_k) \\] Usando probabilidad condicional podemos resolver problemas como el siguiente: Considera el conjunto \\(C = \\{1,2,\\dots, n\\}\\) para \\(n \\geq 2\\). Se extraen dos números \\(a\\) y \\(b\\) (primero el \\(a\\) y luego el \\(b\\)) con probabilidad uniforme sin reemplazo. Determina la probabilidad de que \\(a &gt; b\\). Podemos utilizar probabilidad condicional para representar el evento: \\[\\begin{equation}\\nonumber \\begin{aligned} \\mathbb{P}(a &gt; b) &amp; = \\sum\\limits_{k = 1}^{n} \\mathbb{P}(a &gt; b \\quad | \\quad a = k) \\mathbb{P}(a = k) \\end{aligned} \\end{equation}\\] Donde \\(\\mathbb{P}(a = k) = \\frac{1}{n}\\) para todos los \\(k\\) pues es uniforme (y es el primero en salir). Luego: \\[\\begin{equation}\\nonumber \\begin{aligned} \\mathbb{P}(a &gt; b) &amp; = \\sum\\limits_{k = 1}^{n} \\mathbb{P}(a &gt; b \\quad | \\quad a = k) \\mathbb{P}(a = k) \\\\ &amp; = \\dfrac{1}{n} \\sum\\limits_{k = 1}^{n} \\mathbb{P}(a &gt; b \\quad | \\quad a = k) \\\\ &amp; = \\dfrac{1}{n} \\sum\\limits_{k = 1}^{n} \\mathbb{P}(k &gt; b \\quad | \\quad a = k) \\\\ \\end{aligned} \\end{equation}\\] Notamos que cuando \\(k = 1\\) no hay forma de que \\(k &gt; b\\); cuando \\(k = 2\\) hay una única forma (que \\(b\\) valga \\(1\\)); cuando \\(k = 3\\) hay dos formas. En general para una \\(k\\) genérica hay \\(k-1\\) formas de seleccionar un \\(b\\) menor a \\(k\\) luego: \\[\\begin{equation}\\nonumber \\begin{aligned} \\mathbb{P}(a &gt; b) &amp; = \\dfrac{1}{n} \\sum\\limits_{k = 1}^{n} \\dfrac{k-1}{n} \\\\ &amp; = \\dfrac{1}{n^2} \\sum\\limits_{k = 1}^{n} k-1 \\\\ &amp; = \\dfrac{1}{n^2} \\sum\\limits_{k = 0}^{n-1} k \\\\ &amp; = \\dfrac{1}{n^2} \\dfrac{n(n-1)}{2} \\\\ &amp; = \\dfrac{n-1}{2n} \\end{aligned} \\end{equation}\\] B.5 Independencia Dos eventos \\(A,B\\) son independientes si: \\[ \\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\mathbb{P}(B) \\] Intuitivamente esto significa que saber \\(A\\) no me dice nada de \\(B\\) pues la independencia implica que: \\[ \\mathbb{P}(A | B) = \\mathbb{P}(A) \\] B.6 Variables aleatorias y función de distribución (acumulada) Para hablar de probabilidad uno de los ingredientes principales eran las variables aleatorias. Éstas son funciones (no son variables ni son aleatorias) de tal manera que su imagen inversa pertenece a la sigma-álgebra \\(\\mathcal{F}\\): Una función \\(X: \\Omega \\to \\mathbb{R}\\) es una variable aleatoria si: \\[ X^{-1}(A) = \\{ \\omega \\in \\Omega \\quad : \\quad X(\\omega) \\in A \\} \\in \\mathcal{F} \\] para todo \\(A\\subseteq\\textrm{Dom}_X\\) En general la pregunta \\(\\mathbb{P}(X \\in A)\\) la traducíamos a una pregunta sobre conjuntos: \\[ \\mathbb{P}(X \\in A) = \\mathbb{P}\\Big( \\{ \\omega \\in \\Omega \\quad : \\quad X(\\omega) \\in A \\} \\Big) \\] y esto nos permitía hablar de probabilidades. En particular, construíamos la función de distribución acumulada como sigue: Definimos la función de distribución acumulada de una variable aleatoria \\(X: \\Omega \\to \\mathbb{R}\\) como: \\[ F_X(x) = \\mathbb{P}(X \\leq x) \\] donde \\(X\\) es la variable aleatoria y \\(x\\in\\mathbb{R}\\) es un real. La función de distribución acumulada cumplía varias propiedades: \\(\\lim_{x \\to \\infty} F_X(x) = 1\\) \\(\\lim_{x \\to -\\infty} F_X(x) = 0\\) \\(F_X\\) es no decreciente. \\(F_X\\) es continua por la derecha. \\(F_X\\) tiene límites por la izquierda. Los puntos 4 y 5 se resumen diciendo que la función es càdlág. Tener la acumulada nos permitía calcular probabilidades de intervalos; por ejemplo: \\[ \\mathbb{P}(a &lt; X \\leq b) = F_X(b) - F_X(a) \\] o bien: \\[ \\mathbb{P}(X &lt; x) = \\lim_{z \\to x^-} F(z) \\] Las funciones de distribución acumulada más comunes se veían como en la imagen: Si una función de distribución acumulada \\(F_X\\) era continua entonces decíamos que la variable aleatoria asociada (\\(X\\)) es continua. En particular, la continuidad implica que: \\[ \\mathbb{P}(X = k) = 0 \\qquad \\forall k \\] B.7 Funciones de masa de probabilidad Si una variable aleatoria \\(X\\) tomaba una cantidad finita o numerable de valores decíamos que \\(X\\) es una variable aleatoria discreta. Dentro de las variables aleatorias discretas teníamos varios modelos. Una cosa importante de las variables aleatorias es la función de masa de probabilidad que se define como: Dada una variable aleatoria discreta \\(X\\) definimos la función de masa de probabilidad de \\(X\\) como la función \\(p:\\mathbb{R} \\to \\mathbb{R}\\) tal que: \\[ p(x) = \\mathbb{P}(X = x) \\] para todo \\(x \\in\\mathbb{R}\\). Algunos modelos importantes son: Sea \\(A = \\{ a_1, a_2, \\dots, a_n \\}\\) un conjunto finito de \\(n\\) elementos. Una variable aleatoria \\(X\\) tiene una distribución uniforme discreta si: \\[ \\mathbb{P}\\big( X = a_k \\big) = \\dfrac{1}{n} \\cdot \\mathbb{I}_{A}(a_k) \\qquad \\forall k \\in \\{ 1, 2, \\dots, n \\} \\] Un modelo particular salía de considerar el siguiente problema: Tenemos una moneda que cae Águila con probabilidad \\(p\\) y Sol con probabilidad \\((1-p)\\) (con \\(0 &lt; p &lt; 1\\)). Nos interesa saber cuál es la probabilidad de tener \\(k\\) Águilas en \\(n\\) tiros. Solución A fin de resolver este problema notamos que necesitamos acomodar las \\(k\\) águilas en los \\(n\\) tiros para ello hay \\(\\binom{n}{k}\\) formas de hacerlo; cada águila cae con probabilidad \\(p\\) y hay \\(k\\); como son independientes esto nos da \\(p^k\\); por otro lado hay \\(n-k\\) soles cada uno cayó con probabilidad \\((1-p)\\). Esta lógica da origen al modelo binomial: Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Binomial}(n,p)\\) si: \\[ \\mathbb{P}\\big(X = k \\big) = \\binom{n}{k} p^k (1-p)^{n-k} \\mathbb{I}_{\\{0,1,2,\\dots,n \\}}(k) \\] Una pregunta distinta que nos pudimos hacer fue: Tenemos una moneda que cae Águila con probabilidad \\(p\\) y Sol con probabilidad \\((1-p)\\) (con \\(0 &lt; p &lt; 1\\)). Arrojamos la moneda hasta obtener \\(r\\) Águilas y en ese momento nos detenemos. Determina la probabilidad de que se aviente la moneda \\(k\\) veces. Para ello notamos que la última Águila está fija por lo que sólo debemos poner las \\(r-1\\) Águilas en los \\(k-1\\) lugares restantes, \\(\\binom{k-1}{r-1}\\). Por otro lado, cada Águila tiene probabilidad \\(p\\) y como son \\(k\\) tiros independientes entonces tenemos \\(p^r\\); para los soles tenemos \\((1-p)^{k-r}\\). Esto nos genera el modelo Binomial Negativo: Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Binomial Negativa}(r,p)\\) si: \\[ \\mathbb{P}\\big(X = k \\big) = \\binom{k-1}{r-1} p^r (1-p)^{k-r} \\mathbb{I}_{\\{r, r+1, r+2, \\dots \\}}(k) \\] Finalmente, otro modelo que pudimos hacer con monedas es un caso específico del Binomial Negativo . Aquí la pregunta es, se tira una moneda que tiene probabilidad \\(p\\) de salir Águila hasta que se obtiene el águila. Contamos cuántos tiros ocurrieron hasta que ocurriera el primer Águila y la pregunta de interés es la probabilidad de haber realizado específicamente \\(k\\) tiros. Para ello necesitamos tener \\((k-1)\\) tiros que fueran sol: \\((1-p)^{k-1}\\) y un tiro que saliera águila \\(p\\). Esto nos genera el modelo geométrico: Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Geométrica}(p)\\) si: \\[ \\mathbb{P}\\big(X = k \\big) = (1-p)^k p \\cdot \\mathbb{I}_{\\mathbb{N}}(k). \\] Otro modelo de interés es el siguiente: Se tiene una población de tamaño \\(M\\) donde \\(N\\) individuos pertenecen al partido político AZUL y \\(M-N\\) pertenecen al VERDE Se toma una submuestra de tamaño \\(m\\). Determina la probabilidad de que haya \\(n\\) individuos del partido político AZUL. Para ello notamos que hay \\(\\binom{M}{m}\\) muestras totales. Por otro lado, necesitamos extraer de los \\(N\\) azules a una submuestra de \\(n\\): \\(\\binom{N}{n}\\); finalmente, de los \\(M\\) verdes necesitamos extraer una submuestra de \\(m\\), hay \\(\\binom{M-N}{m-n}\\) formas de hacerlo. Concluimos entonces con el modelo hipergeométrico: Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Hipergeométrica}(M,N,m)\\) si: \\[ \\mathbb{P}\\big(X = n \\big) = \\dfrac{\\binom{M-N}{m-n} \\binom{N}{n} }{\\binom{M}{m}} \\cdot \\mathbb{I}_{\\{0,1,\\dots, \\text{mín}\\{m, N\\} \\}} (n) \\] El modelo Poisson va a ser bastante útil. Para estudiarlo, consideremos un modelo. Vamos a pensar en un servidor de computación (piensa en una página de Internet) que recibe solicitudes de entrar a la página de manera independiente y aleatoria en un intervalo de tiempo entre \\(t = 0\\) y \\(t = 1\\). Como primera aproximación podemos dividir el intervalo en \\(n\\) pedazos cada uno de longitud \\(1/n\\) y asumir que, a fuerza, sólo una conexión se puede realizar en cada uno de esos pedazos. Finalmente, asumamos que la probabilidad \\(p\\) de que se haga una conexión es proporcional a la longitud del intervalo y sea \\(p = \\lambda / n\\). Con estas hipótesis, la probabilidad de tener \\(k\\) conexiones (\\(k\\) entero entre \\(0\\) y \\(n\\)) está dada por un modelo binomial: \\[\\begin{equation}\\nonumber \\begin{aligned} f_n(k) &amp; = \\binom{n}{k} \\Big( \\frac{\\lambda}{n} \\Big)^k \\Big(1 - \\frac{\\lambda}{n} \\Big)^k \\ &amp; = \\frac{\\lambda^k}{k!} \\Big( 1 - \\frac{\\lambda}{n})^n \\frac{n!}{n^k(n-k)!} \\Big( 1 - \\frac{\\lambda}{n})^{-k} \\end{aligned} \\end{equation}\\] de donde concluimos que si continuamos partiendo el intervalo en pedazos cada vez más pequeños obtenemos: \\[\\begin{equation}\\nonumber \\begin{aligned} \\lim_{n \\to \\infty} f_n(k) &amp; = \\frac{e^{-\\lambda} \\lambda^k}{k!} \\end{aligned} \\end{equation}\\] Esto resulta en el modelo Poisson: Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Poisson}(\\lambda)\\) si: \\[ \\mathbb{P}\\big(X = k \\big) = \\dfrac{\\lambda^k e^{-\\lambda}}{k!} \\mathbb{I}_{\\mathbb{N}\\cup \\{ 0 \\}}(k) \\] B.8 Funciones de densidad Por construcción, las variables aleatorias continuas no tienen una función de masa de probabilidad (recuerda que \\(\\mathbb{P}(X = k) = 0\\) si \\(X\\) es continua para todo \\(k\\)). Sin embargo, es posible definir, si \\(F_X\\) es diferenciable algo similar, la función de densidad. Para una variable aleatoria \\(X\\) con función de distribución acumulada \\(F_X\\) diferenciable, definimos la función de densidad como: \\[ f_X(x) = \\dfrac{d}{dx} F_X(x) \\] Notamos que una función de densidad no es una probabilidad y no necesariamente sigue las mismas reglas; lo único que se requiere es: \\(f_X(x) \\geq 0\\) para toda \\(x\\). \\(\\int\\limits_{-\\infty}^{\\infty} f_X(x) dx = 1\\). La primer función de densidad es la que a un intervalo \\([a,b]\\) (ya sea abierto, cerrado o como sea) asigna a cada subintervalo una probabilidad proporcional a su longitud. Éste es el modelo uniforme: Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Uniforme}(a,b)\\) si: \\[ f_X(x) = \\dfrac{1}{b-a}\\mathbb{I}_{(a,b)}(x) \\] Una generalización del modelo uniforme es el beta (eventualmente veremos de dónde sale): Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Beta}(\\alpha,\\beta)\\) si: \\[ f_X(x) = \\dfrac{x^{\\alpha - 1}(1-x)^{\\beta - 1}}{B(\\alpha, \\beta)}\\mathbb{I}_{(0,1)}(x) \\] donde \\[ B(\\alpha, \\beta) = \\dfrac{\\Gamma (\\alpha) \\Gamma (\\beta)}{\\Gamma (\\alpha + \\beta)} \\] Podemos deducir el modelo exponencial a partir de la descripción del Poisson. Volvamos al mismo problema del \\(\\textrm{Poisson}(\\lambda)\\) donde hay computadoras conectándose a un servidor. Sea \\(W\\) la variable aleatoria que denota el tiempo de espera hasta el primer evento. Analicemos su distribución acumulada; notamos que \\[ F_W(w) = \\mathbb{P}(W \\leq w) = 1 - \\mathbb{P}(W &gt; w) \\] Ahora, para que \\(W &gt; w\\) eso significa que ningún evento tuvo que haber ocurrido en los primeros \\(w\\) minutos (horas, lo que sea la unidad de tiempo). Y ese evento es equivalente a que nuestra variable aleatoria Poisson (tasa \\(\\lambda w\\))13 no tenga ningún arribo: \\[ \\mathbb{P}(X = 0) = \\dfrac{(\\lambda w)^0 e^{-\\lambda w}}{0!} = e^{-\\lambda} \\] De donde se obtiene la función de distribución acumulada: \\[ F_W(w) = 1 - e^{-\\lambda w} \\] De donde, al derivar respecto a \\(w\\), se obtiene el modelo exponencial: Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Exponencial}(\\lambda)\\) si: \\[ f_X(x) = \\lambda e^{-\\lambda x} \\mathbb{I}_{(0,\\infty)}(x) \\] Para deducir la distribución gamma, vamos a preguntarnos por exactamente el mismo proceso pero esta vez, en lugar de preguntarnos por el tiempo para la primer conexión nos preguntaremos por el tiempo para la \\(\\alpha\\)-ésima conexión. Para ello, sea \\(W_{\\alpha}\\) el tiempo hasta la \\(\\alpha\\)-ésima conexión. Usamos el mismo truco del complemento que la vez pasada: \\[ F_{W_{\\alpha}}(w) = \\mathbb{P}(W_{\\alpha} \\leq w) = 1 - \\mathbb{P}(W_{\\alpha} &gt; w) \\] Y notamos que para que \\(W_{\\alpha} &gt; w\\) entonces a lo más debieron haber \\(\\alpha-1\\) conexiones. Podemos reescribir: \\[ F_{W_{\\alpha}}(w) = 1 - \\mathbb{P}(W_{\\alpha} &gt; w) = 1 - \\sum\\limits_{k = 0}^{\\alpha - 1} \\dfrac{(\\lambda w)^k e^{-\\lambda w}}{k!} = 1 - e^{- \\lambda w} - \\sum\\limits_{k = 1}^{\\alpha - 1} \\dfrac{(\\lambda w)^k e^{-\\lambda w}}{k!} \\] Derivamos: \\[\\begin{equation}\\nonumber \\begin{aligned} \\dfrac{d}{dw}F_{W_{\\alpha}}(w) &amp; = -\\lambda e^{- \\lambda w} - \\sum\\limits_{k = 1}^{\\alpha - 1} \\dfrac{k \\lambda (\\lambda w)^{k-1} e^{-\\lambda w} - \\lambda (\\lambda w)^k e^{-\\lambda w}}{k!} \\ &amp; = -\\lambda e^{- \\lambda w} - \\lambda e^{- \\lambda w} \\sum\\limits_{k = 1}^{\\alpha - 1} \\underbrace{\\dfrac{(\\lambda w)^{k-1}}{(k-1)!} - \\dfrac{(\\lambda w)^k }{k!}}_{\\text{Telescópica}} \\ &amp; = -\\lambda e^{- \\lambda w} + \\lambda e^{- \\lambda w} \\Bigg( \\dfrac{(\\lambda w)^{\\alpha - 1} }{(\\alpha - 1)!} - 1 \\Bigg) \\ &amp; = \\lambda e^{- \\lambda w} \\dfrac{(\\lambda w)^{\\alpha - 1} }{(\\alpha - 1)!} \\ &amp; = \\dfrac{\\beta^{\\alpha} }{\\Gamma (\\alpha)} w^{\\alpha - 1} e^{- \\frac{w}{\\beta}} \\ \\end{aligned} \\end{equation}\\] donde tomamos \\(\\beta = \\frac{1}{\\lambda}\\). Esto sugiere el modelo gamma: Una variable aleatoria \\(W\\) tiene una distribución \\(\\text{Gamma}(\\alpha,\\beta)\\) si: \\[ f_W(w) = \\dfrac{\\beta^{\\alpha} }{\\Gamma (\\alpha)} w^{\\alpha - 1} e^{- \\frac{w}{\\beta}} \\mathbb{I}_{(0,\\infty)} \\] para \\(\\alpha,\\beta &gt; 0\\). Para deducir el modelo normal consideremos lo siguiente. Pensemos que estamos midiendo la posición de las estrellas en el cielo. Para ello hay dos formas. Bajo coordenadas cartesianas \\((x,y)\\) pensemos que el error de medición es independiente; es decir, si \\(f(x,y)\\) es la densidad de los errores entonces: \\[ \\rho (x,y) = f(x) f(y) \\] Por otro lado, asumamos que existe también una representación en coordenadas polares de la posición de la estrella: \\[ g (r, \\theta) = g(r) \\] donde el error de medición depende sólo del radio (no del ángulo). Notamos entonces que: \\[ f(x) f(y) = g\\Big( \\sqrt{x^2 + y^2} \\Big) \\] Si tomamos \\(y = 0\\) tenemos que \\(f(x) f(0) = g(x)\\) (asumo \\(x &gt; 0\\); los otros casos son similares). Podemos entonces sustituir: \\[ \\dfrac{f(x) f(y)}{f(0)^2} = \\dfrac{f\\Big( \\sqrt{x^2 + y^2} \\Big) }{f(0)} \\] Tomamos logaritmo: \\[ \\ln \\dfrac{f(x)}{f(0)} + \\ln \\dfrac{f(y)}{f(0)} = \\ln \\dfrac{f\\Big( \\sqrt{x^2 + y^2} \\Big) }{f(0)} \\] Notamos que una solución es que: \\[ \\ln \\dfrac{f(x)}{f(0)} = \\alpha x^2 \\] de donde despejamos y obtenemos: \\[ f(x) = \\frac{1}{f(0)} e^{\\alpha x^2} \\] Finalmente sabemos que debe integrar a \\(1\\) y por tanto esto fuerza a \\(\\alpha\\) a ser negativo. En particular tomaremos \\(\\alpha = -\\frac{1}{2}\\) \\[ f(x) = \\frac{1}{f(0)} e^{-\\frac{1}{2} x^2} \\] Y para que integre a \\(1\\):s \\[ f(x) = \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{1}{2} x^2} \\] Por último, notamos que si \\(Z\\sim \\textrm{Normal}(0,1)\\) entonces \\(X = \\sigma Z + \\mu\\) tiene la densidad dada por14: \\[ f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{1}{2\\sigma^2} (x - \\mu)^2} \\] Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Normal}(\\mu,\\sigma)\\) si: \\[ f_X(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}} \\] B.9 Teorema de cambio de variable unidimensional Supongamos que tenemos una variable aleatoria \\(X\\) y nos interesa ver cómo se ve la \\(X\\) después de aplicarle una función \\(\\phi\\). Por ejemplo, si \\(X\\sim\\textrm{Normal}(0,1)\\) la función de densidad de \\(e^X\\) está dada por: \\[ f_X(x) = \\dfrac{1}{x \\sqrt{2 \\pi \\sigma^2}}e^{-(\\ln(x) - \\mu)^2/2\\sigma^2} \\mathbb{I}_{(0,\\infty)}(x). \\] Lo cual cambia mucho la forma de la distribución: La pregunta es, cómo obtener la función de densidad de \\(X\\) si se conoce la función \\(\\phi\\); el teorema de cambio de variable nos da una respuesta cuando \\(\\phi\\) es monótona estrictamente creciente o bien estrictamente decreciente y diferenciable. Sea \\(X\\) una variable aleatoria continua y \\(\\phi\\) una función estrictamente creciente ó estrictamente decreciente y diferenciable. Entonces: \\[ f_{\\phi(X)}(t) = f_X( \\phi^{-1}(t) ) \\cdot \\left| \\dfrac{d}{dt} \\phi^{-1}(t) \\right| \\] DEM: Caso estrictamente decreciente Como \\(\\phi\\) es estrictamente decreciente es invertible y por tanto: \\[\\begin{equation}\\nonumber \\begin{aligned} F_{\\phi(X)}(t) &amp; = \\mathbb{P}(\\phi(X) \\leq t) \\\\ &amp; = \\mathbb{P}(X \\geq \\phi^{-1}(t) ) \\\\ &amp; = 1 - \\mathbb{P}(X \\leq \\phi^{-1}(t) ) \\\\ &amp; = 1 - F_X( \\phi^{-1}(t) ) \\end{aligned} \\end{equation}\\] luego derivamos respecto a \\(t\\): \\[\\begin{equation}\\nonumber \\begin{aligned} f_{\\phi(X)}(t) &amp; = \\dfrac{d}{dt} F_{\\phi(X)}(t) \\\\ &amp; = - \\dfrac{d}{dt} F_X( \\phi^{-1}(t) ) \\\\ &amp; = - f_X( \\phi^{-1}(t) ) \\cdot \\dfrac{d}{dt} \\phi^{-1}(t) \\\\ &amp; = f_X( \\phi^{-1}(t) ) \\cdot \\left| \\dfrac{d}{dt} \\phi^{-1}(t) \\right| \\end{aligned} \\end{equation}\\] Donde el valor absoluto sale de que \\(\\phi^{-1}(t) &lt; 0\\) por ser estrictamente decreciente la \\(\\phi\\). B.10 Probabilidad Multivariada De la misma manera que hablamos de una sola variable aleatoria podemos hablar de muchas como múltiples funciones de \\(\\Omega \\in \\mathbb{R}\\). Para una colección finita \\(\\{ X_i \\}_{i = 1}^n\\) de variables aleatorias podemos hablar de su función de distribución acumulada conjunta como: \\[ F_{\\vec{X}}(x_1, x_2, \\dots, x_n) = \\mathbb{P}\\big( X_1 \\leq x_1, X_2 \\leq x_2, \\dots, X_n \\leq x_n) \\] donde suponemos que \\(\\vec{X} = (X_1, X_2, \\dots, X_n)^T\\) es un vector aleatorio cuyas entradas son las variables de la colección anterior. En el caso de que las \\(n\\) variables sean discretas la función de masa conjunta está dada por: \\[ p_{\\vec{X}}(x_1, x_2, \\dots, x_n) = \\mathbb{P}(X_1 = x_1, X_2 = x_2, \\dots, X_n = x_n) \\] En el caso de que sean continuas (\\(F_{\\vec{X}}\\) diferenciable en sus \\(n\\) entradas) entonces la densidad está dada por: \\[ f_{\\vec{X}}(x_1, x_2, \\dots, x_n) = \\dfrac{\\partial^n}{\\partial x_1 \\partial x_2 \\dots \\partial x_n}F_{\\vec{X}}\\Bigg|_{(x_1, x_2, \\dots, x_n)} \\] En general la función de probabilidad conjunta siempre va a esta dada por: \\[ \\mathbb{P}(X_1 \\in A_1, X_2 \\in A_2, \\dots, X_n \\in A_n) = \\mathbb{P}\\Big(\\{ \\omega \\in \\Omega : X_1(\\omega) \\in A_1 \\text{ y } X_2(\\omega) \\in A_2 \\text{ y } \\dots \\text{ y } X_n(\\omega) \\in A_n \\}\\Big) \\] para \\(A_1, A_2, \\dots, A_n\\) medibles (bajo \\(X_1, X_2, \\dots, X_n\\) respectivamente). Dos variables aleatorias \\(X_i\\) y \\(X_j\\) (\\(i \\neq j\\)) son independientes si: \\[ \\mathbb{P}(X_i \\in A, X_i \\in B) = \\mathbb{P}(X_i \\in A) \\cdot \\mathbb{P}(X_j \\in B) \\] para \\(A,B\\) medibles. Una colección \\(\\{ X_i \\}_{i}\\) de variables aleatorias es completamente independiente si para cualquier subcolección finita \\(\\{ X_{i_k} \\}_{i_k}\\) se tiene que: \\[ \\mathbb{P}(X_{i_1} \\in A_{i_1}, X_{i_2} \\in A_{i_2}, \\dots, X_{i_n} \\in A_{i_n} ) = \\prod_{k = 1}^n \\mathbb{P}(X_{i_k} \\in A) \\] en el contexto de estas notas, a menos que se indique lo contrario, las variables aleatorias que utilicemos serán completamente independientes. Un aspecto interesante de la independencia es que permite partir las funciones de masa, densidad y distribución acumulada en dos funciones independientes. Así, si \\(X,Y\\) son independientes con masa conjunta \\(p\\): \\[ p_{X,Y}(x,y) = \\mathbb{P}(X = x, Y = y) = \\mathbb{P}(X = x)\\cdot\\mathbb{P}(Y = y) = p_X(x)\\cdot p_Y(y) \\] El resultado se mantiene para distribuciones: \\[ F_{X,Y}(x,y) = \\mathbb{P}(X \\leq x, Y \\leq y) = \\mathbb{P}(X \\leq x)\\cdot\\mathbb{P}(Y \\leq y) = F_X(x)\\cdot F_Y(y) \\] y si derivamos (en caso de \\(F\\) diferenciable), se mantiene para densidades: \\[ f_{X,Y}(x,y) = \\dfrac{\\partial^2}{\\partial x\\partial y} F_{X,Y}\\Big|_{(x,y)} = \\dfrac{\\partial^2}{\\partial x\\partial y} F_X(x)\\cdot F_Y(y)\\Big|_{(x,y)} = f_X(x) f_Y(y) \\] B.11 Esperanza, varianza y covarianza Para una función medible \\(g\\) de una variable aleatoria \\(X\\) definimos su valor esperado (si existe) como: \\[ \\mathbb{E}\\big[g(X)\\big] = \\begin{cases} \\sum\\limits_{x \\in \\text{Supp}(X)} g(x) \\cdot \\mathbb{P}(X = x) &amp; \\text{ si } X \\text{ discreta.} \\\\ \\int\\limits_{-\\infty}^{\\infty} g(x) \\cdot f_X(x) dx&amp; \\text{ si } X \\text{ continua} \\end{cases} \\] donde \\(f_X\\) es la densidad de \\(X\\) en el caso continuo y \\(\\text{Supp}(X)\\) es el conjunto imagen de \\(X\\) (el soporte): \\[ \\text{Supp}(X) = \\{ x : X(\\omega) = x \\text{ para } \\omega \\in \\Omega \\} \\] En el caso de conjuntos finitos de variables aleatorias la definción es similar: Para una función \\(g:\\mathbb{R}^n \\to \\mathbb{R}\\) multivariada de \\(n\\) variables aleatorias (sobre los reales) \\(X_1, X_2, \\dots, X_n\\) definimos su valor esperado (si existe y sin pérdida de generalidad suponiendo las primeras \\(j\\) son discretas y las últimas \\(n - (j + 1)\\) continuas) como: \\[\\begin{equation}\\nonumber \\begin{aligned} \\mathbb{E}\\big[ g(X_1, X_2, \\dots, X_n) \\big] = \\\\ &amp; \\int_{-\\infty}^{\\infty} \\dots \\int_{-\\infty}^{\\infty} \\sum_{x_j \\in \\text{Supp}(X_j)} \\dots \\sum_{x_1 \\in \\text{Supp}(X_1)} g(x_1, x_2, \\dots, x_n) p(x_1) \\dots p(x_j) f_{X_{j+1}}(x_{j+1}) \\dots f_{X_{n}}(x_{n}) dx_{j+1} \\dots dx_{n} \\end{aligned} \\end{equation}\\] donde \\(p(x_j)\\) es la masa de \\(X_j\\) (es decir \\(p(x_j) = \\mathbb{P}(X_j = x_j)\\). En el caso particular de dos variables aleatorias \\(X_1\\) y \\(X_2\\) podemos escribir la expresión de manera más sencilla: \\[\\begin{equation}\\nonumber \\begin{aligned} \\mathbb{E}\\big[ &amp; g(X_1, X_2) \\big] = \\begin{cases} \\int\\limits_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(x_1, x_2) f_{X_1}(x_1) f_{X_2}(x_2) dx_1 dx_2 &amp; \\text{ ambas continuas,} \\\\ \\\\ \\sum\\limits_{x \\in \\text{Supp}(X_1)} \\sum\\limits_{x \\in \\text{Supp}(X_2)} g(x_1, x_2) p(x_1) p(x_2) &amp; \\text{ ambas discretas,} \\\\ \\\\ \\int_{-\\infty}^{\\infty} \\sum\\limits_{x \\in \\text{Supp}(X_1)} g(x_1, x_2) p(x_1) p(x_2) &amp; X_1 \\text{ discreta, } X_2 \\text{ continua.} \\\\ \\end{cases} \\end{aligned} \\end{equation}\\] En particular, en el espacio de las variables aleatorias definimos un producto interno, la covarianza la cual está dada por: \\[ \\textrm{Cov}(X_1, X_2) = \\mathbb{E}\\Big[ \\big(X_1 - \\mathbb{E}[X_1]\\big) \\cdot \\big(X_2 - \\mathbb{E}[X_2]\\big) \\Big] \\] La varianza es un caso particular de la covarianza: cuando \\(X_1 = X_2\\): \\[ \\textrm{Cov}(X_1, X_1) = \\mathbb{E}\\Big[ \\big(X_1 - \\mathbb{E}[X_1]\\big)^2 \\Big] \\] B.11.1 Propiedades de valor esperado, varianza y covarianza El valor esperado al ser representable mediante sumas ó integrales cumple todas las propiedades de las sumas (resp integrales) en particular la linealidad: \\[ \\mathbb{E}\\Big[ a X + Y\\Big] = a \\mathbb{E}[X] + \\mathbb{E}[Y] \\] La demostración se hace exactamente igual en el caso de variables discretas, continuas (ó mezcla de una y una). Aquí muestro la de continuas con densidades \\(f_X\\) y \\(f_Y\\): \\[\\begin{equation} \\begin{aligned} \\mathbb{E}\\Big[ a X + Y\\Big] &amp; = \\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\infty} (a x + y) f_{X,Y}(x,y) dx dy \\\\ &amp; = a \\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\infty} x f_{X,Y}(x,y) dx dy + \\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\infty} y f_{X,Y}(x,y) dx dy \\\\ &amp; = a \\Big[ \\int\\limits_{-\\infty}^{\\infty} x f_X(x) dx \\Big] + \\int\\limits_{-\\infty}^{\\infty} y f_Y(y) dy \\\\ &amp; = a \\mathbb{E}[X] + \\mathbb{E}[Y] \\end{aligned} \\end{equation}\\] Otro resultado importante es que si dos variables aleatorias \\(X,Y\\) son independientes entonces el valor esperado del producto se parte: \\[ \\mathbb{E}[XY] = \\mathbb{E}[X] \\cdot \\mathbb{E}[Y] \\] La demostración se hace de manera idéntica en todos los casos. Aquí mostramos el caso de \\(X,Y\\) discretas: \\[\\begin{equation} \\begin{aligned} \\mathbb{E}\\Big[XY\\Big] &amp; = \\sum\\limits_{y \\in \\text{Sup}(Y)} \\sum\\limits_{x \\in \\text{Sup}(X)} xy \\mathbb{P}(X = x, Y = y) \\\\ &amp; = \\sum\\limits_{y \\in \\text{Sup}(Y)} \\sum\\limits_{x \\in \\text{Sup}(X)} xy \\mathbb{P}(X = x) \\mathbb{P}(Y = y) \\\\ &amp; = \\Big[\\sum\\limits_{y \\in \\text{Sup}(Y)} y \\mathbb{P}(Y = y)\\Big] \\Big[\\sum\\limits_{x \\in \\text{Sup}(X)} x \\mathbb{P}(X = x)\\Big] \\\\ &amp; = \\mathbb{E}[X] \\cdot \\mathbb{E}[Y] \\end{aligned} \\end{equation}\\] La linealidad nos permite reescribir la covarianza: \\[\\begin{equation} \\begin{aligned} \\textrm{Cov}(X_1, X_2) &amp; = \\mathbb{E}\\Big[ \\big(X_1 - \\mathbb{E}[X_1]\\big) \\cdot \\big(X_2 - \\mathbb{E}[X_2]\\big) \\Big] \\\\ &amp; = \\mathbb{E}\\Big[ X_1 X_2 \\Big] - \\mathbb{E}\\Big[X_1\\Big]\\mathbb{E}\\Big[X_2\\Big] - \\mathbb{E}\\Big[X_1\\Big]\\mathbb{E}\\Big[X_2\\Big] + \\mathbb{E}\\Big[X_1\\Big]\\mathbb{E}\\Big[X_2\\Big] \\\\ &amp; = \\mathbb{E}\\Big[ X_1 X_2 \\Big] - \\mathbb{E}\\Big[X_1\\Big]\\mathbb{E}\\Big[X_2\\Big] \\end{aligned} \\end{equation}\\] de tal forma que es claro que si \\(X_1\\) y \\(X_2\\) son independientes entonces \\(\\textrm{Cov}(X_1, X_2) = 0\\) por la propiedad anterior del valor esperado. OJO De manera general covarianza \\(0\\) no implica que las variables sean independientes como puede verse con las variables aleatorias siguientes: \\[ f_{X,Y}(x,y) = \\begin{cases} 1/8 &amp; \\text{ si } (x,y) \\in \\{ (-1,-1), (-1,1), (1, -1), (1,1)\\} \\\\ 1/2 &amp; \\text{ si } (x,y) = (0,0), \\\\ 0 &amp; \\text{ en otro caso} \\end{cases} \\] las cuales no son independientes pues \\(\\mathbb{P}(X = 0, Y = 0) = 1/2\\neq 1/4 = \\mathbb{P}(X = 0)\\cdot \\mathbb{P}(Y = 0)\\); sin embargo (ejercicio sugerido) la covarianza es \\(0\\). Una segunda propiedad de interés de la covarianza es que actúa como el producto interno (de hecho es uno): \\[ \\text{Cov}(a X + bY, cW + dV) = ac \\text{Cov}(X,W) + ad \\text{Cov}(X,V) + bc \\text{Cov}(Y,W) + bd \\text{Cov}(Y,V) \\] la cual se demuestra igual mediante la linealidad: \\[\\begin{equation} \\begin{aligned} \\textrm{Cov}(a &amp; X + bY, cW + dV) = \\mathbb{E}\\Big[ (a X + bY) (cW + dV) \\Big] - \\mathbb{E}\\Big[a X + bY\\Big]\\mathbb{E}\\Big[cW + dV\\Big] \\\\ &amp; = \\mathbb{E}\\Big[ ac XW + bc YW + ad XV + bd YV\\Big] - \\bigg( a \\mathbb{E}\\Big[ X \\Big] + b\\mathbb{E}\\Big[ Y\\Big]\\bigg)\\bigg( c\\mathbb{E}\\Big[W\\Big] + d\\mathbb{E}\\Big[V\\Big] \\bigg) \\\\ &amp; = ac \\text{Cov}(X,W) + ad \\text{Cov}(X,V) + bc \\text{Cov}(Y,W) + bd \\text{Cov}(Y,V) \\end{aligned} \\end{equation}\\] donde la última igualdad se sigue de agrupar los términos idénticos tras sus constantes. B.12 Condicionamiento por otra variable aleatoria A rellenarse pronto B.13 Funciones características A rellenarse pronto B.14 Convergencias A rellenarse pronto B.14.1 Teorema de continuidad de Lévy A rellenarse pronto B.15 Ley de los grandes números A rellenarse pronto B.16 Teorema del límite central A rellenarse pronto "]
]
