[
["index.html", "Estadística I: Análisis exploratorio de datos y muestreo Capítulo 1 Historia y conceptos", " Estadística I: Análisis exploratorio de datos y muestreo Rodrigo Zepeda-Tello 2020-10-13 Capítulo 1 Historia y conceptos El libro está en construcción. Por favor si encuentras cualquier error levanta un issue en Github "],
["análisis-exploratorio-de-datos.html", "Capítulo 2 Análisis Exploratorio de Datos 2.1 Inicio 2.2 Librerías 2.3 Base a analizar 2.4 Definiciones y notación 2.5 Estadísticos univariados 2.6 Ejercicio 2.7 Ejercicios 2.8 Gráficas univariadas 2.9 Gráficas bivariadas 2.10 Estadísticos bivariados 2.11 Ejercicio 2.12 Ajuste funcional 2.13 Ejercicios del capítulo", " Capítulo 2 Análisis Exploratorio de Datos 2.1 Inicio Siempre que inicies un nuevo trabajo en R ¡no olvides borrar el historial! rm(list = ls()) #Clear all 2.2 Librerías Para este análisis vamos a tener que llamar a las siguientes librerías previamente instaladas (por única vez) con install.packages: library(tidyverse) library(dplyr) library(moments) library(lubridate) library(ggcorrplot) library(ks) Si no tienes una librería puedes instalarla escribiendo en la consola el install junto con su nombre: install.packages(&quot;lubridate&quot;) 2.3 Base a analizar Como ejemplo analizaremos la base de Carpetas de Investigación de la Fiscalía General de Justicia de la CDMX para el año 2018 y mes de Diciembre misma que se encuentra en este link Si el link anterior no abre ve al sitio https://datos.cdmx.gob.mx/explore/dataset/carpetas-de-investigacion-pgj-cdmx/table/?refine.ao_hechos=2018 y elige la opción de año 2018, mes diciembre y descargar como csv. La forma más fácil en RStudio es yéndonos a Import Dataset en el panel derecho seguido de From Text y seleccionamos el archivo. En este caso hay dos opciones cualquiera de las dos opciones funciona: si en tu ordenador no sirve una, ¡prueba la otra! En mi caso el archivo está en una carpeta que se llama datasets y se lee de la siguiente manera: datos &lt;- read.csv(&quot;datasets/carpetas-de-investigacion-pgj-cdmx.csv&quot;) 2.4 Definiciones y notación Siguiendo la definición de Gelman et al. (2013) , denotamos el conjunto de datos observados como la matriz (base de datos) de \\(n \\times \\ell\\) \\[ Z = \\begin{pmatrix} z_1 \\Big| z_2 \\Big| \\dots \\Big| z_{\\ell} \\end{pmatrix} \\] donde \\(\\ell \\in \\mathbb{N}\\) con \\(\\ell &gt; 0\\) y las \\(z_i\\) sin pérdida de generalidad, son vectores columna de longitud \\(n\\) (\\(z_i = (z_{i,1}, z_{i,2}, \\dots, z_{i,n})^T\\)). Una columna \\(z_{k}\\) con \\(0 \\leq k \\leq \\ell\\) se le conoce como: Numérica si \\(z_{k} \\in \\mathbb{R}^{n}\\). En particular es entera si \\(z_{j} \\in \\mathbb{Z}^{n}\\). Categórica si cada entrada de \\(z_{k}\\) es una indicadora de pertenencia a algún conjunto (por ejemplo Hombre / Mujer ó Ingresos Altos / Ingresos Medios / Ingresos Bajos). Usualmente \\(z_{k}\\) se representa con un caracter o con un entero. Una variable cateórica puede ser lógica si \\(z_{k}\\) es un indicador que toma alguno de los dos valores: TRUE ó FALSE. Ordinal Una variable ordinal es aquél \\(z_{k} \\in \\mathcal{C}\\) donde sobre \\(\\mathcal{C}\\) existe un orden total; es decir si \\(x,y,w\\in z_{k}\\) se tiene que: Ocurre al menos una de las siguientes: \\(x \\leq y\\) ó \\(x \\geq y\\). Si \\(x \\leq y\\) y \\(y \\geq w\\) entonces \\(x \\leq w\\) Si \\(x \\leq y\\) y \\(x \\geq y\\) entonces \\(x = y\\). Variables numéricas univariadas son ordinales por el orden natural de \\(\\mathbb{R}\\). Caracter si \\(z_{k}\\) es un caracter o una cadena de caracteres donde los caracteres son el objeto de análisis en sí (no como pertenencia). Por ejemplo si cada entrada \\(z_{k,m}\\) representa un Tweet. OJO Los datos \\(z_{k,m}\\) son variables fijas ya dadas y NO SON ALEATORIAS. En el caso de nuestra base de datos podemos resumir la información contenida en la misma mediante glimpse: datos %&gt;% glimpse() ## Rows: 19,861 ## Columns: 18 ## $ año_hechos &lt;int&gt; 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, … ## $ mes_hechos &lt;chr&gt; &quot;Diciembre&quot;, &quot;Diciembre&quot;, &quot;Diciembre&quot;, &quot;Diciembr… ## $ fecha_hechos &lt;chr&gt; &quot;2018-12-13 12:00:00&quot;, &quot;2018-12-22 19:00:00&quot;, &quot;2… ## $ delito &lt;chr&gt; &quot;USURPACIÓN DE IDENTIDAD&quot;, &quot;SUSTRACCION DE MENOR… ## $ categoria_delito &lt;chr&gt; &quot;DELITO DE BAJO IMPACTO&quot;, &quot;DELITO DE BAJO IMPACT… ## $ fiscalía &lt;chr&gt; &quot;INVESTIGACIÓN EN MIGUEL HIDALGO&quot;, &quot;INVESTIGACIÓ… ## $ agencia &lt;chr&gt; &quot;MH-2&quot;, &quot;59&quot;, &quot;BJ-1&quot;, &quot;IZP-9&quot;, &quot;75TER&quot;, &quot;FDS-5&quot;,… ## $ unidad_investigacion &lt;chr&gt; &quot;UI-1SD&quot;, &quot;UI-1CD&quot;, &quot;UI-1SD&quot;, &quot;UI-2SD&quot;, &quot;3 S/D&quot;,… ## $ colonia_hechos &lt;chr&gt; &quot;LOMAS DE SOTELO&quot;, NA, &quot;DEL VALLE CENTRO&quot;, &quot;AMPL… ## $ alcaldia_hechos &lt;chr&gt; &quot;MIGUEL HIDALGO&quot;, &quot;CUAUTLA&quot;, &quot;BENITO JUAREZ&quot;, &quot;I… ## $ fecha_inicio &lt;chr&gt; &quot;2019-06-16 12:14:09&quot;, &quot;2019-06-06 16:26:15&quot;, &quot;2… ## $ mes_inicio &lt;chr&gt; &quot;Junio&quot;, &quot;Junio&quot;, &quot;Febrero&quot;, &quot;Febrero&quot;, &quot;Abril&quot;,… ## $ ao_inicio &lt;int&gt; 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, … ## $ calle_hechos &lt;chr&gt; &quot;AV. CONSCRIPTO&quot;, &quot;AVENIDFA DIEZ DE MARZO&quot;, &quot;FEL… ## $ calle_hechos2 &lt;chr&gt; &quot;.&quot;, &quot;HECHOS EN CUAUTLA MORELOS&quot;, &quot;ESQUINA COYOA… ## $ longitud &lt;dbl&gt; -99.22535, NA, -99.17088, -99.03016, -99.13423, … ## $ latitud &lt;dbl&gt; 19.44028, NA, 19.37207, 19.34797, 19.54788, 19.3… ## $ Geopoint &lt;chr&gt; &quot;19.4402832543,-99.2253527208&quot;, &quot;&quot;, &quot;19.37206828… Notamos que el vector columna año_hechos es una variable numérica mientras que mes_hechos es categórica. No hay variables lógicas en esta base. Una variable caracter es el vector columna calle_hechos que no denota un conjunto sino una cadena de caracteres (véanse las faltas de ortografía, por ejemplo). Al ser la tabla de datos una matriz podemos acceder a la entrada en la fila \\(j\\) y columna \\(k\\) haciendo: \\[ \\textrm{base}[j,k] \\] por ejemplo: datos[4,6] ## [1] &quot;INVESTIGACIÓN EN IZTAPALAPA&quot; NOTACIÓN Para facilitar la notación en lo que sigue de estas notas y hasta nuevo aviso, si \\(z_k\\) es una columna categórica de \\(Z\\) denotaremos a los elementos de dicha columna como \\(C = (c_1, c_2, \\dots, c_n) = z_k^T\\). Si \\(z_k\\) es numérica denotamos a los elementos de dicha columna como \\(\\vec{x} = (x_1, x_2, \\dots, x_n) = z_k^T\\). 2.5 Estadísticos univariados 2.5.1 Definición [Estadístico] Un estadístico es una función cuyo dominio es la matriz de datos observados \\(Z\\) o una columna de la misma. Es decir, un estadístico es cualquier función de los datos (ver Wolfe and Schneider (2017)). A continuación veremos algunos ejemplos de estadísticos así como su interpretación. 1. Media poblacional Dado un vector de datos numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) definimos la media poblacional como: \\[ \\bar{x} = \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} x_i, \\qquad x_i \\in \\mathbb{R} \\] En el caso de nuestros datos podemos calcular el promedio de delitos por día como sigue. Primero necesitamos especificar a R que la fecha_hechos es una fecha. Esto lo hacemos mediante la función ymd_hms (year-month-day_hour-minute-second) del paquete de lubridate y la función mutate (que cambia una columna de la base de datos). El siguiente código le indica a R que cambie la columna fecha_hechos volviéndola a leer como fecha: datos &lt;- datos %&gt;% mutate(fecha_hechos = ymd_hms(fecha_hechos)) Para mantener sólo la fecha y eliminar la hora de fecha_hechos podemos generar una nueva columna como sigue: datos &lt;- datos %&gt;% mutate(fecha = date(fecha_hechos)) Finalmente podemos contar (tally) observaciones agrupadas (group_by) por día mediante la combinación de ambas funciones: conteo_delitos &lt;- datos %&gt;% group_by(fecha) %&gt;% tally() ## # A tibble: 6 x 2 ## fecha n ## &lt;date&gt; &lt;int&gt; ## 1 2018-12-01 674 ## 2 2018-12-02 584 ## 3 2018-12-03 790 ## 4 2018-12-04 640 ## 5 2018-12-05 724 ## 6 2018-12-06 718 Hay distintas formas de calcular la media. La primera es tomando la columna directo, para acceder a una columna utilizamos el signo de pesos \\(\\$\\) como sigue: \\[ \\texttt{base} \\texttt{\\$} \\texttt{columna} \\] En nuestro caso: mean(conteo_delitos$n) ## [1] 640.6774 O bien podemos usar la función summarise integrada en dplyr: conteo_delitos %&gt;% summarise(mean(n)) ## # A tibble: 1 x 1 ## `mean(n)` ## &lt;dbl&gt; ## 1 641. NOTA Una media como está descrita arriba no aplica para datos circulares. Por ejemplo, si queremos determinar el mes promedio en el que ocurren las lluvias dentro de los años se sabe que después del mes 12 continúa el mes 1 del próximo año. Una media tradicional no considera datos que pueden ser descritos mediante aritmética modular (como los meses). Para ello se utiliza la media circular: 1.1 Media circular Consideremos el problema de determinar el día promedio de la semana en que más ocurren delitos (de Lunes a Domingo). Podemos resumir los eventos usando la función weekdays: datos &lt;- datos %&gt;% mutate(`Día de la Semana` = weekdays(fecha)) conteo.dia &lt;- datos %&gt;% group_by(`Día de la Semana`) %&gt;% count() de donde se tiene el conteo: Día de la Semana n Monday 3251 Saturday 3197 Friday 2833 Sunday 2722 Wednesday 2701 Thursday 2679 Tuesday 2478 Para obtener el día promedio representamos cada uno de los días en el círculo usando coordenadas polares. Nota que el radio es irrelevante en este caso: sólo el ángulo importa; de ahí que tomemos \\(r = 1\\): Para un conjunto de mediciones con ángulos \\((\\theta_1, \\theta_2, \\dots, \\theta_n)^T\\) el centro de masa asociado a dichas mediciones es el punto \\((\\bar{c}, \\bar{s})\\) donde \\[ \\bar{c} = \\frac{1}{n}\\sum\\limits_{i = 1}^n \\cos (\\theta_i) \\qquad \\text{y} \\qquad \\bar{s} = \\frac{1}{n}\\sum\\limits_{i = 1}^n \\sin (\\theta_i) \\] La dirección media se define como la solución \\(\\bar{\\theta}\\) (si \\(\\bar{r} &gt; 0\\)) a: \\[ \\bar{c} = \\bar{r}\\cos\\bar{\\theta} \\qquad \\text{ y } \\qquad \\bar{s} = \\bar{r}\\sin\\bar{\\theta} \\] donde \\(\\bar{r}\\) se conoce como la longitud resultante promedio . Si \\(\\bar{r} = 0\\) no existe dirección media. De manera explícita, por geometría tenemos que: \\[ \\bar{r} = (\\bar{c}^2 + \\bar{s}^2)^{1/2} \\] y que: \\[ \\bar{\\theta} = \\text{atan2}(\\bar{s}/\\bar{c}) = \\begin{cases} \\text{atan}(\\bar{s}/\\bar{c}) &amp; \\text{ si } \\bar{c} \\geq 0\\\\ \\text{atan}(\\bar{s}/\\bar{c}) + \\pi &amp; \\text{ si } \\bar{c} &lt; 0\\\\ \\end{cases} \\] donde el caso \\(\\bar{c} = 0\\) se interpreta como el límite por la derecha (respectivamente por la izquierda) de la arcotangente de acuerdo con el signo de \\(\\bar{s}\\). Para más información sobre estadística circular puedes consultar Pewsey, Neuhäuser, and Ruxton (2013) 2.6 Ejercicio Utiliza la función atan2 de R junto con cos y sin para seno y coseno para estimar el día promedio en el que ocurren más delitos según la base conteo.dia. 2. Total poblacional (ver Särndal, Swensson, and Wretman (2003)) Dado un vector de datos numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) definimos el total poblacional como: \\[ t_{\\vec{x}} = \\sum\\limits_{i=1}^{n} x_i, \\qquad x_i \\in \\mathbb{R} \\] En este caso de las carpetas de investigación el total nos daría todas las carpetas abiertas durante diciembre. Para ello calculamos el total sumando todos los elementos: sum(conteo_delitos$n) ## [1] 19861 O bien (y esto es una de las cosas interesantes de tidyverse) agregándolo a los cálculos previos: conteo_delitos %&gt;% summarise(mean(n), sum(n)) ## # A tibble: 1 x 2 ## `mean(n)` `sum(n)` ## &lt;dbl&gt; &lt;int&gt; ## 1 641. 19861 3. Varianza poblacional (no ajustada) Dado un vector de datos numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) definimos la varianza poblacional como1: \\[ \\sigma^2_{\\vec{x}} = \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} (x_i - \\bar{x})^2, \\qquad x_i \\in \\mathbb{R} \\] Misma que podemos calcular con el comando var ya sea directamente en la columna: var(conteo_delitos$n) ## [1] 10046.23 O bien a través del summarise integrando con el anterior: conteo_delitos %&gt;% summarise(mean(n), sum(n), var(n)) ## # A tibble: 1 x 3 ## `mean(n)` `sum(n)` `var(n)` ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 641. 19861 10046. La raíz cuadrada de la varianza se conoce como desviación estándar y se denota como sigue: \\[ \\sigma_{\\vec{x}} = \\sqrt{\\sigma^2_{\\vec{x}}} \\] Recuerda que la varianza se interpreta como la distancia cuadrática promedio a la que están los datos. En particular la varianza casi no considera valores que están a menos de \\(1\\) de distancia de \\(\\bar{x}\\) (pues \\((x_i - \\bar{x})^2 &lt; 1\\) en ese caso) pero le da mayor peso a valores que están muy lejanos (donde \\((x_i - \\bar{x})^2 \\gg 1\\) si \\(x_i\\) está muy lejos de \\(\\bar{x}\\)). Gráficamente: Si nos interesara que todos los valores (tanto los cercanos a \\(\\bar{x}\\) como los lejanos) pesaran de manera idéntica entonces usaríamos el MAD: 3.1 Varianza angular (circular) En el caso de datos circulares, Pewsey, Neuhäuser, and Ruxton (2013) define la varianza circular como: \\[ \\textrm{Var} = 1 - \\bar{r} \\] donde \\(\\bar{r} = (\\bar{c}^2 + \\bar{s}^2)^{1/2}\\) es el resolvente explicado anteriormente. 4. Desviación Media Absoluta (MAD) Dado un vector de datos numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) definimos la desviación media absoluta, MAD, como (Panaretos (2016)): \\[ \\text{MAD}_{\\vec{x}} = \\frac{1}{n} \\sum\\limits_{i=1}^{n} | x_i - \\bar{x} | \\] Misma que se puede calcular en R como: mad(conteo_delitos$n) ## [1] 115.6428 o bien dentro del summarise: conteo_delitos %&gt;% summarise(mean(n), sum(n), var(n), mad(n)) ## # A tibble: 1 x 4 ## `mean(n)` `sum(n)` `var(n)` `mad(n)` ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 641. 19861 10046. 116. La MAD también es una forma de medir distancia pero en este caso se tiene que todos aportan por igual los muy alejados y los que no: Para pensarle: En el caso de una variable que se supone que es uniforme y no interesa penalizar valores lejanos de la media ¿cuál sería una mejor manera de cuantificar la dispersión MAD ó varianza? ¿en qué casos importaría la otra? Las siguientes dos definiciones son con base en conceptos de proba. ¿Los recuerdas? 5. Coeficiente de asimetría Dado un vector de datos numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) definimos el coeficiente de asimetría de Fisher (skewness) como: \\[ \\text{Skewness}_{\\vec{x}} = \\frac{1}{n \\sigma^3_{\\vec{x}} } \\sum\\limits_{i = 1}^{n} (x_i - \\bar{x})^3 \\] Para más referencias ver Panaretos (2016). A fin de interpretar el coeficiente de asimetría podemos dividir esa suma en dos pedazos (olvidándonos de la constante): \\[ \\sum\\limits_{i = 1}^{n} (x_i - \\bar{x})^3 = \\underbrace{\\sum\\limits_{\\substack{i = 1 \\\\ \\\\ x_i &gt; \\bar{x}}}^{n} (x_i - \\bar{x})^3}_{\\text{A}} + \\underbrace{\\sum\\limits_{\\substack{i = 1 \\\\ \\\\ x_i &lt; \\bar{x}}}^{n} (x_i - \\bar{x})^3}_{\\text{B}} \\] Notamos que si \\(|A| &gt; |B|\\) la mayor parte de las \\(x_i\\) (o las que se alejan más de la media) son mayores a \\(\\bar{x}\\) y por tanto los datos van a estar sesgados a la derecha:. Por otro lado si \\(|B| &gt; |A|\\) significa que hay más \\(x_i\\) (o con mayor peso) del lado izquierdo de la media que del lado derecho de la misma y por tanto los datos están sesgados a la izquierda. Datos insesgados son aquellos donde \\(\\text{Skewness}_{\\vec{x}} = 0\\). En el caso de las carpetas podemos calcular la asimetría que no se encuentra preprogramada en R como sigue: #Estimación de la desviación estándar desv.est &lt;- sd(conteo_delitos$n) #Estimación del x barra x.barra &lt;- mean(conteo_delitos$n) #Obtención de la n (longitud del vector) n.longitud &lt;- length(conteo_delitos$n) #Cálculo de la asimetría (1/desv.est^3)*mean((conteo_delitos$n - x.barra)^3) ## [1] -0.4528209 ¿Qué implica el resultado anterior? 6. Curtosis Dado el mismo vector \\(\\vec{x}\\) que en el enunciado anterior el coeficiente de curtosis se define como \\[ \\text{Curtosis}_{\\vec{x}} = \\frac{1}{n \\sigma^4_{\\vec{x}} } \\sum\\limits_{i = 1}^{n} (x_i - \\bar{x})^4 \\] La interpretación de la curtosis es similar a la que hicimos de la varianza en el sentido que el elevar a la cuarta va a magnificar los efectos de aquellos valores que estén a más de \\(\\sigma\\) de distancia de la media pues podemos reescribir la suma como: \\[ \\frac{1}{n \\sigma^4_{\\vec{x}} } \\sum\\limits_{i = 1}^{n} (x_i - \\bar{x})^4 = \\frac{1}{n \\sigma^4_{\\vec{x}} } \\underbrace{\\sum\\limits_{\\substack{i = 1 \\\\ \\\\ | x_i - \\bar{x}| &lt; \\sigma}}^{n} (x_i - \\bar{x})^4}_{\\text{A}} + \\frac{1}{n \\sigma^4_{\\vec{x}} } \\underbrace{\\sum\\limits_{\\substack{i = 1 \\\\ \\\\ | x_i - \\bar{x}| &gt; \\sigma}}^{n} (x_i - \\bar{x})^4}_{\\text{B}} \\] Notamos que la única parte importante que apota a la curtosis es la dada por B que es la que capta las colas de la distribución (pues ese lado es \\(\\gg 1\\)) . De ahí que podamos decir que, entre dos vectores de datos, uno tiene colas más pesadas que el otro si su curtosis es mayor. En este caso podemos analizar la latitud y longitud de los datos a través de la curtosis: datos %&gt;% summarise(kurtosis(latitud, na.rm = T), kurtosis(longitud, na.rm = T)) ## kurtosis(latitud, na.rm = T) kurtosis(longitud, na.rm = T) ## 1 2.857934 3.045037 donde se agregó el comando na.rm = T para eliminar los valores de no respuesta (missing) marcados como NA. Del análisis notamos que la longitud tiene colas más pesadas que la latitud. NOTACIÓN Dado un vector \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) de valores numéricos denotamos el \\(j\\)-ésimo valor muestral (\\(1 \\leq j \\leq n\\)) como \\(x_{(j)}\\) tal que \\(x_{(1)} = \\min \\{ x_1, x_2, \\dots, x_n \\}\\) y \\[ x_{(j)} = \\min \\{ x_1, x_2, \\dots, x_n \\} \\setminus \\{ x_{(1)}, x_{(2)}, \\dots, x_{(j-1)} \\} \\] Es decir \\(x_{(j)}\\) es el valor en orden \\(j\\) al momento de ordenar la muestra. Como nota adicional se define \\(x_{(0)} = 0\\) y \\(x_{(n+1)} = 0\\). Nota La curtosis a veces se define con un denominador distinto (en términos de las \\(n\\)) como en Myatt and Johnson (2007). 7. Mediana Dado un vector de valores numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) definimos la mediana como (Panaretos (2016)): \\[ \\text{Mediana}_{\\vec{x}} = \\dfrac{x_{(\\lfloor \\frac{n+1}{2} \\rfloor)} + x_{(\\lceil \\frac{n+1}{2} \\rceil)}}{2} \\] La mediana puede calcularse fácilmente haciendo: median(conteo_delitos$n) ## [1] 646 8. Cuantil Dado un vector de valores numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) el \\(\\alpha\\)-ésimo cuantil está dado por: \\[ \\text{Cuantil}_{\\vec{x}}(\\alpha) = \\dfrac{x_{(\\lfloor \\alpha\\cdot (n+1) \\rfloor)} + x_{(\\lceil \\alpha\\cdot (n+1)\\rceil)}}{2} \\] donde \\(x_{(0)} = x_{(n+1)} = 0\\). R no calcula los cuantiles de manera exacta sino que por velocidad los aproxima mediante la función quantile. Por ejemplo en el cálculo de los cuantiles \\(\\alpha = 0.1\\) y \\(\\alpha = 0.66\\): conteo_delitos %&gt;% summarise(quantile(n, c(0.1, 0.66))) ## # A tibble: 2 x 1 ## `quantile(n, c(0.1, 0.66))` ## &lt;dbl&gt; ## 1 501 ## 2 707 La función summary también es bastante útil resumiendo múltiples observaciones de la base: summary(conteo_delitos$n) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 397.0 568.0 646.0 640.7 721.0 790.0 Ésta incluye los cuartiles los cuales corresponden a los cuantiles asociados a \\(\\alpha =0.25, 0.5, 0.75\\) y \\(1\\). Nota Hay múltiples definiciones de cuantil (ver Hyndman and Fan (1996) para un intento de homologación). En particular R utiliza una distinta y tus cómputos no van a coincidir si lo haces con esta definición y con la de R. Si quieres saber más de R consulta ?quantile 9. Rango intercuartílico Definimos el rango intercuartílico (Panaretos (2016)) para valores numèricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) como la distancia entre el cuantil \\(0.75\\) y el \\(0.25\\) (primer y tercer cuartil): \\[ \\text{IQR}_{\\vec{x}} = \\text{Cuantil}_{\\vec{x}}(0.75) - \\text{Cuantil}_{\\vec{x}}(0.25) \\] IQR(conteo_delitos$n) ## [1] 153 10. Valores atípicos (outliers) Dado un vector de datos numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) seguimos a Panaretos (2016) para definir los valores atípicos outliers como aquellas observaciones: \\[ \\textrm{Outliers}_{\\vec{x}} = \\Big\\{ x_i \\in \\vec{x} \\big| x_i \\not\\in \\big[ \\text{Cuantil}_{\\vec{x}}(0.25) - \\frac{3}{2} \\text{IQR}_{\\vec{x}}, \\text{Cuantil}_{\\vec{x}}(0.75) + \\frac{3}{2} \\text{IQR}_{\\vec{x}}\\big] \\Big\\} \\] Los outliers en esta definción son valores que serían verdaderamente improbables bajo una distribución normal. Particularmente en el caso de la normal los outliers son valores que tienen una probabilidad de salir aproximadamente de 0.0069766 (por eso son atípicos porque no se esperaría que aparecieran nunca). Para identificar los outliers calculamos el IQR primero y los cuartiles: iqr &lt;- IQR(conteo_delitos$n) cuartil1 &lt;- quantile(conteo_delitos$n, 0.25) cuartil3 &lt;- quantile(conteo_delitos$n, 0.75) después identificamos el límite inferior y superior del conjunto lim.inf &lt;- cuartil1 - 3/2*iqr lim.sup &lt;- cuartil3 + 3/2*iqr finalmente preguntamos por cuáles están antes o después: outliers &lt;- conteo_delitos %&gt;% filter(n &lt; lim.inf | n &gt; lim.sup) En este caso no tenemos outliers. NOTA Según la aplicación que tenemos la definición de outlier cambia. La actual es la que se utiliza para datos que pudieran ser descritos mediante una Normal; empero, no siempre esta definición de outlier es un buen modelo (por ejemplo en datos como ingreso que son cantidades positivas, con mucha asimetría y cola pesada). Un buen tratamiento sobre los outliers puedes encontrarlo en SURI, Murty, and Athithan (2019). 11. Rango El rango (Peck, Olsen, and Devore (2015)) se define como la diferencia entre el mínimo y el máximo de los valores de un vector numérico \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\): \\[ \\textrm{Rango}_{\\vec{x}} = \\max \\{x_1, x_2, \\dots, x_n\\} - \\min \\{x_1, x_2, \\dots, x_n\\} \\] En R puede calcularse con la resta: #Obtenemos máximo y mínimo maximo &lt;- max(conteo_delitos$n) minimo &lt;- min(conteo_delitos$n) #Rango maximo - minimo ## [1] 393 Nota En algunos casos el rango se refiere al intervalo \\([a,b]\\) de valores donde \\(a = \\min \\{x_1, x_2, \\dots, x_n\\}\\) y \\(b = \\max \\{x_1, x_2, \\dots, x_n\\}\\). Éste es el caso de la función range en R: range(conteo_delitos$n) ## [1] 397 790 12. Conteo asociado a un conjunto Sea \\(\\vec{y} = (y_1, y_2, \\dots, y_n)^T\\) un vector de datos de cualquier tipo (numéricos, categóricos, lógicos, caracteres, etc). Para un conjunto \\(A\\) definimos el conteo asociado al conjunto \\(A\\) como: \\[ \\text{Conteo}_{\\vec{y}}(A) = \\sum\\limits_{i = 1}^{n} \\mathbb{I}_A (y_i) \\] donde \\[ \\mathbb{I}_A (y) = \\begin{cases} 1 &amp; \\text{ si } y \\in A, \\\\ 0 &amp; \\text{ en otro caso }, \\end{cases} \\] es una variable indicadora. Una forma rápida de obtener dicho conteo en R es mediante table: table(datos$delito) ## ## ABANDONO DE PERSONA ABORTO ABUSO DE AUTORIDAD ABUSO DE CONFIANZA ## 53 15 102 276 ## ABUSO SEXUAL ACOSO SEXUAL ## 252 30 O bien si se desean contar en la base de datos por ejemplo los delitos de ABANDONO DE PERSONA pueden hacerse mediante un filtro. datos %&gt;% filter(delito == &quot;ABANDONO DE PERSONA&quot;) %&gt;% tally() ## n ## 1 53 Al filtro pueden agregárseles grupos por si se desea obtener por fecha: datos %&gt;% filter(delito == &quot;ABANDONO DE PERSONA&quot;) %&gt;% group_by(fecha) %&gt;% tally() ## # A tibble: 21 x 2 ## fecha n ## &lt;date&gt; &lt;int&gt; ## 1 2018-12-01 3 ## 2 2018-12-02 3 ## 3 2018-12-04 2 ## 4 2018-12-05 8 ## 5 2018-12-06 1 ## 6 2018-12-07 1 ## 7 2018-12-10 1 ## 8 2018-12-12 2 ## 9 2018-12-13 3 ## 10 2018-12-14 2 ## # … with 11 more rows El filtro funciona igual que un if pudiéndose usar (&amp;) u (|): datos %&gt;% filter(delito == &quot;ABANDONO DE PERSONA&quot; | delito == &quot;ABORTO&quot;) %&gt;% group_by(fecha) %&gt;% tally() ## # A tibble: 25 x 2 ## fecha n ## &lt;date&gt; &lt;int&gt; ## 1 2018-12-01 3 ## 2 2018-12-02 3 ## 3 2018-12-04 5 ## 4 2018-12-05 8 ## 5 2018-12-06 1 ## 6 2018-12-07 2 ## 7 2018-12-10 1 ## 8 2018-12-12 2 ## 9 2018-12-13 4 ## 10 2018-12-14 2 ## # … with 15 more rows datos %&gt;% filter(delito == &quot;ABANDONO DE PERSONA&quot; &amp; fiscalía == &quot;INVESTIGACIÓN EN IZTAPALAPA&quot;) %&gt;% group_by(fecha) %&gt;% tally() ## # A tibble: 3 x 2 ## fecha n ## &lt;date&gt; &lt;int&gt; ## 1 2018-12-02 1 ## 2 2018-12-13 1 ## 3 2018-12-20 1 13. Moda En términos simples, la moda es el conjunto de los valores que más se repiten. Matemáticamente (ver Peck, Olsen, and Devore (2015)) la moda es el conjunto \\(\\textrm{Moda}_{\\vec{y}} = \\{ m_1, m_2, \\dots, m_k \\}\\) tal que \\(m \\in \\textrm{Moda}\\) sí y sólo si \\[ \\sum_{i = 1}^{n} \\mathbb{I}_{\\{m\\}}(y_i) \\geq \\sum_{i = 1}^{n} \\mathbb{I}_{\\{ \\ell\\}}(y_i) \\qquad \\forall \\ell \\neq m \\textrm{ donde } y_i \\in \\vec{y}. \\] Para calcularla en R no existe una función predefinida para calcular la moda. Nosotros podemos crearla con el comando function. El término function nos sirve para construir funciones; por ejemplo, una función que eleva al cuadrado: elevar.cuadrado &lt;- function(x){ return(x^2) } Observa la estructura que siempre será de esta forma: nombre de la función &lt;- function(parámetro, otro parámetro){ #Lo que sea que haga return(lo que devuelve) } Podemos llamar a la función con un número: elevar.cuadrado(8) ## [1] 64 o bien con un vector: elevar.cuadrado(12) ## [1] 144 En nuestro caso vamos a crear una función que se llame moda para estimar la moda: #Función para estimar la moda de un vector x moda &lt;- function(x){ #Contar cuántas veces aparecen las observaciones conteo &lt;- table(x) #Obtengo el máximo que aparece max_aparece &lt;- max(conteo) #Busco cuáles aparecen más y obtengo los nombres moda &lt;- names(conteo)[which(conteo == max_aparece)] #Finalmente checo que si los datos eran numéricos moda debe #ser numérico if (is.numeric(x)){ moda &lt;- as.numeric(moda) } return(moda) } Podemos probar nuestra función con datos que ya sepamos su resultado nada más para asegurarnos que funciona: #Creamos un vector numérico con dos modas vector.ejemplo.1 &lt;- c(1,6,6,1,2,7,8,10) moda(vector.ejemplo.1) ## [1] 1 6 Podemos probarlo también con caracteres: #Creamos un vector numérico con dos modas vector.ejemplo.2 &lt;- c(&quot;manzana&quot;,&quot;pera&quot;,&quot;guayaba&quot;,&quot;perejil&quot;,&quot;manzana&quot;) moda(vector.ejemplo.2) ## [1] &quot;manzana&quot; Una vez sabemos funciona podemos buscar el delito que ocurrió más: moda(datos$delito) ## [1] &quot;VIOLENCIA FAMILIAR&quot; 2.7 Ejercicios Construye una función que tome de input dos variables: \\(x\\) un vector y \\(k\\) un entero de tal manera que calcule el \\(k\\)-ésimo momento central de los datos: \\[ \\text{Momento}_{\\vec{x}}(k) = \\frac{1}{n} \\sum\\limits_{i=1}^n (x_i - \\bar{x})^k \\] La función debe tener la siguiente estructura: kesimo.momento &lt;- function(x, k){ #Rellena aquí } Sin usar la opción de trim ni trimmed.mean crea una función que calcule la media de los datos que están entre el cuantil \\(\\alpha/2\\) y el cuantil \\(1 - \\alpha/2\\) (\\(0 \\leq \\alpha \\leq 1\\). A esta media se le conoce como media truncada al nivel \\(\\alpha \\times 100\\%\\). Matemáticamente se define como: \\[ \\textrm{Media Truncada}_{\\vec{x}}(\\alpha) = \\frac{1}{n_\\alpha} \\sum\\limits_{i = 1}^{n} x_i \\cdot \\mathbb{I}_{[q_{\\alpha/2}, q_{1-\\alpha/2}]}(x_i) \\] donde \\(n_{\\alpha} = \\sum_{i=1}^n \\mathbb{I}_{[q_{\\alpha/2}, q_{1-\\alpha/2}]}(x_i)\\) es la cantidad de \\(x_i\\) que están en el intervalo \\([q_{\\alpha/2}, q_{1-\\alpha/2}]\\) donde \\(q_{\\alpha/2} = \\text{Cuantil}_{\\vec{x}}(\\alpha/2)\\) y \\(q_{1 - \\alpha/2} = \\text{Cuantil}_{\\vec{x}}(1 - \\alpha/2)\\). Una función llamada jesimo.dato de dos argumentos que dado un vector de datos \\(\\vec{x}\\) me devuelva el \\(j\\)-ésimo dato ordenado (es decir el \\(x_{(j)}\\)). NOTA No confundir con devolver el \\(x_j\\) que es la \\(j\\)-ésima entrada. Como sugerencia usar arrange, order ó sort. Un ejemplo de lo que debe hacer la función es: x &lt;- c(12,8,9,7,14, 21) jesimo.dato(x, 4) ## [1] 12 2.8 Gráficas univariadas 1. Gráfica de caja (boxplot) Una gráfica de caja pretende resumir los cuartiles, la mediana e identificar los outliers todo en una sola imagen (Panaretos (2016)). Para ello considera un vector numérico \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) tal que: \\(q_1\\) sea el primer cuartil (\\(\\textrm{Cuantil}_{\\vec{x}}(0.25)\\)), \\(q_2\\) sea la mediana (que es lo mismo que el segundo cuartil o bien \\(\\textrm{Cuantil}_{\\vec{x}}(0.5)\\)) y \\(q_3\\) corresponda al tercer cuartil (\\(\\textrm{Cuantil}_{\\vec{x}}(0.75)\\)). \\(w_1 = \\min \\{x_j \\in \\vec{x} | x_j \\geq q_1 - \\frac{3}{2} IQR \\}\\) es el valor más pequeño de \\(\\vec{x}\\) que no es outlier y \\(w_2 = \\max \\{x_j \\in \\vec{x} | x_j \\leq q_3 + \\frac{3}{2} IQR \\}\\) es el valor más grande de \\(\\vec{x}\\) que no es outlier. Sea \\(\\textrm{Outliers}_{\\vec{x}}\\) el conjunto de outliers como lo definimos anteriormente: \\[ \\textrm{Outliers}_{\\vec{x}} = \\Big\\{ x_i \\in \\vec{x} \\big| x_i \\not\\in \\big[ q_1 - \\frac{3}{2} \\text{IQR}_{\\vec{x}}, q_3 + \\frac{3}{2} \\text{IQR}_{\\vec{x}}\\big] \\Big\\} \\] donde \\(\\textrm{Outliers}_{\\vec{x}} = \\{ o_1, o_2, \\dots, o_d \\}\\). Una gráfica de caja corresponde al siguiente diagrama: La imagen anota la mediana, los cuartiles así como el rango de valores donde se sabe que no hay outliers. Finalmente la gráfica identifica los outliers si es que hay. Para armar una gráfica de boxplot usamos la librería de ggplot2 especificando dentro de la función ggplot la base de datos de donde sale nuestra información: ggplot(conteo_delitos) + geom_boxplot(aes(x = n)) la cual pone la mediana en 646 como habíamos calculado, los cuartiles en 568 y 721 respectivamente. Finalmente no presenta outliers pues nuestro análisis previo nos mostraba que no había outliers. Podemos personalizar nuestra gráfica agregando títulos con la función lab: ggplot(conteo_delitos) + geom_boxplot(aes(x = n)) + labs( x = &quot;Cantidad de carpetas de investigación abiertas por día&quot;, y = &quot;&quot;, title = &quot;Gráfica de cajas de los delitos en CDMX&quot;, subtitle = &quot;Fuente: Carpetas de investigación FGJ de la Ciudad de México&quot;, caption = &quot;Datos de Diciembre 2018&quot; ) Finalmente, podemos personalizar los colores de la gráfica editando directamente en el geom_boxplot: ggplot(conteo_delitos) + geom_boxplot(aes(x = n), color = &quot;red&quot;, fill = &quot;deepskyblue4&quot;) + labs( x = &quot;Cantidad de carpetas de investigación abiertas por día&quot;, y = &quot;&quot;, title = &quot;Gráfica de cajas de los delitos en CDMX&quot;, subtitle = &quot;Fuente: Carpetas de investigación FGJ de la Ciudad de México&quot;, caption = &quot;Datos de Diciembre 2018&quot; ) 2. Gráfica de barras Sea \\(\\vec{c} = (c_1, c_2, \\dots, c_n)^T\\) un vector de datos categóricos. Sea \\(C = \\{ a_i | a_i \\in \\vec{c} \\}\\) el conjunto de \\(\\ell\\) valores únicos que se tienen registrados en el vector \\(\\vec{c}\\). Denotamos la cantidad de veces que aparece \\(a_i\\) en \\(\\vec{c}\\) como \\(n_i\\); es decir: \\[ n_i = \\sum\\limits_{k = 1}^n \\mathbb{I}_{\\{a_i\\}}(c_k) \\] Una gráfica de barras consiste en una representación gráfica del conjunto: \\[ \\text{Barras} = \\{ (a_i, n_i) | a_i \\in C \\} \\] Gráficamente: Podemos crear una gráfica de barras con el comando geom_col para ello creemos unas barras correspondientes al tipo de delito (sólo en delitos que categoria_delito dice ROBO) haciendo una nueva base que cuente por delito: conteo_tipo &lt;- datos %&gt;% filter(str_detect(categoria_delito,&quot;ROBO&quot;)) %&gt;% group_by(delito) %&gt;% tally() Y hagamos la gráfica: ggplot(conteo_tipo) + geom_col(aes(x = delito, y = n), color = &quot;white&quot;) + theme_bw() Para evitar que se encime todo el texto podemos establecer un ángulo del mismo al usar theme: ggplot(conteo_tipo) + geom_col(aes(x = delito, y = n), color = &quot;white&quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 90, size = 3)) NOTA Una mala praxis es usar gráficas de pay pues es muy complicado contar una historia a partir de ellas. ¡No lo hagas! 2.9 Gráficas bivariadas 1. Gráfica de puntos (scatterplot) Dada una matriz de datos \\(Z\\) consideramos dos columnas numéricas \\(z_i\\) y \\(z_j\\) (\\(i \\neq j\\)) de dicha matriz. Sea \\(\\mathbb{X} = \\{ (z_{i,1}, z_{j,1}), (z_{i,2}, z_{j,2}), \\dots, (z_{i,n}, z_{j,n}) \\}\\) el conjunto de parejas ordenadas correspondientes a dichas columnas. Una gráfica de puntos consiste en la proyección de dichos puntos sobre \\(\\mathbb{R}^2\\). Para generarla en R podemos usar ggplot: ggplot(datos) + geom_point(aes(x = longitud, y = latitud), size = 1, color = &quot;purple&quot;, alpha = 0.2) donde los parámetros size establecen el tamaño del punto, color su color y alpha su nivel de transparencia (\\(0 \\leq \\alpha \\leq 1\\)). 2. Gráfica de líneas (lineplot) Dada una matriz de datos \\(Z\\) consideramos dos columnas numéricas \\(z_i\\) y \\(z_j\\) (\\(i \\neq j\\)) de dicha matriz. Sea \\(\\mathbb{X} = \\{ (z_{i,1}, z_{j,1}), (z_{i,2}, z_{j,2}), \\dots, (z_{i,n}, z_{j,n}) \\}\\) el conjunto de parejas ordenadas correspondientes a dichas columnas. Para evitar confusión de subíndices escribiré a las \\(z_i\\) como \\(x\\) y a las \\(z_j\\) como \\(y\\) de tal forma que \\(\\mathbb{X} = \\{ (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n) \\}\\) Supongamos, sin pérdida de generalidad que los datos están ordenados según las \\(x\\): \\(x_1 \\leq x_2 \\leq \\dots \\leq x_n\\). Sea \\(f\\) la función de interpolación lineal dada por: \\[ f(x) = \\begin{cases} y_1 + \\frac{y_2 - y_1}{x_2 - x_1} (x -x_1) &amp; \\text{ si } x_1 \\leq x \\leq x_2 \\\\ \\vdots \\\\ y_{k-1} + \\frac{y_k - y_{k-1}}{x_k - x_{k-1}} (x -x_{k-1}) &amp; \\text{ si } x_{k-1} \\leq x \\leq x_k \\\\ \\vdots \\\\ y_{n-1} + \\frac{y_{n} - y_{n-1}}{x_n - x_{n-1}} (x -x_{n-1}) &amp; \\text{ si } x_{n-1} \\leq x \\leq x_n \\\\ \\end{cases} \\] Una gráfica de líneas corresponde a la representación gráfica del conjunto \\[ \\textrm{Gr}_f = \\Big\\{ \\big(x, f(x)\\big) | x_1 \\leq x \\leq x_n \\Big\\} \\] De manera un poco más intuitiva notamos que si tenemos, por ejemplo, \\(\\mathbb{X} = \\{(x_1, y_1),(x_2, y_2), (x_3, y_3), (x_4, y_4)\\}\\) una gráfica de líneas se construye interpolando una línea entre \\((x_1, y_1)\\) y \\((x_2, y_2)\\), otra línea entre \\((x_2, y_2)\\) y \\((x_3, y_3)\\) y, finalmente, otra recta entre \\((x_3, y_3)\\) y \\((x_4, y_4)\\). Usando la ecuación de la línea \\[ y = \\frac{y_2 - y_1}{x_2 - x_1} (x - x_1) + y_1 \\] interpolamos cada uno de los puntos como en la gráfica siguiente: Para realizar una gráfica de líneas podemos usar de nuevo ggplot2 con la opción de geom_line: ggplot(conteo_delitos) + geom_line(aes(x = fecha, y = n)) Podemos cambiar el tema y agregar puntos de otro color para que nuestra gráfica se vea más bonita: ggplot(conteo_delitos) + geom_line(aes(x = fecha, y = n)) + geom_point(aes(x = fecha, y = n), color = &quot;red&quot;, size = 3) + theme_classic() + labs( x = &quot;Fecha de apertura de la carpeta de investigación&quot;, y = &quot;Cantidad de carpetas de investigación en FGJ&quot; ) Finalmente con geom_label podemos agregar anotaciones a nuestra gráfica: ggplot(conteo_delitos) + geom_line(aes(x = fecha, y = n)) + geom_point(aes(x = fecha, y = n), color = &quot;red&quot;, size = 3) + theme_classic() + labs( x = &quot;Fecha de apertura de la carpeta de investigación&quot;, y = &quot;Cantidad de carpetas de investigación en FGJ&quot; ) + geom_label(aes(x = dmy(&quot;25/12/2018&quot;), y = 425), label = &quot;Efecto de Navidad&quot;) 2.9.1 Ejercicio Utiliza las siguiente bases de datos para replicar exactamente el formato de las gráficas que se muestran abajo de las bases. No todo viene en estas notas, la idea es que investigues y para ello te sugiero consultar este libro Gráfica de barras datos.barras &lt;- data.frame(Pais = c(&quot;EEUU&quot;,&quot;Canadá&quot;,&quot;México&quot;), PIB = c(20.54, 17.13, 1.21)) Los colores usados son firebrick, deepskyblue3 y forestgreen: Línea x &lt;- seq(-2*pi, 2*pi, length.out = 100) datos.linea &lt;- data.frame(x = x, y = sin(x)) Boxplot x &lt;- c(1,10, 100, -2, 3, 5, 6, 12, -8, 31, 2, pi, 3) datos.linea &lt;- data.frame(Dientes = x) Puntos datos.arbol &lt;- data.frame(altura = c(1.7, 1.4, 1.8, 1.9, 1.5, 1.7, 1.6, 1.8, 1.7, 1.8), ancho = c(1.2, 1.4, 1.2, 1, 1.5, 1.7, 1.6, 1.2, 1.2, 1), tipo = c(&quot;Pino&quot;,&quot;Sauce&quot;,&quot;Sauce&quot;,&quot;Sauce&quot;,&quot;Pino&quot;, &quot;Pino&quot;,&quot;Pino&quot;,&quot;Sauce&quot;,&quot;Sauce&quot;,&quot;Sauce&quot;)) 2.10 Estadísticos bivariados NOTACIÓN Para esta sección vamos a considerar dos (vectores) columnas de la matriz de datos \\(Z\\) y los denominaremos \\(\\vec{x}\\) y \\(\\vec{y}\\) (en lugar de \\(z_i\\) y \\(z_j\\)). En particular, denotaremos \\(\\mathcal{X} = \\{ a_{i, x } | a_{i,x} \\in \\vec{x} \\}\\) el conjunto de valores únicos del vector \\(\\vec{x}\\) y \\(\\mathcal{Y} = \\{ a_{y ,j} | a_{y ,j} \\in \\vec{y} \\}\\) el conjunto de valores únicos de \\(\\vec{y}\\). La cardinalidad de dichos conjuntos es \\(\\ell_{x}\\) y \\(\\ell_{y}\\) respectivamente. Finalmente, definimos el conteo de cuántas veces aparece el valor \\(a_{i,x }\\) (respectivamente el \\(a_{y ,j}\\)) en los vectores \\(\\vec{x}\\) (respectivamente \\(\\vec{y}\\)) como: \\[\\begin{equation} \\begin{aligned} n_{i,x } &amp; = \\sum\\limits_{k=1}^{n} \\mathbb{I}_{\\{ a_{i,x } \\}}(x_k) \\\\ n_{y ,j} &amp; = \\sum\\limits_{k=1}^{n} \\mathbb{I}_{\\{ a_{y ,j} \\}}(y_k) \\end{aligned} \\end{equation}\\] para \\(1 \\leq i \\leq \\ell_{x}\\) y \\(1 \\leq j \\leq \\ell_{y}\\). Por poner un ejemplo, considera el siguiente conjunto de datos: x y Rojo Coche Azul Taza Verde Árbol Rojo Taza Verde Libro En este sentido el vector es \\(\\vec{x} = (\\text{Rojo},\\text{Azul},\\text{Verde},\\text{Rojo},\\text{Verde})^T\\) mientras que el conjunto de valores únicos asociados está dado por \\(\\mathcal{X} = \\{ \\text{Rojo},\\text{Azul},\\text{Verde} \\}\\). En este sentido (siguiendo el conjunto) se tiene que \\(a_{1,x} = \\text{Rojo}\\), \\(a_{2,x} = \\text{Azul}\\) y \\(a_{3,x } = \\text{Verde}\\) mientras que (siguiendo el vector) se observa \\(x_1 = \\text{Rojo}\\), \\(x_2 = \\text{Azul}\\), \\(x_3 = \\text{Verde}\\), \\(x_4 = \\text{Rojo}\\), \\(x_5 = \\text{Verde}\\). Finalmente notamos que el conteo de veces que aparece cada cosa es: \\(n_{1,x} = 2\\) (aparece el \\(a_{1, x}\\) que es rojo dos veces), \\(n_{2,x} = 1\\) y \\(n_{3,x } = 2\\) (el azul y verde dados por \\(a_{2,x}\\) y \\(a_{3,x}\\) respectivamente aparecen una vez para azul y dos veces para verde). Por otro lado, \\(\\vec{y} = (\\text{Coche},\\text{Taza},\\text{Árbol},\\text{Taza},\\text{Libro})^T\\) con su conjunto de valores únicos \\(\\mathcal{Y} = \\{ \\text{Coche},\\text{Taza},\\text{Árbol}, \\text{Libro} \\}\\). Para el caso de \\(\\vec{y}\\) se tiene que \\(y_1 = \\text{Coche}\\), \\(y_2 = \\text{Taza}\\), \\(y_3 = \\text{Árbol}\\), \\(y_4 = \\text{Taza}\\), \\(y_5 = \\text{Libro}\\) mientras que en el caso de valores únicos \\(a_{y, 1} = \\text{Coche}\\), \\(a_{y, 2} = \\text{Taza}\\), \\(a_{y, 3} = \\text{Árbol}\\), \\(a_{\\cdot, 4} = \\text{Libro}\\). Los conteos asociados son: \\(n_{y,1} = n_{y,3} = n_{y,4} = 1\\) (aparecen el coche, el árbol y el libro una vez) mientras que \\(n_{y,2} = 2\\) representa que la taza está dos veces. Por otro lado denotamos a la submatriz de \\(Z\\) compuesta solamente por las columnas \\(\\vec{x}\\) y \\(\\vec{y}\\) como: \\[ Z_{(x,y)} = \\begin{pmatrix} x_1 &amp; y_1 \\\\ x_2 &amp; y_2 \\\\ \\vdots &amp; \\vdots \\\\ x_n &amp; y_n \\\\ \\end{pmatrix} \\] Sea \\(\\mathcal{X}\\times\\mathcal{Y} = \\{ a_{i,j} = (x_i,y_j) | x_i \\in \\mathcal{X} \\quad \\&amp; \\quad y_j \\in \\mathcal{Y}\\}\\) el conjunto de observaciones únicas posibles de las parejas \\((x,y)\\) (todas las permutaciones). Finalmente, el conteo de cuántas veces aparece el vector bivariado \\(a_{i,j}\\) en los datos está dado por: \\[ n_{i,j} = \\sum\\limits_{k = 1}^{n} \\mathbb{I}_{\\{ a_{i,j} \\}}\\big( (x_k, y_k) \\big) \\] En el ejemplo anterior, la tabla se vería: ## Warning: Setting row names on a tibble is deprecated. Coche Taza Árbol Libro Total (fila) Rojo 1 1 0 0 2 Azul 0 1 0 0 1 Verde 0 0 1 1 2 Total (columna) 1 2 1 1 5 Una excelente referencia para esta sección es el capítulo 4 de Peck, Olsen, and Devore (2015). 1. Tabla de contingencia Para \\(\\vec{x}\\), \\(\\vec{y}\\) definidas como al inicio de la sección (y siguiendo la notación anterior), definimos una tabla de contingencia como la matriz \\(N_{x,y}\\) dada por: \\[ N_{x,y} = \\begin{pmatrix} n_{1,1} &amp; n_{1,2} &amp; \\dots &amp; n_{1, \\ell_y} \\\\ n_{2,1} &amp; n_{2,2} &amp; \\dots &amp; n_{2, \\ell_y} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ n_{\\ell_x,1} &amp; n_{\\ell_x,2} &amp; \\dots &amp; n_{\\ell_x, \\ell_y} \\\\ \\end{pmatrix} \\] Al vector \\(n_x = (n_{1,x}, n_{2,x}, \\dots, n_{\\ell_x,x})^T\\) se le conoce como distribución frecuencial (observada) marginal de \\(\\vec{x}\\) mientras que \\(n_y = (n_{y,1}, n_{y,2}, \\dots, n_{y,\\ell_y})^T\\) es la distribución frecuencial (observada) marginal de \\(\\vec{y}\\). Una tabla de contingencia representa el conteo de observaciones de una variable ajustado por la otra. Para crear una tabla de contingencia en R podemos usar el mismo comando table que ya usamos antes pero esta vez introduciendo dos vectores como en el siguiente ejemplo donde notamos alcaldía contra año del registo: table(datos$alcaldia_hechos, datos$ao_inicio) ## ## 2018 2019 ## VERACRUZ 0 1 ## VILLAGRAN 1 0 ## XALATLACO 2 0 ## XOCHIMILCO 465 115 ## XOCHITEPEC 1 0 ## ZACATECAS 0 2 Para agregar las distribuciones frecuenciales marginales a la tabla podemos usar el comando addmargins: addmargins(table(datos$alcaldia_hechos, datos$ao_inicio)) ## ## 2018 2019 Sum ## VILLAGRAN 1 0 1 ## XALATLACO 2 0 2 ## XOCHIMILCO 465 115 580 ## XOCHITEPEC 1 0 1 ## ZACATECAS 0 2 2 ## Sum 15952 3896 19848 2. Tabla de frecuencias Una tabla de frecuencia es la matriz \\(\\text{Freq}_{x,y}\\) dada por: \\[ \\text{Freq}_{x,y} = \\begin{pmatrix} f_{1,1} &amp; f_{1,2} &amp; \\dots &amp; f_{1, \\ell_y} \\\\ f_{2,1} &amp; f_{2,2} &amp; \\dots &amp; f_{2, \\ell_y} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ f_{\\ell_x,1} &amp; f_{\\ell_x,2} &amp; \\dots &amp; f_{\\ell_x, \\ell_y} \\\\ \\end{pmatrix} \\] donde \\(f_{i,j} = \\frac{n_{i,j}}{n}\\) representa la frecuencia relativa de la observación de \\((a_{i,x}, a_{y,j})\\) i.e. cuánto representa del total. Al vector \\(f_{x} = (f_{1,x}, f_{2,x}, \\dots, f_{\\ell_x,x})^T\\) se le conoce como la distribución frecuencial marginal relativa de \\(\\vec{x}\\). Análogamente para \\(y\\) se tiene la distribución frecuencial marginal relativa de \\(\\vec{y}\\) dada por: \\(f_{y} = (f_{y,1}, f_{y,2}, \\dots, f_{y,\\ell_y})^T\\). Las entradas de dichos vectores son de la forma \\(f_{i,x} = n_{i,x}/n\\) y \\(f_{y,j} = n_{y,j}/n\\). En R podemos obtener las frecuencias mediante prop.table: prop.table(table(datos$alcaldia_hechos, datos$ao_inicio)) ## ## 2018 2019 ## VERACRUZ 0.000000e+00 5.038291e-05 ## VILLAGRAN 5.038291e-05 0.000000e+00 ## XALATLACO 1.007658e-04 0.000000e+00 ## XOCHIMILCO 2.342805e-02 5.794035e-03 ## XOCHITEPEC 5.038291e-05 0.000000e+00 ## ZACATECAS 0.000000e+00 1.007658e-04 Así mismo, podemos agregar las marginales: addmargins(prop.table(table(datos$alcaldia_hechos, datos$ao_inicio))) ## ## 2018 2019 Sum ## VILLAGRAN 5.038291e-05 0.000000e+00 5.038291e-05 ## XALATLACO 1.007658e-04 0.000000e+00 1.007658e-04 ## XOCHIMILCO 2.342805e-02 5.794035e-03 2.922209e-02 ## XOCHITEPEC 5.038291e-05 0.000000e+00 5.038291e-05 ## ZACATECAS 0.000000e+00 1.007658e-04 1.007658e-04 ## Sum 8.037082e-01 1.962918e-01 1.000000e+00 3. Riesgo Relativo (discreto) Para definir Riesgo Relativo empezaremos por un ejemplo. Tomamos la tabla donde se guardó un registro de personas según si fumaban o no así como si dichas personas desarrollaron o no enfisema pulmonar. ## Warning: Setting row names on a tibble is deprecated. FUMA NO FUMA Con enfisema 100 40 Sin enfisema 30 50 Si quisiéramos analizar la hipótesis de que FUMAR está asociado con ENFISEMA tendríamos que ver, dentro de la población de fumadores (FUMAR = SÍ) cuántos hay (proporcionalmente) con ENFISEMA. La hipótesis es que si no hubiera relación, saldría que las proporciones de fumadores con y sin enfisema serían \\(50\\%\\) cada una. La proporción de fumadores con enfisema está dada por \\(100/130\\) mientras que la de no fumadores con enfisema es \\(40/90\\). El riesgo relativo (intuitivamente). se define como la división entre ambas proporciones: \\[ \\text{Riesgo Relativo de Enfisema} = \\dfrac{\\frac{\\text{Expuestos enfermos}}{\\text{Total de expuestos}}}{\\frac{\\text{No Expuestos enfermos}}{\\text{Total de no expuestos}}} = \\dfrac{100/130}{40/90} \\approx 1.73 \\] Lo que se interpreta como que los fumadores tienen \\(1.73\\) veces más riesgo de desarrollar enfisema que los no fumadores ya que si despejamos de la fórmula anterior: \\[ \\frac{\\text{Expuestos enfermos}}{\\text{Total de expuestos}} \\approx 1.73 \\times \\frac{\\text{No Expuestos enfermos}}{\\text{Total de no expuestos}} \\] De manera general, dadas dos vectores lógicos \\(\\vec{x}\\) (interpretada como el resultado) y \\(\\vec{y}\\) (interpretada como la exposición) con una tabla de contingencia y frecuencias marginales dadas por la tabla: ## Warning: Setting row names on a tibble is deprecated. Expuesto (y) NO expuesto (y) Resultado (x) a b Sin resultado (x) c d definimos el riesgo relativo de \\(\\vec{x}\\) dado \\(\\vec{y}\\) como: \\[ RR(\\vec{x}|\\vec{y}) = \\dfrac{\\frac{a}{a + c}}{\\frac{b}{b + d}} \\] Mientras que el riesgo relativo de no \\(\\vec{x}\\) dado \\(\\vec{y}\\) está dado por: \\[ RR(\\neg \\vec{x}|\\vec{y}) = \\dfrac{\\frac{c}{a + c}}{\\frac{d}{b + d}} \\] La base de datos de los delitos no contiene información suficiente para poder calcular un riesgo relativo pero podemos crear la base de datos correspondiente a la tabla como sigue: fumadores &lt;- data.frame(SI_FUMA = c(100, 30), NO_FUMA =c(40, 50)) Podemos agregar nombres a las filas para tener la base de datos mejor: rownames(fumadores) &lt;- c(&quot;ENFISEMA&quot;,&quot;NO_ENFISEMA&quot;) La tabla se ve así: fumadores ## SI_FUMA NO_FUMA ## ENFISEMA 100 40 ## NO_ENFISEMA 30 50 Luego el riesgo relativo de ENFISEMA está dado por: numerador &lt;- fumadores[&quot;ENFISEMA&quot;,&quot;SI_FUMA&quot;]/sum(fumadores$SI_FUMA) denominador &lt;- fumadores[&quot;ENFISEMA&quot;,&quot;NO_FUMA&quot;]/sum(fumadores$NO_FUMA) rr &lt;- numerador/denominador #El riesgo relativo rr ## [1] 1.730769 Por otro lado, el riesgo relativo de no enfisema es: numerador &lt;- fumadores[&quot;NO_ENFISEMA&quot;,&quot;SI_FUMA&quot;]/sum(fumadores$SI_FUMA) denominador &lt;- fumadores[&quot;NO_ENFISEMA&quot;,&quot;NO_FUMA&quot;]/sum(fumadores$NO_FUMA) rr_neg &lt;- numerador/denominador #El riesgo relativo rr_neg ## [1] 0.4153846 Éste último se interpreta como si la proporción de individuos sin enfisema es \\(0.41\\) veces más pequeña entre fumadores que no fumadores. 4. Razón de momios (discreto) Para dos vectores lógicos \\(\\vec{x}\\) y \\(\\vec{y}\\) definimos la razón de momios como: \\[ \\textrm{OR}(\\vec{x}|\\vec{y}) =\\dfrac{RR(\\vec{x}|\\vec{y})}{RR(\\neg\\vec{x}|\\vec{y})} \\] Podemos calcular en R la razón de momios a partir de los datos: razon.momios &lt;- rr/rr_neg donde la razón de momios de 4.17 se interpreta como “si un individuo tiene enfisema, la factibilidad de que dicho individuo sea fumador es 4.17 veces más alta”. Esta interpretación se obtiene a partir de un despeje y sustitución: \\[\\begin{equation}\\nonumber \\begin{aligned} RR(\\vec{x}|\\vec{y}) &amp; = 4.16 \\cdot RR(\\neg\\vec{x}|\\vec{y}) \\\\ \\\\ \\Leftrightarrow \\dfrac{\\frac{\\text{Expuestos enfermos}}{\\text{Total de expuestos}}}{\\frac{\\text{No Expuestos enfermos}}{\\text{Total de no expuestos}}} &amp; = 4.16 \\cdot \\dfrac{\\frac{\\text{Expuestos no enfermos}}{\\text{Total de expuestos}}}{\\frac{\\text{No Expuestos no enfermos}}{\\text{Total de no expuestos}}} \\\\ \\\\ \\Leftrightarrow \\frac{\\text{Expuestos enfermos}}{\\text{No Expuestos enfermos}} &amp; = 4.16\\cdot \\frac{\\text{Expuestos no enfermos}}{\\text{No Expuestos no enfermos}} \\\\ \\\\ \\Leftrightarrow \\frac{\\text{Expuestos enfermos}}{\\text{Expuestos no enfermos}} &amp; = 4.16\\cdot \\frac{\\text{No Expuestos enfermos}}{\\text{No Expuestos no enfermos}} \\end{aligned} \\end{equation}\\] 5. Correlación (Bravais-Pearson) Sean \\(\\vec{x}\\) y \\(\\vec{y}\\) dos vectores columa numéricos de nuestra matriz de datos \\(Z\\). Tomemos \\(\\tilde{x} = (x_1 - \\bar{x}, x_2 - \\bar{x}, \\dots, x_n - \\bar{x})\\) la versión centrada de \\(\\vec{x}\\) y \\(\\tilde{y} = (y_1 - \\bar{y}, y_2 - \\bar{y}, \\dots, y_n - \\bar{y})\\) la versión centrada de \\(\\vec{y}\\). Al coseno entre dichos vectores (bajo el producto punto) se le conoce como correlación de Bravais-Pearson y se le denota \\(\\rho_{\\vec{x},\\vec{y}}\\). Es decir: \\[ \\rho_{\\vec{x},\\vec{y}} = \\cos(\\tilde{x},\\tilde{y}) = \\dfrac{\\tilde{x} \\cdot \\tilde{y}}{\\|\\tilde{x}\\| \\cdot \\|\\tilde{y}\\|} \\] donde \\(\\tilde{x}\\cdot\\tilde{y} = \\sum_{i=1}^{n} (x_i - \\bar{x}) \\cdot (y_i - \\bar{y})\\) representa el producto de los vectores \\(\\tilde{x}\\) y \\(\\tilde{y}\\) y se conoce como covarianza entre \\(\\vec{x}\\) y \\(\\vec{y}\\). Por otro lado, \\[ \\|\\tilde{x}\\| = \\sqrt{\\sum\\limits_{i=1}^{n} (x_i - \\bar{x})^2} = \\sigma_{\\vec{x}} \\] Por tanto la correlación también puede medirse como: \\[ \\rho_{\\vec{x},\\vec{y}} = \\cos(\\tilde{x},\\tilde{y}) = \\frac{1}{\\sigma_{\\vec{y}} \\sigma_{\\vec{x}}}\\sum\\limits_{i=1}^{n} (x_i - \\bar{x}) \\cdot (y_i - \\bar{y}) \\] Para matriz de datos \\(Z\\) con \\(\\ell\\) columnas, definimos la matriz de correlaciones \\(\\mathcal{C}\\) como la matriz dada por: \\[ \\mathcal{C} = \\begin{pmatrix} \\rho(z_1,z_1) &amp; \\rho(z_1,z_2) &amp; \\dots &amp; \\rho(z_1,z_{\\ell}) \\\\ \\rho(z_2,z_1) &amp; \\rho(z_2,z_2) &amp; \\dots &amp; \\rho(z_2,z_{\\ell}) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\rho(z_{\\ell},z_1) &amp; \\rho(z_{\\ell},z_2) &amp; \\dots &amp; \\rho(z_{\\ell},z_{\\ell}) \\end{pmatrix} \\] Donde notamos (demuestra) que \\(\\rho(z_i, z_i) = 1\\). Podemos usar la base mtcars precargada en R para analizar las correlaciones: data(mtcars) datos.coches &lt;- mtcars La base está explicada en la ayuda de R: ?mtcars Podemos obtener la correlación entre el número de millas por galón mpg y el peso del automóvil wt haciendo: cor(datos.coches$mpg, datos.coches$wt, method = &quot;pearson&quot;) ## [1] -0.8676594 Esta correlación se interpreta como que por cada aumento en el peso corresponde una disminución en las millas por galón. Podemos ver gráficamente que esto es así: ggplot(datos.coches) + geom_point(aes(x = wt, y = mpg), color = &quot;purple&quot;) + theme_bw() Para obtener toda la matriz de correlaciones de la base podemos tomar cor aplicado a toda la base de datos: cor(datos.coches, method = &quot;pearson&quot;) ## mpg cyl disp hp drat wt ## mpg 1.0000000 -0.8521620 -0.8475514 -0.7761684 0.68117191 -0.8676594 ## cyl -0.8521620 1.0000000 0.9020329 0.8324475 -0.69993811 0.7824958 ## disp -0.8475514 0.9020329 1.0000000 0.7909486 -0.71021393 0.8879799 ## hp -0.7761684 0.8324475 0.7909486 1.0000000 -0.44875912 0.6587479 ## drat 0.6811719 -0.6999381 -0.7102139 -0.4487591 1.00000000 -0.7124406 ## wt -0.8676594 0.7824958 0.8879799 0.6587479 -0.71244065 1.0000000 ## qsec 0.4186840 -0.5912421 -0.4336979 -0.7082234 0.09120476 -0.1747159 ## vs 0.6640389 -0.8108118 -0.7104159 -0.7230967 0.44027846 -0.5549157 ## am 0.5998324 -0.5226070 -0.5912270 -0.2432043 0.71271113 -0.6924953 ## gear 0.4802848 -0.4926866 -0.5555692 -0.1257043 0.69961013 -0.5832870 ## carb -0.5509251 0.5269883 0.3949769 0.7498125 -0.09078980 0.4276059 ## qsec vs am gear carb ## mpg 0.41868403 0.6640389 0.59983243 0.4802848 -0.55092507 ## cyl -0.59124207 -0.8108118 -0.52260705 -0.4926866 0.52698829 ## disp -0.43369788 -0.7104159 -0.59122704 -0.5555692 0.39497686 ## hp -0.70822339 -0.7230967 -0.24320426 -0.1257043 0.74981247 ## drat 0.09120476 0.4402785 0.71271113 0.6996101 -0.09078980 ## wt -0.17471588 -0.5549157 -0.69249526 -0.5832870 0.42760594 ## qsec 1.00000000 0.7445354 -0.22986086 -0.2126822 -0.65624923 ## vs 0.74453544 1.0000000 0.16834512 0.2060233 -0.56960714 ## am -0.22986086 0.1683451 1.00000000 0.7940588 0.05753435 ## gear -0.21268223 0.2060233 0.79405876 1.0000000 0.27407284 ## carb -0.65624923 -0.5696071 0.05753435 0.2740728 1.00000000 Finalmente, el paquete ggcorrplot puede ayudarnos a visualizar gráficamente dicha matriz: ggcorrplot(cor(datos.coches, method = &quot;pearson&quot;), lab = TRUE, type = &quot;upper&quot;) + labs(title = &quot;Matriz de Correlaciones&quot;) Una correlación de Pearson igual a \\(1\\) ó \\(-1\\) se interpreta como que hay una relación lineal perfecta mientras que una correlación igual a \\(0\\) se interpreta como que no hay relación lineal (aunque puede existir de otro tipo) #Ejemplo de correlación lineal perfecta x &lt;- seq(0,4, length.out = 9) y &lt;- 2*x + 1 Gráficamente: El valor en este caso de la correlación es: cor(x,y, method = &quot;pearson&quot;) ## [1] 1 Mientras que por otro lado podemos tener variables relacionadas pero sin correlación : #Ejemplo sin correlación lineal pero con variables relacionadas x &lt;- seq(-4, 4,length.out = 9) y &lt;- x^2 cor(x,y, method = &quot;pearson&quot;) ## [1] 0 6. Correlación de rango de Spearman Para hablar de la correlación de rango de Spearman es necesario definir una variable como ordinal. Un vector \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) de variables numéricas o categóricas es ordinal si existe una relación \\(\\leq\\) de orden total sobre los elementos del vector tal que: Es antisimétrica: si \\(x_i \\leq x_j\\) y \\(x_j \\leq x_i\\) entonces \\(x_i = x_j\\). Es transitiva: si \\(x_i \\leq x_j\\) y \\(x_j \\leq x_k\\) entonces \\(x_i \\leq x_k\\). Es conexa: \\(x_i \\leq x_j\\) ó \\(x_j \\leq x_i\\). De manera intuitiva un vector es ordinal si hay un orden para sus entradas. Por ejemplo, cuando calificas un servicio como Malo \\(\\leq\\) Regular \\(\\leq\\) Bueno o bien cuando se compara nivel educativo (en términos de años) Primaria \\(\\leq\\) Secundaria \\(\\leq\\) Preparatoria \\(\\leq\\) Educación superior. Toda variable numérica es ordinal. Para un vector ordinal definimos su ordenamiento como \\(x_{(1)} = \\min \\{ x_1, x_2, \\dots, x_n \\}\\) y \\(x_{(j)} = \\min \\{ x_1, x_2, \\dots, x_n \\} \\setminus \\{ x_{(1)}, x_{(2)}, \\dots, x_{(j-1)} \\}\\) de tal forma que \\(x_{(1)} \\leq x_{(2)} \\leq \\dots \\leq x_{(n)}\\). El rango de \\(x_{(j)}\\) denotado como \\(R(x_{(j)})\\) es \\(j\\) (su posición en el ordenamiento). Es decir: \\[ R(x_i) = j \\Leftrightarrow x_i = x_{(j)} \\] Dado un vector \\(\\vec{x}\\) definimos su vector de rango como: \\[ R(\\vec{x}) = \\big( R(x_1), R(x_2), \\dots, R(x_n) )^T \\] Para dos variables ordinales, \\(\\vec{x}\\) y \\(\\vec{y}\\) se define la correlación de rango de Spearman como la correlación de Pearson entre sus vectores de rangos: \\[ \\rho_{\\text{Spearman}} =\\rho\\big( R(\\vec{x}), R(\\vec{y})) \\] Mientras que la correlación de Pearson mide linealidad; la de Spearman mide monotonicidad (que si una aumenta la otra también; que si una disminuye la otra también). #Comparativo de correlaciones: la de Pearson no encuentra mucha línea x &lt;- seq(0.1, 1, length.out = 25) y &lt;- exp(1/x^2) En este caso la correlación de Pearson es muy mala: cor(x,y, method = &quot;pearson&quot;) ## [1] -0.3396831 Mientras que la de Spearman sí muestra la relación: cor(x,y, method = &quot;spearman&quot;) ## [1] -1 7. \\(\\tau\\) de Kendall Consideremos \\(\\vec{x}\\) y \\(\\vec{y}\\) dos vectores columna ordinales de una matriz de datos \\(Z\\). Para cualquier par de observaciones \\((x_i, y_i)\\) y \\((x_j, y_j)\\) con \\(i &lt; j\\) decimos que dos observaciones son concordantes (\\(c\\)) si los rangos de ambas \\(x\\) y \\(y\\) coinciden; es decir si se cumple una de las siguientes: \\(R(x_i) &lt; R(x_j)\\) y \\(R(y_i) &lt; R(y_j)\\) o bien, \\(R(x_i) &gt; R(x_j)\\) y \\(R(y_i) &gt; R(y_j)\\). Observaciones discordantes (\\(d\\)) ocurren cuando los rangos de las \\(x\\) y las \\(y\\) son inversos el uno del otro; es decir, se cumple una de las siguientes: \\(R(x_i) &gt; R(x_j)\\) y \\(R(y_i) &lt; R(y_j)\\) o bien, \\(R(x_i) &lt; R(x_j)\\) y \\(R(y_i) &gt; R(y_j)\\). En el caso que cualquiera de las dos, \\(x\\) ó \\(y\\) sean igualdades (\\(x_i = x_j\\) ó \\(y_i = y_j\\)) no son discordantes ni concordantes. Observa que existen \\(\\binom{n}{2}\\) distintos pares de \\((x_i,y_i)\\) y \\((x_j,y_j)\\) para comparar. Sea \\(c_{\\vec{x},\\vec{y}}\\) la cantidad de pares concordantes y \\(d_{\\vec{x},\\vec{y}}\\) la cantidad de pares discordantes. Luego la probabilidad de que dos pares seleccionados de manera uniforme sean concordantes es: \\[ \\dfrac{c_{\\vec{x},\\vec{y}}}{\\binom{n}{2}} \\] mientras que la probabilidad de que dos pares seleccionados uniformemente sean discordantes es: \\[ \\dfrac{d_{\\vec{x},\\vec{y}}}{\\binom{n}{2}}. \\] Definimos entonces la \\(\\tau\\) de Kendall como la diferencia entre ambas probabilidades empíricas: \\[ \\tau_{\\vec{x},\\vec{y}} = \\dfrac{c_{\\vec{x},\\vec{y}} - d_{\\vec{x},\\vec{y}}}{\\binom{n}{2}} \\] La tau de Kendall cumple que: \\[ -1 \\leq \\tau_{\\vec{x},\\vec{y}} \\leq 1 \\] donde el \\(-1\\) se alcanza sólo si son completamente discordantes (el rango de \\(x\\) es el inverso del rango de las \\(y\\)) y el \\(1\\) si son completamente concordantes (el rango de \\(x\\) y de \\(y\\) tienen el mismo orden). Una \\(\\tau\\) cercana a cero se interpreta como ausencia de relación en los rangos. Podemos aplicar la tau de Kendall a la siguiente base de datos que contiene la calificación de dos servicios de un restaurante: calidad_alimentos calidad_servicio Malo 1 estrella Bueno 4 estrellas Bueno 5 estrellas Regular 2 estrellas Bueno 5 estrellas Bueno 4 estrellas Para ello codificamos las variables como factor diciéndole que son variables ordinales order = TRUE e indicando el orden de los niveles: calidad_alimentos &lt;- factor(c(&quot;Malo&quot;,&quot;Bueno&quot;,&quot;Bueno&quot;,&quot;Regular&quot;,&quot;Bueno&quot;,&quot;Bueno&quot;), order = TRUE, levels = c(&quot;Malo&quot;,&quot;Regular&quot;,&quot;Bueno&quot;)) calidad_servicio &lt;- factor(c(&quot;1 estrella&quot;, &quot;4 estrellas&quot;, &quot;5 estrellas&quot;, &quot;2 estrellas&quot;, &quot;5 estrellas&quot;,&quot;4 estrellas&quot;), order = TRUE, levels = c(&quot;1 estrella&quot;,&quot;2 estrellas&quot;,&quot;3 estrellas&quot;, &quot;4 estrellas&quot;,&quot;5 estrellas&quot;)) Esto de las variables ordinales permite hacer comparaciones ordinales, por ejemplo: calidad_alimentos[2] &gt; calidad_alimentos[4] ## [1] TRUE Los datos se ven así: Finalmente, calculamos la \\(\\tau\\) de Kendall, para ello es necesario obtener el rango de nuestras variables ordinales: rango_alimentos &lt;- as.numeric(calidad_alimentos) rango_servicio &lt;- as.numeric(calidad_servicio) cor(rango_alimentos, rango_servicio, method = &quot;kendall&quot;) ## [1] 0.8320503 Lo cual indica que hay una relación entre la calificación de calidad de alimentos y la del servicio. 8. Ajuste de modelo lineal Sean \\(\\vec{x}\\) y \\(\\vec{y}\\) dos vectores columna de una matriz de datos \\(Z\\). Supongamos, además, se tiene la hipótesis de que existe una relación afín entre los vectores; es decir que: \\[ \\vec{y} \\approx \\beta_1 \\vec{x} + \\beta_0 \\vec{1} \\] donde \\(\\vec{1} = (1, 1, \\dots, 1)^T\\) es un vector con todas las entradas idénticas a \\(1\\) y \\(\\beta_0, \\beta_1 \\in \\mathbb{R}\\). Algunas razónes para tener esta hipótesis podría ser una correlación de Pearson cercana a \\(\\pm 1\\) o por inspección gráfica. Esta hipótesis implica que: \\[ y \\approx \\underbrace{\\beta_1 x + \\beta_0}_{\\hat{y}} \\] Podemos entonces trazar la línea \\(y = \\beta_0 + \\beta_1 x\\) y graficar contra los puntos \\(\\{(x_i,y_i)\\}_{i=1}^{n}\\), Si la línea no ajusta perfecto tendremos errores \\(e_i = (y_i - \\hat{y}_i)^2\\) de predicción las cuales representan la diferencia entre la \\(y\\) observada (\\(y_i\\)) y la \\(y\\) predicha por la línea \\(\\hat{y}_i = \\beta_1 x_i + \\beta_0\\). La suma de estos errores es: \\[ \\textrm{SSR}(\\beta_0, \\beta_1) = \\sum\\limits_{i=1}^{n} e_i = \\sum\\limits_{i=1}^{n} \\big( y_i - \\hat{y}_i \\big)^2 = \\sum\\limits_{i=1}^{n} \\big( y_i - (\\beta_1 x_i + \\beta_0) \\big)^2 \\] El nombre de SSR es por ( Sum of Squared Residuals ) dado que en estadística se define un residual como \\(r_i = (y_i - \\hat{y}_i)\\) Gráficamente: Lo que se busca entonces es minimizar el error respecto a las constantes a determinar: \\(\\beta_0\\) y \\(\\beta_1\\). Para ello buscamos un punto de inflexión derivando: \\[ \\dfrac{\\partial\\textrm{SSR}}{\\partial \\beta_0} = \\sum\\limits_{i=1}^{n} 2\\big(y_i - (\\beta_1 x_i + \\beta_0 ) \\big) = 0 \\] De donde se sigue que: \\[ \\sum\\limits_{i=1}^n y_i - \\beta_1 \\sum\\limits_{i=1}^{n} x_i - n \\beta_0 = 0 \\Rightarrow \\beta_0 = \\dfrac{1}{n} \\sum\\limits_{i=1}^n y_i - \\beta_1 \\dfrac{1}{n} \\sum\\limits_{i=1}^n x_i \\Rightarrow \\bar{y} - \\beta_1 \\bar{x}, \\] de donde concluimos que de cumplirse la relación lineal se tiene que: \\[ \\beta_0 = \\bar{y} - \\beta_1 \\bar{x}. \\] Por otro lado, la derivada respecto a \\(\\beta_1\\) es: \\[ \\dfrac{\\partial\\textrm{SSR}}{\\partial \\beta_1} = - \\sum\\limits_{i=1}^{n} 2\\big(y_i - (\\beta_1 x_i + \\beta_0 ) \\big) \\cdot x_i = 0 \\] De donde se sigue (si suponemos que existe al menos un \\(x_i \\neq 0\\)): \\[\\begin{equation}\\nonumber \\begin{aligned} 0 &amp; = - \\sum\\limits_{i=1}^n \\Big( x_i y_i - \\beta_1 x_i^2 - \\underbrace{\\beta_0}_{\\bar{y} - \\beta_1 \\bar{x}} x_i \\Big) \\\\ &amp; = \\sum\\limits_{i=1}^n \\Big( x_i y_i - \\beta_1 x_i^2 - \\bar{y}x_i + \\beta_1 \\bar{x} x_i \\Big) \\\\ &amp; = \\sum\\limits_{i=1}^n \\Big( y_i + \\beta_1 x_i - \\bar{y} - \\beta_1 \\bar{x} \\Big) x_i \\\\ &amp; = \\sum\\limits_{i=1}^n \\Big( y_i - \\bar{y} \\Big) x_i - \\beta_1 \\sum\\limits_{i=1}^n\\Big( x_i - \\bar{x} \\Big) x_i \\end{aligned} \\end{equation}\\] de donde se sigue (suponiendo que existen \\(i,j\\) tales que \\(x_i \\neq x_j\\) que: \\[ \\beta_1 = \\dfrac{\\sum_{i=1}^n \\Big( y_i - \\bar{y} \\Big)x_i}{\\sum_{i=1}^n\\Big( x_i - \\bar{x} \\Big)x_i} = \\dfrac{\\sum_{i=1}^n \\Big( y_i - \\bar{y} \\Big)\\Big( x_i - \\bar{x} \\Big)}{\\sum_{i=1}^n\\Big( x_i - \\bar{x} \\Big)^2} = \\dfrac{\\sigma_{\\vec{x}} \\cdot \\sigma_{\\vec{y}} \\cdot \\rho_{\\vec{x},\\vec{y}}}{n \\sigma_{\\vec{x}}^2} \\] por lo cual: \\[ \\beta_1 = \\dfrac{\\sigma_{\\vec{y}}}{\\sigma_{\\vec{x}}} \\cdot \\dfrac{\\rho_{\\vec{x},\\vec{y}}}{n} \\] De donde se tienen las fórmulas para el \\(\\beta_0\\) y \\(\\beta_1\\). 2.10.1 Ejercicio Demuestra la igualdad que usamos anteriormente: \\[ \\dfrac{\\sum_{i=1}^n \\Big( y_i - \\bar{y} \\Big) x_i}{\\sum_{i=1}^n\\Big( x_i - \\bar{x} \\Big)x_i} = \\dfrac{\\sum_{i=1}^n \\Big( y_i - \\bar{y} \\Big)\\Big( x_i - \\bar{x} \\Big)}{\\sum_{i=1}^n\\Big( x_i - \\bar{x} \\Big)^2} \\] En R podemos ajustar un modelo lineal para dos variables de una base de datos con lm: modelo.lineal &lt;- lm(mpg ~ wt, data = datos.coches) coef(modelo.lineal) ## (Intercept) wt ## 37.285126 -5.344472 Gráficamente podemos ver el modelo: ggplot(datos.coches) + geom_point(aes(y = mpg, x = wt)) + geom_smooth(aes(y = mpg, x = wt), method = &quot;lm&quot;, formula = y ~ x, se = FALSE) + theme_minimal() Para predecir, dada una nueva observación, cuál debe haber sido el valor de \\(\\hat{y}\\) para una nueva observación \\(x_*\\) (o varias nuevas observaciones) puede usarse la función predict datos_a_predecir &lt;- data.frame(wt = c(5.5, 6, 6.5)) predict(modelo.lineal, datos_a_predecir) ## 1 2 3 ## 7.890533 5.218297 2.546061 Hay que tener mucho cuidado con la generalización de un modelo lineal como los siguientes valores muestran: datos_a_predecir &lt;- data.frame(wt = c(7,8,9)) predict(modelo.lineal, datos_a_predecir) ## 1 2 3 ## -0.1261748 -5.4706464 -10.8151180 O bien el siguiente comic de xkcd: Para hacer la extrapolación gráfica podemos agregar un fullrange = TRUE combinado con un xlim ggplot(datos.coches) + geom_point(aes(y = mpg, x = wt)) + geom_smooth(aes(y = mpg, x = wt), method = &quot;lm&quot;, formula = y ~ x, se = FALSE, fullrange=TRUE) + theme_minimal() + xlim(c(1,6.5)) 2.11 Ejercicio Generaliza el proceso de estimación para cuando se tiene un polinomio \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\) Utiliza los datos confirmados de COVID-19 a nivel nacional (sólo los confirmados) disponibles en este link. Ajusta un modelo cuadrático (en el lm la fórmula ahora es del estilo de y ~ poly(x,2) ) y predice cuántos casos confirmados habrá el 29 de junio. Grafica tu ajuste así como tu predicción en la misma gráfica. 9. Ajuste general Podemos generalizar el ajuste de mínimos cuadrados planteando el modelo \\(y = f(x, \\vec{\\beta})\\) donde \\(x\\) puede ser una matriz y \\(\\vec{\\beta}\\) es un vector de parámetros. Supondremos que \\(f\\) es diferenciable en \\(\\vec{\\beta}\\). Como ejemplo, en el caso del ajuste lineal: \\[ y = f(x, \\vec{\\beta}) = \\beta_0 + \\beta_1 x \\qquad \\text{con} \\qquad \\vec{\\beta}= (\\beta_0,\\beta_1)^T. \\] o bien podríamos pensar en un ajuste polinomial: \\[ y = f(x, \\vec{\\beta}) = \\sum\\limits_{i = 0}^n \\beta_i x^i \\qquad \\text{con} \\qquad \\vec{\\beta}= (\\beta_0,\\beta_1, \\dots, \\beta_n)^T. \\] No tiene que ser un polinomio, \\(f\\) puede ser lo que ella quiera ser siempre y cuando sea diferenciable en los parámetros: \\[ y = f(x, \\vec{\\beta}) = \\Bigg[\\cos(\\beta_0 + x) + \\int\\limits_{0}^{\\beta_1 x} e^{-t^2} dx \\Bigg] \\cdot \\beta_2 \\ln(x) \\qquad \\text{con} \\qquad \\vec{\\beta}= (\\beta_0,\\beta_1, \\beta_3)^T. \\] 2.12 Ajuste funcional Hacemos una apuesta por teléfono. Yo voy a tirar una moneda \\(10\\) veces y si salen más Soles que Águilas yo gano 50 pesos. Si salen más Águilas que Soles tú ganas la misma cantidad. Al realizar el ejercicio yo te comunico que salieron en total \\(10\\) Soles y por tanto me debes el dinero. ¿Sospecharías algo de mí? Si no hablamos de probabilidad no hay forma en la que se pueda justificar que aparentemente hay algo raro con la moneda. Claro, siempre puede ser un caso improbable (hay gente que lo ha hecho) pero es raro que me hayan salido tantos Soles. Para cuantificar qué tan raro es el evento podemos suponer que las monedas siguen un modelo Binomial con parámetro \\(p = 1/2\\) y en este caso \\(n = 10\\) (fueron 10 tiros). La probabilidad de que haya obtenido \\(10\\) soles bajo este modelo es de: dbinom(10,10, 1/2) ## [1] 0.0009765625 ¡Rarísimo! Este resultado te haría sospechar que quizá mi moneda no es justa y no se obtienen la misma cantidad de Águilas que Soles cuando la tiro. Esto porque, aparentemente, en mi moneda la probabilidad de Sol debería de ser \\(p = 1\\) (por tu triste experiencia). Si por ejemplo en el onceavo tiro saliera un Águila, concluirías que, en mi moneda, aparentemente, la probabilidad de Sol es \\(p = \\frac{10}{11}\\). Por supuesto, entre más tiros y más información obtienes, mejor podrás caracterizar la moneda y con mayor sustento tendrás sospechas (o no) de que mi moneda es tramposa. Formalmente, en el ejemplo anterior, lo que se hace es suponer que existe una variable aleatoria \\(X \\in\\{ \\text{Águila}, \\text{Sol}\\}\\) (el resultado de la moneda) de la cual observamos \\(n = 11\\) realizaciones codificadas en el siguiente vector: \\[ \\vec{x} = \\big( \\text{Sol}, \\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Águila}\\big)^T \\] Aproximamos entonces la probabilidad \\(\\mathbb{P}(X = \\text{Sol})\\) mediante: \\[ \\mathbb{P}(X = \\text{Sol}) \\approx \\dfrac{1}{n}\\sum\\limits_{i=1}^n \\mathbb{I}_{\\{\\text{Sol}\\}}(x_i) = \\dfrac{10}{11} \\] Mientras que la de Águila se aproxima mediante: \\[ \\mathbb{P}(X = \\text{Águila}) \\approx \\dfrac{1}{n}\\sum\\limits_{i=1}^n \\mathbb{I}_{\\{\\text{Águila}\\}}(x_i) = \\dfrac{1}{11} \\] Para ver que éstas son buenas aproximaciones, podemos considerar un vector aleatorio de los posibles datos observados: \\[ \\vec{X} = (X_1, X_2, \\dots, X_{11})^T \\] Donde \\(X_1\\) es una variable aleatoria que representa lo que pudo haber salido en el primer tiro, \\(X_2\\) es una v.a. que representa lo que pudo haber salido en el segundo tiro y en general \\(X_k\\) es una v.a. que representa lo que pudo haber salido en el \\(k\\)-ésimo tiro. Suponiendo que la moneda tiene una probabilidad \\(p\\) de arrojar Sol y \\(1-p\\) de arrojar Águila, notamos que las variables indicadoras evaluadas en las \\(X_i\\) (aleatorias) son variables aleatorias \\[ \\mathbb{I}_{\\{\\text{Sol}\\}}(X_i) \\sim \\text{Beroulli}(p) \\] y que por tanto \\[ \\hat{p} = \\dfrac{1}{n}\\sum\\limits_{i=1}^n \\mathbb{I}_{\\{\\text{Sol}\\}}(X_i) \\] es una variable aleatoria (al ser suma de variables aleatorias). Podemos entonces calcular su valor esperado: \\[ \\mathbb{E}\\big[\\hat{p}\\big] = \\mathbb{E}\\bigg[\\dfrac{1}{n}\\sum\\limits_{i=1}^n \\mathbb{I}_{\\{\\text{Sol}\\}}(X_i)\\bigg] = \\dfrac{1}{n}\\sum\\limits_{i=1}^n \\mathbb{E}\\big[ \\mathbb{I}_{\\{\\text{Sol}\\}}(X_i)\\big] = \\dfrac{1}{n}\\sum\\limits_{i=1}^n p = \\dfrac{1}{n}\\cdot np = p \\] Es decir, que en promedio el estimador \\(\\hat{p}\\) va a atinarle al verdadero valor \\(p\\). Esto lo podemos ver si hacemos nsim\\(= 1000\\) simulaciones de \\(100\\) tiros de una moneda con probabilidad p\\(= 8/10\\) de sol. nsim &lt;- 1000 tiros &lt;- 100 p.val &lt;- 8/10 #Creamos un vector para guardar los valores de p gorro p.gorro &lt;- rep(NA, nsim) #Loop recorriendo cada una de las nsim simulaciones for (i in 1:nsim){ experimento &lt;- sample(c(&quot;Sol&quot;,&quot;Águila&quot;), tiros, replace = TRUE, prob = c(p.val, 1 - p.val)) soles &lt;- table(experimento)[&quot;Sol&quot;] p.gorro[i] &lt;- soles/tiros } Podemos ver que en promedio le atinamos al valor verdadero: #Vemos que en promedio le atina: mean(p.gorro) ## [1] 0.80131 Lo mismo podemos verlo gráficamente: #Graficamos ggplot() + geom_point(aes(x = 1:nsim, y = p.gorro, color = as.character(p.gorro)), size = 2, alpha = 0.2) + geom_hline(aes(yintercept = p.val), size = 1.5, linetype = &quot;solid&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;) + labs( x = &quot;Simulaciones&quot;, y = &quot;Estimación de p&quot;, title = &quot;Simulación de proceso de estimación\\nde que una moneda caiga Sol&quot; ) + geom_label(aes(x = nsim/2, y = p.val), label = &quot;Verdadero valor de p&quot;) ¿Qué significa esto? El que en promedio \\(\\hat{p}\\) sea \\(p\\) (formalmente, que \\(\\mathbb{E}\\big[\\hat{p}\\big] = p\\)) significa que, si yo hago muchísimos experimentos (o procesos de muestreo) de la misma cosa, mi \\(\\hat{p}\\) es un buen estimador porque en promedio le va a atinar. Empero, esto no dice nada de qué tan bueno es mi estimador \\(\\hat{p}\\) para mi caso (mi muestra o mi experimento) específico. Puedes pensarlo con los exámenes: que alguien tenga un promedio de 8 dice que en general le ha ido bien en los exámenes, pero no dice nada respecto al primer examen de cálculo que hizo (donde pudo tener \\(10\\) ó \\(5\\) para llegar a ese promedio de \\(8\\) pero no podemos saber de manera específica cuánto fue ). Esto es igual: en promedio el estimador \\(\\hat{p}\\) será \\(p\\) pero para un análisis específico no sabemos. OJO Los datos observados no son variables aleatorias: esos ya son fijos, ya los viste. Los posibles datos observados sí son variables aleatorias ya que ellos, consisten en las variables que se pudieron haber observado y te permiten calcular las probabilidades de tus datos observados bajo algún modelo. En el caso de la moneda, los datos observados son \\(\\vec{x} = \\big( \\text{Sol}, \\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Águila}\\big)^T\\) pero los que se pudieron haber observado son todas las \\(\\binom{n}{2}\\) formas en las que la moneda pudo haber salido. 1. Estimación de una función de masa de probabilidad Formalmente, para una variable aleatoria discreta \\(X\\) que puede tomar los valores \\(\\{ a_1, a_2, \\dots, a_{\\ell} \\}\\) de la cual se observaron \\(n\\) realizaciones descritas mediante \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) (observados, fijos, constantes). Definimos la función de masa de probabilidad empírica como: \\[ \\hat{p}(x) = \\begin{cases} \\\\ \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}_{\\{a_1\\}}(x_i) &amp; \\text{ si } x = a_1 \\\\ \\\\ \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}_{\\{a_2\\}}(x_i) &amp; \\text{ si } x = a_2 \\\\ \\\\ \\vdots \\\\ \\\\ \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}_{\\{a_{\\ell}\\}}(x_i) &amp; \\text{ si } x = a_{\\ell} \\\\ 0 &amp; \\text{ en otro caso} \\end{cases} \\] donde se supone que \\(\\mathbb{P}(X = x) \\approx \\hat{p}(x)\\). Notamos que lo anterior puede resumirse en: \\[ \\hat{p}(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}_{\\{ x\\}}(x_i) \\] Análogamente, nota que para un conjunto (medible) \\(A\\), la aproximación para \\(\\mathbb{P}(X \\in A)\\) está dada por: \\[ \\hat{p}(A) = \\dfrac{1}{n}\\sum\\limits_{i=1}^{n} \\mathbb{I}_A (x_i). \\] Podemos graficar para la base de datos conteo_delitos la probabilidad de que, dado que se cometió un delito, éste haya ocurrido en el dia \\(d_i\\) de diciembre. Para ello usamos un geom_col: ggplot(conteo_delitos) + geom_col(aes(x = fecha, y = n/sum(n), fill = n)) + scale_fill_gradient(&quot;Delito&quot;, low = &quot;orange&quot;, high = &quot;red&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) + labs( y = &quot;p(x)&quot;, x = &quot;x&quot;, title = &quot;Aproximación a p(x)&quot; ) Una propiedad interesante de la función de masa de probabilidad es que, en promedio, le atina al verdadero valor (lo que comentábamos antes de que \\(\\hat{p} = p\\)). Es decir, suponiendo que \\(X\\) tiene una función de masa dada por: \\[ p(x) = \\begin{cases} p_1 &amp; \\text{ si } x = a_1 \\\\ p_2 &amp; \\text{ si } x = a_2 \\\\ \\vdots \\\\ p_{\\ell} &amp; \\text{ si } x = a_{\\ell} \\\\ \\end{cases} \\] y suponiendo un vector de muestras posibles \\(\\vec{X} = (X_1, X_2, \\dots, X_n)^T\\) notamos que \\[ \\mathbb{I}_{\\{ a_j \\}}(X_i) \\sim \\text{Bernoulli} (p_j) \\] Luego para cualquier \\(x\\) se tiene que: \\[ \\mathbb{E}\\big[ \\hat{p}(x)\\big] = \\mathbb{E}\\bigg[ \\dfrac{1}{n}\\sum\\limits_{i=1}^{n} \\mathbb{I}_{\\{ x \\}}(X_i) \\bigg] = \\dfrac{1}{n}\\sum\\limits_{i=1}^{n}\\mathbb{E}\\big[ \\mathbb{I}_{\\{ x \\}}(X_i) \\big] = \\frac{1}{n} n \\cdot p_j = p_j. \\] 2. Función de distribución empírica Recuerda que para cualquier variable aleatoria \\(X:\\mathbb{R}\\to\\mathbb{R}\\) existe su función de distribución \\(F_X\\) dada por: \\[ F_X(x) = \\mathbb{P}(X \\leq x) \\] La idea de la función de distribución empírica es reconstruir (a partir de los datos observados) a \\(F_X\\). Para ello, notamos que queremos estimar \\[ \\mathbb{P}(X \\leq x) \\qquad \\forall x\\in\\mathbb{R} \\] esto es equivalente a estimar: \\[ \\mathbb{P}\\big(X \\in (-\\infty, x] \\big) \\] y podemos aplicar la aproximación que usamos arriba para un conjunto \\(A\\): \\[ \\mathbb{P}\\big(X \\in (-\\infty, x] \\big) \\approx \\sum\\limits_{i=1}^n \\mathbb{I}_{(-\\infty, x]}(x_i) \\] La función de distribución empírica está definida para un vector numérico \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) por: \\[ \\hat{F} (x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\mathbb{I}_{(-\\infty, x]}(x_i) \\] La función de distribución empírica es una función de distribución pues cumple las siguientes propiedades (demuéstralo): \\(\\lim_{x \\to -\\infty} \\hat{F}(x) = 0\\) \\(\\lim_{x \\to \\infty} \\hat{F}(x) = 1\\) Si \\(x &lt; y\\) entonces \\(\\hat{F}(x) \\leq \\hat{F}(y)\\) (no decreciente) \\(\\hat{F}\\) es continua por la derecha con límites por la izquierda (càdlàg). Para demostrar 4. basta con demostrar que para \\(x_i\\) fija, la función \\(i(x) = \\mathbb{I}_{(-\\infty, x]}(x_i)\\) es continua por la derecha con límites por la izquierda pues \\(\\hat{F}(x)\\) es una suma de dichas funciones. En particular, podemos notar que la función de distribución empírica \\(\\hat{F} (x)\\) le atina a la función de distribución; es decir: \\[ \\mathbb{E}\\big[\\hat{F} (x) \\big] = F(x) \\] Para ello consideramos un vector de valores posibles \\(\\vec{X} = (X_1, X_2, \\dots, X_n)^T\\) donde las \\(X_i\\) tienen la misma distribución que \\(X\\). Y notamos que: \\[ \\mathbb{I}_{(-\\infty, x]}(X_i) \\sim \\textrm{Bernoulli}\\big(F(x)\\big) \\] pues \\(\\mathbb{I}_{(-\\infty, x]}(X_i) = 1\\) si \\(X_i \\leq x\\) y \\(\\mathbb{I}_{(-\\infty, x]}(X_i) = 0\\) si \\(X_i &gt; x\\). Luego: \\[ \\mathbb{P}\\Big( \\mathbb{I}_{(-\\infty, x]}(X_i) = 1 \\Big) = \\mathbb{P}(X_i \\leq x) = \\mathbb{P}(X\\leq x) = F(x) \\] donde la igualdad del medio se sigue de que \\(X_i\\) y \\(X\\) tienen la misma distribución. Entonces: \\[ \\mathbb{E}\\big[ \\hat{F}(x) \\big] = \\mathbb{E}\\Big[ \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} \\mathbb{I}_{(-\\infty, x]}(X_i) \\Big] = \\dfrac{1}{n}\\sum\\limits_{i=1}^{n} \\mathbb{E}\\big[ \\mathbb{I}_{(-\\infty, x]}(X_i)\\big] = \\dfrac{1}{n} n \\cdot F(x) = F(x) \\] En R podemos calcular la función de distribución empírica con el comando ecdf el cual cuenta la cantidad de observaciones y regresa una función. Así, para la base de datos conteo_delitos podemos calcular la función de distribución empírica ecdf asociada a la cantidad de delitos que se cometen en un día mediante: Fgorro &lt;- ecdf(conteo_delitos$n) De esta forma podemos calcular la probabilidad de que en un día se cometan menos de 500 delitos: Fgorro(500) ## [1] 0.09677419 O bien podemos graficar la función: x &lt;- seq(300, 1000, length.out = 100) y &lt;- Fgorro(x) ggplot() + geom_step(aes(x = x, y = y), color = &quot;red&quot;) + labs( x = &quot;Número de carpetas de investigación (x)&quot;, y = &quot;Probabilidad de que en un día\\nse abran menos de x carpetas&quot;, title = &quot;Distribución acumulada de carpetas de investigación en CDMX&quot; ) + theme_minimal() Mediante simulaciones, podemos observar que \\(\\hat{F}\\) realmente le atina a \\(F\\) como sigue: #Cantidad de simulaciones nsim &lt;- 100 #Tamaño de la muestra en cada simulacion n_muestra &lt;- 100 #Valores a evaluar la función x &lt;- seq(-5, 5, length.out = 200) #Base de datos para guardar resultados de simulaciones F_simulado &lt;- data.frame(matrix(NA, ncol = nsim, nrow = length(x))) for (i in 1:nsim){ valores_simulados &lt;- rnorm(n_muestra) F_empirica &lt;- ecdf(valores_simulados) F_simulado[,i] &lt;- F_empirica(x) } F_simulado$Valor_x &lt;- x #Cambiamos el formato de la base para graficar F_simulado &lt;- F_simulado %&gt;% pivot_longer(cols = -Valor_x) ggplot(F_simulado) + geom_step(aes(x = Valor_x, y = value, color = name), alpha = 0.1) + geom_line(aes(x = Valor_x, y = pnorm(Valor_x)), color = &quot;black&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;) + labs( x = &quot;x&quot;, y = &quot;F(x)&quot;, title = &quot;Simulaciones de funciones de distribuciones acumuladas empíricas&quot;, subtitle = &quot;Para X ~ Normal(0,1)&quot; ) 2. Histograma Para una variable aleatoria continua, la aproximación \\(\\hat{p}\\) que hicimos no funciona (la masa siempre es \\(0\\)). Por lo que es necesario analizar alternativas para estudiar la densidad si suponemos que los datos pueden modelarse mediante algo continuo. Para construir un histograma consideremos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) y una constante \\(h &gt; 0\\) llamada el ancho de banda (binwidth). Sea \\(\\{ I_j \\}\\) una colección de intervalos no vacíos de \\(\\mathbb{R}\\) tal que \\(\\cup_{j=1} I_j = \\mathbb{R}\\) e \\(I_j \\cap I_k = \\emptyset\\) (i.e. los \\(\\{ I_j \\}\\) forman una partición de \\(\\mathbb{R}\\)). Supongamos, además, los \\(I_j\\) son de la forma: \\[ I_j = \\Big[\\kappa + (j-1) h, \\kappa + jh \\Big) \\] para algún \\(\\kappa \\in \\mathbb{R}\\) fijo. Sea \\[ n_j(\\vec{x}) = \\sum\\limits_{i=1}^n \\mathbb{I}_{I_j}(x_i) \\] la cantidad de \\(x_i\\) en el intervalo \\(I_j\\). Un histograma es la gráfica de la función (ver Panaretos (2016)): \\[ \\text{hist}_{\\vec{x}}(x) = \\frac{1}{n \\cdot h} \\sum\\limits_{j} n_j(\\vec{x}) \\cdot \\mathbb{I}_{I_j}(x) \\] Una propiedad interesante de un histograma es que éste aproxima correctamente las probabilidades \\(\\mathbb{P}(X \\in I_j)\\). Para ver esto, consideramos un vector de valores posibles \\(\\vec{X} = (X_1, X_2, \\dots, X_n)^T\\) y que \\(x\\in I_j\\), luego: \\[ \\mathbb{E}\\Big[ \\int_{I_j} \\text{hist}_{\\vec{X}}(x) dx \\Big] = \\mathbb{E}\\bigg[ \\frac{1}{n \\cdot h} \\sum\\limits_{j} n_j(\\vec{X}) \\cdot \\int_{I_j} \\mathbb{I}_{I_j}(x) dx \\bigg] = \\frac{1}{n \\cdot h} \\sum\\limits_{j} \\mathbb{E}\\big[ n_j(\\vec{X}) \\big] \\cdot h = \\dfrac{1}{n}\\sum\\limits_{j} \\mathbb{E}\\big[ n_j(\\vec{X}) \\big] \\] donde las \\(n_j(\\vec{X})\\) son variables aleatorias en este caso y: \\[ \\mathbb{E}\\big[ n_j(\\vec{X})\\big] = \\sum\\limits_{i=1}^n \\mathbb{E}\\big[\\mathbb{I}_{I_j}(X_i)\\big] = \\sum\\limits_{i=1}^n\\mathbb{P}(X_i \\in I_j) = n \\mathbb{P}(X \\in I_j) \\] donde la última igualdad se da pues las \\(X_i\\) tienen la misma distribución que \\(X\\). Luego: \\[ \\mathbb{E}\\Big[ \\int_{I_j} \\text{hist}_{\\vec{X}}(x) dx \\Big] = \\mathbb{P}(X \\in I_j) \\] Es decir, el valor esperado del área bajo un histograma en un intervalo \\(I_j\\) coincide con la probabilidad de que \\(X\\) pertenezca a dicho intervalo. Gráficamente: En R podemos hacer un histograma a través de geom_histogram. En este caso lo haremos de la latitud: #En este caso binwidth = h y kappa = boundary ggplot(datos) + geom_histogram(aes(x = latitud, y = ..density..), binwidth = 0.02, boundary = -99, color = &quot;white&quot;, fill = &quot;purple&quot;) + theme_light() 2.12.1 Ejercicio Considera la siguiente base de datos (obtenida de Cross Validated): mis.datos &lt;- data.frame( A = c(3.15, 5.46, 3.28, 4.20, 1.98, 2.28, 3.12, 4.10, 3.42, 3.91, 2.06, 5.53, 5.19, 2.39, 1.88, 3.43, 5.51, 2.54, 3.64, 4.33, 4.85, 5.56, 1.89, 4.84, 5.74, 3.22, 5.52, 1.84, 4.31, 2.01, 4.01, 5.31, 2.56, 5.11, 2.58, 4.43, 4.96, 1.90, 5.60, 1.92), B = c(2.90, 5.21, 3.03, 3.95, 1.73, 2.03, 2.87, 3.85, 3.17, 3.66, 1.81, 5.28, 4.94, 2.14, 1.63, 3.18, 5.26, 2.29, 3.39, 4.08, 4.60, 5.31, 1.64, 4.59, 5.49, 2.97, 5.27, 1.59, 4.06, 1.76, 3.76, 5.06, 2.31, 4.86, 2.33, 4.18, 4.71, 1.65, 5.35, 1.67), C = c(2.65, 4.96, 2.78, 3.70, 1.48, 1.78, 2.62, 3.60, 2.92, 3.41, 1.56, 5.03, 4.69, 1.89, 1.38, 2.93, 5.01, 2.04, 3.14, 3.83, 4.35, 5.06, 1.39, 4.34, 5.24, 2.72, 5.02, 1.34, 3.81, 1.51, 3.51, 4.81, 2.06, 4.61, 2.08, 3.93, 4.46, 1.4, 5.1, 1.42), D = c(2.40, 4.71, 2.53, 3.45, 1.23, 1.53, 2.37, 3.35, 2.67, 3.16, 1.31, 4.78, 4.44, 1.64, 1.13, 2.68, 4.76, 1.79, 2.89, 3.58, 4.10, 4.81, 1.14, 4.09, 4.99, 2.47, 4.77, 1.09, 3.56, 1.26, 3.26, 4.56, 1.81, 4.36, 1.83, 3.68, 4.21, 1.15, 4.85, 1.17) ) Grafica un histograma de las variables A, B, C y D de dicha base con un ancho de banda (binwidth) igual a 1. ¿Podemos concluir la forma de la distribución a partir del histograma? Es decir ¿hay distribuciones sesgadas a la izquierda, a la derecha, uniformes, centradas o con colas pesadas? Realiza el mismo histograma pero ahora con un ancho de banda de 0.25 ¿por qué hubo cambios? Analiza la base de datos (los valores en función de la columna A) y concluye. 3. Densidad kernel Un histograma tiene muchos bemoles: en particular, es necesario decidir quién es \\(h\\) y quién \\(\\kappa\\) y no hay una regla clara de cómo hacerlo. La densidad kernel es un intento de mejorar esta situación. Para ello recordamos que si \\(X\\) es una variable aleatoria continua con densidad \\(F\\) entonces: \\[ f(x) = F&#39;(x) = \\lim_{h \\to 0} \\dfrac{F(x + h) - F(x - h)}{2h} \\] Por lo que para un \\(h\\) positiva con \\(h \\approx 0\\) tenemos que: \\[ f(x) \\approx \\dfrac{F(x + h) - F(x - h)}{2h} \\] En el caso de un vector de observaciones \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) recordamos que podemos asociar una función de distribución empírica \\(\\hat{F}\\) y por tanto obtener el estimador de Rosenblatt de la densidad \\(f\\) mediante: \\[ \\hat{f}(x) = \\dfrac{\\hat{F}(x + h) -\\hat{F}(x - h)}{2h} \\] Podemos reescribir esto como: \\[ \\hat{f}(x) = \\dfrac{1}{2nh} \\sum\\limits_{i = 1}^n \\mathbb{I}_{(x-h, x+h]}(x_i) = \\dfrac{1}{nh}\\sum\\limits_{i = 1}^n K\\Big( \\frac{x_i - x}{h}\\Big) \\] donde: \\[ K(u) = \\frac{1}{2} \\mathbb{I}_{(-1, 1]}(u) \\] se conoce como el kernel rectangular. Una vez que llegamos hasta este punto notamos que para cualquier \\(K\\) que cumple: \\(\\int\\limits_{-\\infty}^{\\infty} K(u) du = 1\\) \\(K(u) \\geq 0\\) la función \\(\\hat{f}\\) es una función de densidad. La función \\(\\hat{f}\\) se conoce como el estimador de densidad del kernel K. Algunos ejemplos de kernels \\(K\\) son: Rectangular: \\(K(u) = \\frac{1}{2} \\mathbb{I}_{(-1, 1]}(u)\\) Triangular: \\(K(u) = (1 - |u|) \\mathbb{I}_{(-1, 1]}(u)\\) Epanechnikov: \\(K(u) = \\frac{3}{4}(1 - u^2) \\mathbb{I}_{(-1, 1]}(u)\\) Gaussiano: \\(K(u) = \\frac{1}{\\sqrt{2\\pi}}\\text{exp}(-u^2/2)\\) OJO No confundir el Kernel \\(K\\) (que es una función que integra a \\(1\\)) con función de densidad kernel que es una función de los datos que utiliza un kernel y es una densidad por sí misma. En R podemos calcular la densidad kernel en n puntos con relativa facilidad mediante density: densidad_kernel &lt;- density(datos$latitud, kernel = &quot;gaussian&quot;, n = 700, na.rm = TRUE) Nota que R en automático preselecciona los valores de h mediante un criterio preprogramado de optimización. Podemos ver dicha densidad gráficamente (y compararla con un histograma): ggplot(datos) + geom_histogram(aes(x = latitud, y = ..density..), binwidth = 0.01, boundary = 19, fill = &quot;purple&quot;, color = &quot;white&quot;) + geom_density(aes(x = latitud), kernel = &quot;gaussian&quot;, size = 1.5) + theme_bw() Esto no se queda ahí, podemos generalizar el concepto de kernel a dos dimensiones para aproximar una función de densidad \\(f(x,y)\\) de dos variables aleatorias sí tenemos dos vectores \\(\\vec{x}\\) y \\(\\vec{y}\\) y calculamos: \\[ \\hat{f}(x,y) =\\dfrac{1}{nh^2} \\sum\\limits_{i = 1}^n K \\Big( \\frac{x_i - x}{h}\\Big) K \\Big( \\frac{y_i - y}{h}\\Big) \\] En particular esto nos permite generar una densidad en R para saber en qué coordenadas de latitud y longitud ocurren más los delitos: ggplot(datos) + geom_point(aes(x = longitud, y = latitud), alpha = 0.025) + geom_density_2d_filled(aes(x = longitud, y = latitud), alpha = 0.75) + geom_density2d(aes(x = longitud, y = latitud)) + theme_void() 2.12.2 Ejercicio sugerido Este ejercicio es para que tengas la seguridad de que comprendiste los conceptos previos y sabes calcularlos. Es tedioso pero bueno para aclarar dudas. Considera la siguiente base de datos: x y z w 1 -100 Rojo Bueno 2 -2 Azul Malo 3 2 Azul Regular 2 3 Rojo Bueno 1 1 Verde Bueno 3 4 Amarillo Malo Calcula a mano (es decir puedes usar calculadora pero no lo calcules en R) y luego verifica tus cálculos haciéndolo en R: El total de \\(\\vec{x}\\) La media y varianza de \\(\\vec{y}\\) La curtosis y la asimetría de \\(\\vec{x}\\) (su media es 2 y su varianza 0.8). Determina si tiene un sesgo a la derecha, a la izquierda o ninguno. Determina mediante la curtosis si \\(\\vec{x}\\) tiene colas más pesadas que \\(\\vec{y}\\). Calcula el cuantil \\(0.25\\) y el \\(0.75\\) de \\(\\vec{y}\\) así como su rango intercuartílico (IQR). ¿Hay valores atípicos (outliers) en \\(\\vec{y}\\)? En caso afirmativo, determina cuáles son. ¿Cuál es el rango de \\(\\vec{y}\\)? (no confundir con el IQR). Determina la moda de \\(\\vec{z}\\) Determina la mediana de \\(\\vec{x}\\). Determina la MAD de \\(\\vec{x}\\) Realiza el conteo de cuáles \\(\\vec{z}\\) pertenecen al conjunto \\(A = \\{ \\text{Rojo}, \\text{Amarillo} \\}\\) Realiza una tabla de contingencia de \\(\\vec{w}\\) y \\(\\vec{z}\\). Determina la distribución frecuencial (observada) marginal de \\(\\vec{w}\\). Realiza una tabla de frecuencias de \\(\\vec{w}\\) y \\(\\vec{z}\\). Calcula el riesgo relativo de estar en un choque dado que manejas en CDMX a partir de los datos en la tabla: CDMX MTY Total Chocó 1100 4000 5100 No Chocó 120 5080 5200 Total 1220 9080 10300 Interpreta tu resultado. De la tabla anterior calcula la razón de momios asociada a chocar dado que manejas en CDMX. Interprétala. Calcula la correlación de Bravais Pearson de \\(\\vec{x}\\) y \\(\\vec{y}\\). Interpreta. Obtén la correlación de Spearman de \\(\\vec{x}\\) y \\(\\vec{y}\\) Para \\(\\vec{w}\\) y \\(\\vec{x}\\) obtén la \\(\\tau\\) de Kendall (son 15 comparaciones para generarla) Descartando el outlier de \\(\\vec{y}\\) (y su \\(\\vec{x}\\) asociada), ajusta un modelo lineal \\(\\hat{y} = \\hat{\\beta}_1 x + \\hat{\\beta}_0\\) y grafícalo para ver qué tan buen modelo es. Realiza una gráfica de caja (boxplot) para \\(\\vec{y}\\) Realiza un scatterplot para la submatriz \\(Z_{(x,y)}\\). Realiza una gráfica de líneas para \\(Z_{(x,y)}\\) identificando la función de interpolación lineal \\(f(x)\\) asociada. Realiza una gráfica de barras de \\(\\vec{w}\\) especificando quiénes son los \\(a_i\\) y los \\(n_j\\). Estima mediante \\(\\hat{p}\\) la función de probabilidad de \\(\\vec{w}\\). Identifica la función de distribución empírica para \\(\\vec{x}\\), \\(\\hat{F}\\) y grafícala. Realiza un histograma con \\(h = 2\\) para \\(x\\). Toma \\(\\kappa = 4\\). Ajusta una densidad kernel a \\(\\vec{x}\\) con \\(h = 1\\) y usando un kernel \\(K\\) triangular. Calcula \\(\\hat{f}(x)\\) para \\(x = 0,1,2,3,4\\). 2.13 Ejercicios del capítulo Dado \\(\\vec{x}\\) vector de variables ordinales, obtén una expresión matemática para los siguientes estadísticos: La media de las diferencias entre las \\(x_i\\) quitando la de \\(x_k\\) consigo misma. El valor numérico o categoría menos común en \\(\\vec{x}\\). Si ordenamos todos los valores, la diferencia más alta entre algún \\(x_{(i)}\\) y su sucesor: \\(x_{(i+1)}\\). Este cálculo de para una S dada como vector numérico: #x es la muestra; x &lt;- c(x1,x2, ..., xn) datos_nuevos &lt;- c() for (i in 1:length(x)){ datos_nuevos &lt;- c(datos_nuevos, x[i]^i) } mean(datos_nuevos) #Este valor es el que me interesa Demuestra que: \\[ \\sigma^2_{\\vec{x}} = \\dfrac{1}{n}\\sum\\limits_{i=1}^n x_i^2 - \\bar{x}^2 \\] Para unos datos observados numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) se tiene que las \\(x\\) toman el valor \\(a_{x,1}\\) \\(n_{x,1}\\) veces, el valor \\(a_{x,2}\\), \\(n_{x,2}\\) veces y el valor \\(a_{x,\\ell}\\), \\(n_{x,\\ell}\\) veces (\\(n_j \\geq 0\\), \\(0 &lt; \\ell \\leq n\\) y \\(\\sum_{j=1}^{\\ell} n_j = n\\)). Demuestra que: \\[ \\bar{x} = \\dfrac{1}{\\sum_{j = 1}^{\\ell} n_{j}} \\cdot \\sum\\limits_{j=1}^{\\ell} n_j a_j \\] Sea \\(n\\) impar y \\(f\\) una función estrictamente decreciente. Demuestra que si \\(x_*\\) es la mediana de \\(\\vec{x} = ( x_1, x_2,\\dots, x_n)^T\\) entonces \\(f(x_*)\\) es la mediana de \\(\\Tilde{\\vec{x}} = \\big( f(x_1), f(x_2),\\dots, f(x_n)\\big)^T\\). Demuestra que si \\(\\bar{x}\\) es la media observada de \\(\\vec{x} = ( x_1, x_2,\\dots, x_n)^T\\) y \\(f(\\bar{x})\\) es la media observada de \\(\\Tilde{\\vec{x}} = \\big( f(x_1), f(x_2),\\dots, f(x_n)\\big)^T\\) y además \\(f\\) es diferenciable, entonces \\(f(x) = a \\cdot x + b\\) (es decir es una transformación afín). Hint Deriva. Sea \\(\\phi\\) una función convexa. Demuestra que: \\[ \\phi \\Big( \\bar{x} \\Big) \\leq \\dfrac{1}{n}\\sum\\limits_{i=1}^n \\phi(x_i) \\] Hint Deriva. Recuerda que si \\(\\phi\\) es convexa, para \\(0 \\leq \\alpha \\leq 1\\) se tiene que: \\[ \\phi\\big(\\alpha x + (1-\\alpha) y\\big) \\leq \\alpha \\phi(x) + (1-\\alpha)\\phi(y) \\] Sea \\(\\hat{p}_R(x)\\) la densidad kernel asociada a \\(\\vec{x} = (x_1, x_2, \\dots, x_n)\\) con un núcleo (kernel) \\(K(u) \\geq 0\\) y \\(h &gt; 0\\). Demuestra: \\(\\hat{p}_h(x)\\) es una función de densidad de probabilidad ( integra a \\(1\\)). Determina la media de una variable aleatoria \\(X\\) que se distribuye con densidad \\(\\hat{p}_h(x)\\) bajo: Kernel triangular. Determina la varianza de una variable aleatoria \\(X\\) que se distribuye con densidad \\(\\hat{p}_h(x)\\) bajo: Kernel Epanechnikov Sea \\(\\text{hist}_{\\vec{x}}\\) la función de histograma para un vector numérico \\(\\vec{x}\\) con \\(h &gt; 0\\), \\(\\kappa \\in \\mathbb{R}\\) fijos y una partición \\(\\{I_j\\}_{j \\in \\mathbb{Z}}\\). Demuestra que \\(\\text{hist}_{\\vec{x}}\\) es una función de densidad. Demuestra que para \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) la función de distribución empírica \\(\\hat{F}(x)\\) es una función de distribución acumulada; es decir: \\(\\lim_{x \\to -\\infty} \\hat{F} (x) = 0\\) \\(\\lim_{x \\to \\infty} \\hat{F} (x) = 1\\) \\(\\lim_{x \\to x_0^+} \\hat{F} (x) = \\hat{F} (x_0)\\) (continua por la derecha) \\(\\lim_{x \\to x_0^-} \\hat{F} (x)\\) existe \\(F_n (x)\\) es no decreciente. La tabla muestra datos observados del PIB de un país en billones de dólares: Año PIB 2000 0.5 2005 1.2 2010 1.5 2015 2.1 Ajusta una parábola \\(q (x) = a \\cdot x^2 + b\\cdot x + c\\) para obtener la mejor parábola que ajuste esos puntos. ¿Qué valor de PIB se espera para el 2020 bajo este modelo? Demuestra que si \\(\\vec{x} = -\\vec{y}\\) (dos vectores numéricos) la correlación de Spearman entre ambos es \\(-1\\). Para una variable aleatoria \\(T\\) que representa un tiempo, se define una función de supervivencia como la probabilidad de que \\(T\\) dure más que un cierto tiempo \\(t\\); es decir: \\[\\begin{equation}\\nonumber S(t) = \\mathbb{P}(T &gt; t) \\end{equation}\\] Construye \\(\\hat{S}\\) una aproximación empírica a la función de supervivencia \\(S\\) tal que \\(\\mathbb{E}[\\hat{S}(t)] = S(t)\\). Demuestra este último resultado. Demuestra que \\(\\text{Var}\\big[ \\hat{F}(x) \\big] = \\frac{1}{n} F(x) \\big( 1 - F(x) \\big)\\). Recuerda que para dos variables aleatorias \\(X\\) y \\(Y\\) se define la covarianza \\(\\text{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]\\). Calcula entonces la covarianza dada por \\(\\text{Cov}\\big(\\hat{p}(u), \\hat{p}(v)\\big)\\). Demuestra que si \\(X\\) es independiente de \\(Y\\) entonces \\(\\mathbb{I}_A(X)\\) es independiente de \\(\\mathbb{I}_A(Y)\\). Recuerda que dos variables aleatorias \\(X\\), \\(Y\\) son independientes si y sólo si \\(\\mathbb{P}(X \\in A, Y \\in B) = \\mathbb{P}(X \\in A) \\mathbb{P}(Y \\in B)\\) para conjuntos \\(A,B\\) medibles. Sean \\(\\rho\\) la \\(\\rho\\) de Spearman y \\(\\tau\\) la \\(\\tau\\) de Kendall para dos vectores ordinales \\(\\vec{x}\\) y \\(\\vec{y}\\). Demuestra que: \\[ \\frac{1 + \\rho}{2} \\geq \\Big( \\frac{1 + \\tau}{2} \\Big)^2 \\] Da un ejemplo de vector \\(\\vec{x}\\) de al menos dos entradas tal que \\(\\text{MAD}_{\\vec{x}} \\geq \\sigma_{\\vec{x}}\\) Demuestra que \\(\\text{MAD}_{\\vec{x}} = 0 \\Leftrightarrow \\sigma_{\\vec{x}} = 0\\) para el mismo vector \\(\\vec{x}\\). Si un vector \\(\\vec{x}\\) tiene \\(3\\) entradas, media \\(\\bar{x} = 1\\) y varianza \\(\\sigma_{\\vec{x}} = 1\\) y además se sabe que su curtosis es \\(1\\), ¿quién es \\(\\vec{x}\\)? Bajo la correlación de Pearson demuestra que para vectores \\(\\vec{x}, \\vec{y}\\) y \\(\\vec{z}\\) si \\(\\rho_{\\vec{x},\\vec{y}} = 1\\) y \\(\\rho_{\\vec{x},\\vec{z}} = 1\\) entonces \\(\\rho_{\\vec{y},\\vec{z}} = 1\\) Sean \\(\\vec{w}, \\vec{x}, \\vec{y}, \\vec{z} \\in\\mathbb{R}^n\\) y \\(a,b,c,d\\in\\mathbb{R}\\) demuestra que: \\[ \\rho(a \\vec{x} + b \\vec{w}, c \\vec{y} + d \\vec{z}) = K_1\\cdot\\rho(\\vec{x}, \\vec{y}) + K_2\\cdot\\rho(\\vec{w}, \\vec{y}) + K_3\\cdot\\rho(\\vec{x}, \\vec{z}) + K_4\\cdot\\rho(\\vec{w}, \\vec{z}) \\] para algunas constantes \\(K_1, K_2, K_3, K_4\\); donde, además, \\(\\rho\\) es la correlación de Pearson. Sean \\(\\vec{x} = (x_1,x_2, \\dots, x_n)^T\\) el vector de los datos observados (fijo) y \\(\\vec{X} = (X_1,X_2, \\dots, X_n)^T\\) el vector de los datos posibles (aleatorio). Supongamos que las entradas de \\(\\vec{X}\\) son independientes e idénticamente distribuidas con la misma distribución de \\(X\\) con media \\(\\mu\\) y varianza \\(\\sigma^2\\). Demuestra que \\(\\mathbb{E}\\big[ \\bar{X} \\big] = \\mu\\) ¿Qué te dice esto de \\(\\bar{x}\\)? Demuestra que \\(\\mathbb{E}\\big[ \\sigma_{\\vec{X}}^2 \\big] \\neq \\sigma^2\\) ¿Qué te dice esto de \\(\\sigma_{\\vec{x}}^2\\)? Donde \\(\\sigma_{\\vec{X}}^2 = \\frac{1}{n}\\sum_{i=1}^n \\big( X_i - \\bar{X}\\big)^2\\). Hint: usa el ejercicio 2 de esta sección. Quiénes son \\(\\vec{x}, \\vec{y}\\) si su tabla de contingencia es la dada por Cuadro : Consomé Taco Postre Tacos el Güero 1 3 2 Tacos la Güera 4 2 1 Da un ejemplo de vectores \\(\\vec{x}\\) y \\(\\vec{y}\\) de tamaño \\(10\\) cuya \\(\\tau\\) de Kendall sea \\(0\\) pero estén completamente relacionados; es decir exista una función \\(g\\) tal que \\(\\vec{y} = g(\\vec{x})\\). Construye una función function en R que dado un vector x &lt;- c(x_1, x_2, ..., x_n) regrese la densidad kernel (bajo kernel gaussiano) asociada a x y evaluada en los puntos dados por el vector t &lt;- c(t_1, t_2, ..., t_m). Considera la siguiente base de datos de calificaciones. Calcula la mediana de calificaciones, media, varianza y el IQR. Calificación Cantidad de alumnos 10 3 9 4 8 12 7 11 6 1 5 4 A partir de la siguiente gráfica determina si los incisos son verdaderos o falsos o no se puede determinar: La correlación de Pearson de \\(\\vec{x}\\) y \\(\\vec{y}\\) es \\(-1\\) Verdadero Falso No se puede determinar La correlación de Spearman de \\(\\vec{x}\\) y \\(\\vec{y}\\) es \\(-1\\) Verdadero Falso No se puede determinar La tau de Kendall de \\(\\vec{x}\\) y \\(\\vec{y}\\) es \\(-1\\) Verdadero Falso No se puede determinar A partir de la siguiente gráfica determina si los incisos son verdaderos o falsos o no se puede determinar: El coeficiente de asimetría de \\(\\vec{x}\\) es positivo. Verdadero Falso No se puede determinar El coeficiente de curtosis de \\(\\vec{x}\\) es positivo. Verdadero Falso No se puede determinar La distribución de \\(\\vec{x}\\) tiene una sola moda. Verdadero Falso No se puede determinar Considera el problema de mínimos cuadrados donde ahora suponemos que existe una submatriz \\(X_{n \\times k}\\) de la base de datos \\(Z\\) y un vector columna \\(y\\) tal que cada entrada de las \\(y\\), \\(y_i\\) es una función lineal de las \\(x_{i,j}\\): \\[ y_i = \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\dots + \\beta_k x_{i,k} \\] Suponemos, además que las columnas de \\(X_{n \\times k}\\) son linealmente independientes (i.e. es de rango completo). En este caso la función de error a minimizar para estimar las \\(\\beta\\)s es: \\[ \\text{SSR}(\\beta_1, \\beta_2, \\dots, \\beta_k) = \\sum\\limits_{i=1}^n \\big( y_i - \\sum\\limits_{j=1}^k \\beta_j x_{i,j}\\big)^2 \\] Demuestra que en este caso el problema es equivalente a minimizar: \\[\\begin{equation}\\nonumber \\text{SSR}(\\vec{\\beta}) = (\\vec{y} - X \\vec{\\beta})^T (\\vec{y} - X \\vec{\\beta}) \\end{equation}\\] Obtén entonces que: \\[\\begin{equation}\\nonumber \\beta = (X^T X)^{-1} X^T \\vec{y} \\end{equation}\\] Finalmente, suponiendo que \\(y = \\alpha + \\gamma x + \\eta z\\) es un hiperplano de \\(x\\) y \\(z\\) calcula los coeficientes: x y z 1 7 -1 2 9 -2 3 12 -3 1 14 1 Verifica los coeficientes haciendo la regresión en R. References "],
["muestreo-aleatorio-simple.html", "Capítulo 3 Muestreo Aleatorio Simple 3.1 Inicio 3.2 Librerías 3.3 Notación 3.4 Muestreo Aleatorio Simple sin Reemplazo (MAS/sR) 3.5 Teorema del Límite Central (Aplicación) 3.6 Ejemplo Resumen: Estimación de una proporción bajo muestreo aleatorio simple sin reemplazo 3.7 Ejemplo Resumen: Estimación del total de individuos en una fotografía 3.8 Ejercicio: 3.9 Ejemplo Resumen: Estimación de una región crítica 3.10 Ejemplo Resumen: Estimación del total de una población 3.11 Demostración del Teorema del Límite Central para Muestras Finitas 3.12 Muestreo Aleatorio Simple Bernoulli (BE) 3.13 Ejemplo Resumen: Aduana 3.14 Muestreo Aleatorio Simple con Reemplazo (MAS/cR) 3.15 Ejemplo Resumen: Proporción de trabajadores enfermos con o sin reemplazo 3.16 Ejemplo Resumen: Captura-Recaptura con reemplazo 3.17 Muestreo Aleatorio Simple Ponderado (MAS/P) 3.18 Ejercicios", " Capítulo 3 Muestreo Aleatorio Simple 3.1 Inicio Siempre que inicies un nuevo trabajo en R ¡no olvides borrar el historial! rm(list = ls()) #Clear all 3.2 Librerías Para este análisis vamos a tener que llamar a las siguientes librerías previamente instaladas (por única vez) con install.packages: library(tidyverse) library(dplyr) library(imager) library(rlist) library(gridExtra) library(kableExtra) 3.3 Notación Supongamos una matriz de datos de tamaño \\(N \\times L\\) dada por: \\[ U = \\begin{pmatrix} z_1 \\Big| z_2 \\Big| \\dots \\Big| z_{\\ell} \\end{pmatrix} \\] donde las \\(z_i\\) representan las columnas de la matriz. La \\(U\\) será conocida como la matriz universo (el universo ó la población) si contiene toda la información de la población. Intuitivamente, la matriz \\(U\\) son todos los datos de un censo: esta es una matriz ideal donde están todos los datos. A cualquier permutación en las filas de una submatriz \\(S\\) (tamaño \\(n \\times \\ell\\) con \\(0 &lt; n \\leq N\\) y \\(0 &lt; \\ell \\leq L\\)) de \\(U\\) se le conoce como una muestra de \\(U\\). Si \\(S\\) es una variable aleatoria (por ejemplo, porque se construyó a partir de un mecanismo de aleatoreidad) decimos que \\(S\\) es una muestra aleatoria (denotamos \\(\\mathcal{S}\\) a la variable aleatoria y \\(S\\) a un valor específico de la misma).2 En particular en esta sección (y hasta establecer lo contrario) consideraremos que el universo \\(U\\) es de tamaño \\(N \\times 1\\) y la submatriz \\(S\\) (resp. \\(\\mathcal{S}\\)) es de tamaño \\(n \\times 1\\). En notación \\(U = (x_1, x_2, \\dots, x_N)^T\\) y técnicamente \\(S = (x_{i_1}, x_{i_2}, \\dots, x_{i_n})^T\\) para un conjunto de índices \\(i_1, i_2, \\dots, i_n\\). Sin embargo para simplificar la notación consideraremos que en \\(S\\) están los primeros \\(n\\) de los \\(x_i\\); es decir: \\[ S = (x_1, x_2, \\dots, x_n)^T \\] Cuando estemos hablando de la muestra como variable aleatoria \\(\\mathcal{S}\\) y no como valores observados (fijos) \\(S\\), denotaremos: \\[ \\mathcal{S} = (X_1, X_2, \\dots, X_n)^T \\] donde \\(\\mathcal{S}\\) representa la muestra posible y cada \\(X_i\\) es una variable aleatoria con el valor posible de la i-ésima entrada. Un esquema muestral es una función \\(\\mathbb{P}\\) de probabilidad definida en el conjunto de submatrices de \\(U\\). Ésta es el punto medular de todas las estrategias de muestreo: distintos esquemas muestrales generan diferentes distribuciones y pueden llevar a distintas inferencias sobre un fenómeno. 3.3.1 Ejemplo Considera la matriz universo con tres letras: \\[ U = \\begin{pmatrix} \\text{A} \\\\ \\text{B} \\\\ \\text{C} \\end{pmatrix} \\] Ésta es la matriz universo. Las submatrices3 que pueden crearse a partir de dicho universo son: De dimensión \\(n = 1\\): \\(S^1 = (\\text{A})^T\\), \\(S^2 = (\\text{B})^T\\), \\(S^3 = (\\text{C})^T\\). De dimensión \\(n = 2\\): \\(S^4 = (\\text{A}, \\text{B})^T\\), \\(S^5 = (\\text{A}, \\text{C})^T\\), \\(S^6 = (\\text{B}, \\text{C})^T\\), \\(S^7 = (\\text{B}, \\text{A})^T\\), \\(S^8 = (\\text{C}, \\text{A})^T\\), \\(S^9 = (\\text{C}, \\text{B})^T\\). De dimensión \\(n = 3\\): \\(S^{10} = (\\text{A}, \\text{B}, \\text{C})^T\\), \\(S^{11} = (\\text{B}, \\text{A}, \\text{C})^T\\), \\(S^{12} = (\\text{A}, \\text{C}, \\text{B})^T\\), \\(S^{13} = (\\text{C}, \\text{B}, \\text{A})^T\\), \\(S^{14} = (\\text{B}, \\text{C}, \\text{A})^T\\), \\(S^{15} = (\\text{C}, \\text{A}, \\text{B})^T\\), Un esquema muestral sería la función de probabilidad: \\[ \\mathbb{P}(\\mathcal{S} = S^k) = \\begin{cases} 0.1 &amp; \\text{ si } k = 1, \\\\ 0.2 &amp; \\text{ si } k = 3, \\\\ 0.5 &amp; \\text{ si } k = 11, \\\\ 0.2 &amp; \\text{ si } k = 15, \\\\ 0 &amp; \\text{ en otro caso.} \\end{cases} \\] Otro esquema muestral posible sería: \\[ \\mathbb{P}(\\mathcal{S} = S^k) = \\begin{cases} \\frac{1}{3} &amp; \\text{ si } k = 1, \\\\ \\frac{1}{3} &amp; \\text{ si } k = 2, \\\\ \\frac{1}{3} &amp; \\text{ si } k = 3, \\\\ 0 &amp; \\text{ en otro caso.} \\end{cases} \\] Este último esquema, intuitivamente, corresponde a la selección aleatoria de un elemento de \\(U\\) con una probabilidad uniforme de que cada elemento salga. A fin de simplificar el problema (y hasta que se diga lo contrario) agregaremos la hipótesis de intercambiabilidad; es decir, consideraremos es irrelevante el orden de las filas de las submatrices de datos. Por ejemplo, bajo intercambiabilidad, \\(S^4 = (\\text{A}, \\text{B})^T\\) es la misma matriz que \\(S^7 = (\\text{B}, \\text{A})^T\\). Un ejemplo de muestra donde el orden sí importa (i.e. no son intercambiables) es cuando se realizan exámenes orales según una selección aleatoria de la lista. La tercera persona en presentar el examen estará informada por el ¿qué te preguntó el profe?, ¿estuvo difícil? que las primeras dos le cuenten. Bajo intercambiabilidad, los esquemas muestrales estarán definidos únicamente sobre los siguientes vectores: De dimensión \\(n = 1\\): \\(S^1 = (\\text{A})^T\\), \\(S^2 = (\\text{B})^T\\), \\(S^3 = (\\text{C})^T\\). De dimensión \\(n = 2\\): \\(S^4 = (\\text{A}, \\text{B})^T\\), \\(S^5 = (\\text{A}, \\text{C})^T\\), \\(S^6 = (\\text{B}, \\text{C})^T\\). De dimensión \\(n = 3\\): \\(S^{7} = (\\text{A}, \\text{B}, \\text{C})^T\\). En este caso un esquema muestral sería: \\[ \\mathbb{P}(\\mathcal{S} = S^k) = \\begin{cases} \\frac{1}{16} &amp; \\text{ si } k = 1, \\\\ \\frac{3}{16} &amp; \\text{ si } k = 2, \\\\ 0 &amp; \\text{ si } k = 3, \\\\ \\frac{7}{16} &amp; \\text{ si } k = 4, \\\\ \\frac{1}{16} &amp; \\text{ si } k = 5, \\\\ \\frac{4}{16} &amp; \\text{ si } k = 6, \\\\ 0 &amp; \\text{ en otro caso.} \\end{cases} \\] Dado un elemento \\(x_i\\) del universo, podemos preguntarnos por la probabilidad de que dicho \\(x_i\\) esté en la muestra. Siguiendo el ejemplo anterior: \\[ \\mathbb{P}(\\text{A} \\in \\mathcal{S}) = \\mathbb{P}(\\mathcal{S} = S^1) + \\mathbb{P}(\\mathcal{S} = S^4) + \\mathbb{P}(\\mathcal{S} = S^5) + \\mathbb{P}(\\mathcal{S} = S^7) = \\frac{9}{16}. \\] Como notación, para una población \\(U = (x_1, x_2, \\dots, x_N)^T\\) y una muestra aleatoria \\(\\mathcal{S}\\) denotamos la probabilidad de que \\(x_k\\) esté en la muestra como: \\[ \\pi_k = \\mathbb{P}(x_k \\in \\mathcal{S}) \\] Estas probabilidades (para \\(k = 1,2,\\dots, N\\)) se conocen como probabilidades de inclusión de primer orden. La probabilidad conjunta de que \\(x_k\\) y \\(x_l\\) (ambos) estén en la muestra (probabilidad de inclusión de segundo orden) está dada por: \\[ \\pi_{k,l} = \\mathbb{P}(x_k \\in \\mathcal{S}, x_l \\in \\mathcal{S}) \\] Notamos que por definición \\(\\pi_{kk} = \\pi_k\\). Análogamente se pueden crear probabilidades de inclusión de cualquier orden deseado. Finalmente, una población \\(U = (x_1, x_2, \\dots, x_N)^T\\) y una muestra aleatoria \\(\\mathcal{S}\\) definimos la variable indicadora de que \\(x_k\\) esté en la muestra como: \\[ \\mathbb{I}_{\\mathcal{S}}(x_k) = \\begin{cases} 1 &amp; \\text{ si } x_k \\in \\mathcal{S} \\\\ 0 &amp; \\text{ si } x_k \\not\\in \\mathcal{S} \\\\ \\end{cases} \\] Notamos que para una muestra aleatoria \\(\\mathcal{S}\\) las indicadoras tienen una distribución conocida: \\[ \\mathbb{I}_{\\mathcal{S}}(x_k) \\sim \\text{Bernoulli}(\\pi_k) \\] pues \\[ \\mathbb{P}\\big( \\mathbb{I}_{\\mathcal{S}}(x_k) = 1\\big) = \\mathbb{P}\\big(x_k \\in \\mathcal{S}) = \\pi_k \\] Como las \\(\\mathbb{I}_{\\mathcal{S}}(x_k)\\) son Bernoulli podemos calcular su varianza: \\[ \\text{Var}\\Big( \\mathbb{I}_{\\mathcal{S}}(x_k)\\Big) = \\pi_k (1 - \\pi_k) \\] Finalmente, recordamos que la covarianza entre dos variables aleatorias \\(X\\), \\(Y\\) se define como: \\[ \\text{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\cdot\\mathbb{E}[Y] \\] Por lo que calculamos la covarianza entre dos indicadoras (de \\(x_k\\) y \\(x_l\\)): \\[\\begin{equation} \\begin{aligned} \\text{Cov}\\Big(\\mathbb{I}_{\\mathcal{S}}(x_k), \\mathbb{I}_{\\mathcal{S}}(x_l) \\Big) &amp; = \\mathbb{E}\\Big[\\mathbb{I}_{\\mathcal{S}}(x_k) \\cdot \\mathbb{I}_{\\mathcal{S}}(x_l)\\Big] - \\mathbb{E}\\Big[\\mathbb{I}_{\\mathcal{S}}(x_k)\\Big] \\mathbb{E}\\Big[\\mathbb{I}_{\\mathcal{S}}(x_l)\\Big] \\\\ &amp; = 1 \\cdot \\mathbb{P}\\Big( \\mathbb{I}_{\\mathcal{S}}(x_k) \\cdot \\mathbb{I}_{\\mathcal{S}}(x_l) = 1) + 0 \\cdot \\mathbb{P}\\Big( \\mathbb{I}_{\\mathcal{S}}(x_k) \\cdot \\mathbb{I}_{\\mathcal{S}}(x_l) = 0) - \\pi_k \\pi_l \\\\ &amp; = \\mathbb{P}\\Big( \\mathbb{I}_{\\mathcal{S}}(x_k) = 1, \\mathbb{I}_{\\mathcal{S}}(x_l) = 1) - \\pi_k \\pi_l \\\\ &amp; = \\pi_{k,l} -\\pi_k \\pi_l \\end{aligned} \\end{equation}\\] La cantidad \\(\\pi_{k,l} -\\pi_k \\pi_l\\) usualmente se denota \\(\\Delta_{k,l}\\): \\[ \\Delta_{k,l} = \\pi_{k,l} -\\pi_k \\pi_l \\] A continuación hablaremos de algunos esquemas de muestreo comunmente utilizados y, finalmente, llegaremos a una generalización de los mismos. 3.3.2 Ejercicio Demuestra las siguientes propiedades de los \\(\\pi_k\\) para un diseño muestral \\(\\mathbb{P}\\) con tamaño fijo de la muestra \\(n\\in\\mathbb{N}\\): \\(\\sum\\limits_{k=1}^N \\pi_k = n\\) \\(\\sum\\limits_{\\substack{k=1 \\\\\\\\ k \\neq l}}^N \\sum\\limits_{l=1}^N \\pi_{k,l} = n(n-1)\\) \\(\\sum\\limits_{\\substack{l=1 \\\\\\\\ l \\neq k}}^N \\pi_{k,l} = (n-1) \\pi_k\\) 3.4 Muestreo Aleatorio Simple sin Reemplazo (MAS/sR) Vamos a considerar una de las formas más sencillas de muestreo: el aleatorio simple sin reemplazo . Para ello seleccionamos de \\(U = (x_1, x_2, \\dots, x_N)^T\\) a \\(n\\in\\mathbb{N}\\) (fijo) observaciones asignándole la probabilidad de ser seleccionada a cada una de \\(\\frac{1}{N}\\). Una vez se selecciona la primera, se selecciona una de las que restan de \\(U\\) con probabilidad \\(\\frac{1}{N-1}\\). El proceso se repite hasta extraer \\(n\\) elementos. Comencemos por un ejemplo, supongamos tenemos una población de cinco personas: \\[ U = \\Big( \\text{Ana}, \\text{Beto}, \\text{Carlos}, \\text{Diana}, \\text{Enriqueta}\\Big)^T \\] Si queremos tomar una muestra de \\(3\\) personas sin reemplazo, las muestras posibles son: \\(\\Big( \\text{Ana}, \\text{Beto}, \\text{Carlos}\\Big)^T\\) \\(\\Big( \\text{Ana}, \\text{Carlos}, \\text{Diana}\\Big)^T\\) \\(\\Big( \\text{Ana}, \\text{Beto}, \\text{Diana}\\Big)^T\\) \\(\\Big( \\text{Ana}, \\text{Beto}, \\text{Enriqueta}\\Big)^T\\) \\(\\Big( \\text{Ana}, \\text{Carlos}, \\text{Enriqueta}\\Big)^T\\) \\(\\Big( \\text{Ana}, \\text{Diana}, \\text{Enriqueta}\\Big)^T\\) \\(\\Big( \\text{Beto}, \\text{Carlos}, \\text{Diana}\\Big)^T\\) \\(\\Big( \\text{Beto}, \\text{Diana}, \\text{Enriqueta}\\Big)^T\\) \\(\\Big( \\text{Beto}, \\text{Carlos}, \\text{Enriqueta}\\Big)^T\\) \\(\\Big( \\text{Carlos}, \\text{Diana}, \\text{Enriqueta}\\Big)^T\\) Obtener una muestra aleatoria se puede hacer en R con un vector mediante sample: #Vector de nombres nombres &lt;- c(&quot;Ana&quot;,&quot;Beto&quot;,&quot;Carlos&quot;,&quot;Diana&quot;,&quot;Enriqueta&quot;) #Muestra sample(nombres, 3, replace = FALSE) ## [1] &quot;Beto&quot; &quot;Ana&quot; &quot;Diana&quot; Formalmente, un esquema de muestreo es aleatorio simple sin reemplazo si dada una constante \\(n \\in \\mathbb{N}\\) (con \\(0 &lt; n \\leq N\\)) se tiene: \\[ \\mathbb{P}\\big( \\mathcal{S} = S \\big) = \\begin{cases} \\frac{1}{\\binom{N}{n}} &amp; \\text{ si } \\#S = n \\\\ 0 &amp; \\text{ en otro caso.} \\end{cases} \\] En el caso de muestreo aleatorio simple sin reemplazo podemos calcular las probabilidades de inclusión como siguen: \\[ \\pi_k = \\mathbb{P}(x_k \\in \\mathcal{S}) = \\sum\\limits_{i=1}^{M_1} \\frac{1}{\\binom{N}{n}} = \\frac{\\binom{N-1}{n-1}}{\\binom{N}{n}} = \\frac{n}{N} = f \\] donde la tercera igualdad se sigue de que hay \\(M_1 = \\binom{N-1}{n-1}\\) muestras que contienen al \\(x_k\\). (La lógica es, fijo el \\(x_k\\) y entonces me quedan \\(N-1\\) valores de \\(x\\) a acomodar en \\(n-1\\) espacios). Por otro lado: \\[ \\pi_{k,j} = \\mathbb{P}(x_k \\in \\mathcal{S}, x_j \\in S) = \\sum\\limits_{i=1}^{M_2} \\frac{1}{\\binom{N}{n}} = \\frac{\\binom{N-2}{n-2}}{\\binom{N}{n}} = \\dfrac{n(n-1)}{N(N-1)} \\] pues hay \\(M_2 = \\binom{N-2}{n-2}\\) muestras conteniendo a \\(x_k\\) y \\(x_j\\) a la vez. Para estimar el total poblacional dado por: \\[ t = \\sum\\limits_{i=1}^N x_i \\] bajo MAS/sR podemos tomar: \\[ \\hat{t} = N \\cdot \\bar{x}_{\\mathcal{S}} = N \\frac{1}{n} \\sum\\limits_{k = 1}^n x_k= \\sum\\limits_{k = 1}^n \\dfrac{x_k}{n/N} = \\sum\\limits_{k=1}^N \\frac{x_k}{\\pi_k} \\cdot \\mathbb{I}_{\\mathcal{S}}(x_k) \\] Notamos entonces que el estimador \\(\\hat{t}\\) es una variable aleatoria pues depende de las indicadoras de la muestra. En particular: \\[ \\mathbb{E}\\big[ \\hat{t} \\big] = \\mathbb{E}\\bigg[\\sum\\limits_{k=1}^N \\frac{x_k}{\\pi_k} \\cdot \\mathbb{I}_{\\mathcal{S}}(x_k) \\bigg] = \\sum\\limits_{k=1}^N \\frac{x_k}{\\pi_k} \\underbrace{\\mathbb{E}\\bigg[\\mathbb{I}_{\\mathcal{S}}(x_k) \\bigg]}_{\\pi_k} = t \\] de donde se sigue que en promedio el estimador \\(\\hat{t}\\) vale el total. Definición: [Insesgado] Un estimador \\(\\hat{\\theta}\\) es un estimador insesgado de \\(\\theta\\) si: \\[ \\mathbb{E}\\big[ \\hat{\\theta} - \\theta] = 0 \\] En nuestro caso \\(\\hat{t}\\) es insesgado. En general, la cantidad \\(\\mathbb{E}\\big[ \\hat{\\theta} - \\theta]\\) se conoce como el sesgo . De manera numérica, podemos simular la estimación del total en 1000 simulaciones como sigue: nsim &lt;- 1000 N &lt;- 1000 n &lt;- 100 base.completa &lt;- data.frame(x = rnorm(N)) total &lt;- sum(base.completa$x) total.muestra &lt;- rep(NA, nsim) for (i in 1:nsim){ muestra &lt;- sample(base.completa$x, n) total.muestra[i] &lt;- N*mean(muestra) } mean(total.muestra) ## [1] -78.75412 Podemos ver las simulaciones como sigue: ggplot() + geom_histogram(aes(x = total.muestra, y = ..density..), fill = &quot;#008B8B&quot;, color = &quot;white&quot;, binwidth = 40) + geom_vline(aes(xintercept = total), linetype = &quot;dashed&quot;) + theme_classic() Como podrás notar la \\(\\hat{t}\\) es una variable aleatoria y por tanto tiene varianza. De hecho: \\[ \\textrm{Var}(\\hat{t}) = \\sum\\limits_{k = 1}^N \\sum\\limits_{l = 1}^N \\Delta_{k,l} \\frac{x_k}{\\pi_k} \\frac{x_l}{\\pi_l} \\] Para demostrarlo seguimos las igualdades: \\[\\begin{equation}\\nonumber \\begin{aligned} \\textrm{Var}(\\hat{t}) &amp; = \\textrm{Var}\\Bigg( \\sum\\limits_{k=1}^N \\frac{x_k}{\\pi_k} \\cdot \\mathbb{I}_{\\mathcal{S}}(x_k) \\Bigg) \\\\ &amp; = \\sum\\limits_{k=1}^N \\frac{x_k^2}{\\pi_k^2} \\cdot \\textrm{Var}\\Big(\\mathbb{I}_{\\mathcal{S}}(x_k) \\Big) + \\sum\\limits_{k = 1}^N \\sum\\limits_{\\substack{l = 1 \\\\ \\\\ l \\neq k}}^{N} \\frac{x_k}{\\pi_k} \\frac{x_l}{\\pi_l} \\cdot \\textrm{Cov}\\Big(\\mathbb{I}_{\\mathcal{S}}(x_k), \\mathbb{I}_{\\mathcal{S}}(x_l) \\Big) \\\\ &amp; = \\sum\\limits_{k=1}^N \\frac{x_k^2}{\\pi_k^2} \\cdot \\underbrace{\\pi_k (1 - \\pi_k)}_{\\Delta_{k,k}} + \\sum\\limits_{k = 1}^N \\sum\\limits_{\\substack{l = 1 \\\\ \\\\ l \\neq k}}^{N} \\frac{x_k}{\\pi_k} \\frac{x_l}{\\pi_l} \\cdot \\underbrace{\\textrm{Cov}\\Big(\\mathbb{I}_{\\mathcal{S}}(x_k), \\mathbb{I}_{\\mathcal{S}}(x_l) \\Big)}_{\\Delta_{k,l}} \\\\ &amp; = \\sum\\limits_{k = 1}^N \\sum\\limits_{\\substack{l = 1 \\\\ \\\\ l \\neq k}}^{N}\\Delta_{k,l} \\frac{x_k}{\\pi_k} \\frac{x_l}{\\pi_l} \\end{aligned} \\end{equation}\\] Numéricamente, en el ejemplo anterior la varianza (simulada) de \\(\\hat{t}\\) es: var(total.muestra) ## [1] 8515.982 mientras que la real está dada por (ver ejercicio más adelante): f &lt;- n/N varianza &lt;- N^2*(1 - f)/n*var(base.completa$x) print(varianza) ## [1] 8278.693 Nota que tenemos un problema: para estimar \\(\\textrm{Var}(\\hat{t})\\) necesitamos conocer todas las \\(x_k\\) de la población ¡lo cual es imposible! Entonces necesitamos un estimador de la varianza de \\(\\hat{t}\\) para lo cual proponemos: \\[ \\widehat{\\textrm{Var}}(\\hat{t}) = \\sum\\limits_{k = 1}^n \\sum\\limits_{l = 1}^n \\dfrac{\\Delta_{k,l}}{\\pi_{k,l}} \\frac{x_k}{\\pi_k} \\frac{x_l}{\\pi_l} \\] Para demostrar que el estimador es insesgado tomamos el valor esperado y agregamos las variables indicadoras correspondientes: \\[ \\widehat{\\textrm{Var}}(\\hat{t}) = \\sum\\limits_{k = 1}^N \\sum\\limits_{l = 1}^N \\dfrac{\\Delta_{k,l}}{\\pi_{k,l}} \\frac{x_k}{\\pi_k} \\frac{x_l}{\\pi_l} \\mathbb{I}_{\\mathcal{S}}(x_k) \\mathbb{I}_{\\mathcal{S}}(x_l) \\] Se sigue la demostración: \\[\\begin{equation}\\nonumber \\begin{aligned} \\mathbb{E}\\Big[\\widehat{\\textrm{Var}}(\\hat{t}) \\Big] &amp; = \\mathbb{E}\\bigg[ \\sum\\limits_{k = 1}^N \\sum\\limits_{l = 1}^N \\dfrac{\\Delta_{k,l}}{\\pi_{k,l}} \\frac{x_k}{\\pi_k} \\frac{x_l}{\\pi_l} \\mathbb{I}_{\\mathcal{S}}(x_k) \\mathbb{I}_{\\mathcal{S}}(x_l) \\bigg] \\\\ &amp; = \\sum\\limits_{k = 1}^N \\sum\\limits_{l = 1}^N \\dfrac{\\Delta_{k,l}}{\\pi_{k,l}} \\frac{x_k}{\\pi_k} \\frac{x_l}{\\pi_l} \\underbrace{\\mathbb{E}\\bigg[ \\mathbb{I}_{\\mathcal{S}}(x_k) \\mathbb{I}_{\\mathcal{S}}(x_l) \\bigg]}_{*} \\\\ \\end{aligned} \\end{equation}\\] donde notamos que: \\[\\begin{equation}\\nonumber \\begin{aligned} * = \\mathbb{E}\\bigg[ \\mathbb{I}_{\\mathcal{S}}(x_k) \\mathbb{I}_{\\mathcal{S}}(x_l) \\bigg] &amp; = \\textrm{Cov}\\Big( \\mathbb{I}_{\\mathcal{S}}(x_k), \\mathbb{I}_{\\mathcal{S}}(x_l) \\Big) + \\mathbb{E}\\Big[ \\mathbb{I}_{\\mathcal{S}}(x_k)\\Big] \\mathbb{E}\\Big[ \\mathbb{I}_{\\mathcal{S}}(x_l)\\Big] \\\\ &amp; = \\pi_{k,l} - \\pi_k \\pi_l + \\pi_k\\pi_l \\\\ &amp; = \\pi_{k,l} \\end{aligned} \\end{equation}\\] de donde se sigue: \\[\\begin{equation}\\nonumber \\begin{aligned} \\mathbb{E}\\Big[\\widehat{\\textrm{Var}}(\\hat{t}) \\Big] &amp; = \\sum\\limits_{k = 1}^N \\sum\\limits_{l = 1}^N \\dfrac{\\Delta_{k,l}}{\\pi_{k,l}} \\frac{x_k}{\\pi_k} \\frac{x_l}{\\pi_l}\\underbrace{ \\pi_{k,l}}_{*} \\\\ &amp; = \\sum\\limits_{k = 1}^N \\sum\\limits_{l = 1}^N \\Delta_{k,l} \\frac{x_k}{\\pi_k} \\frac{x_l}{\\pi_l} = \\textrm{Var}(\\hat{t}) \\end{aligned} \\end{equation}\\] Podemos calcular la varianza estimada para una muestra aleatoria simple sin reemplazo como sigue (ver ejercicio): f &lt;- n/N varianza &lt;- N^2*(1 - f)/n*var(muestra) print(varianza) ## [1] 7811.204 Observaciones La media muestral \\(\\bar{x}_{\\mathcal{S}} = \\frac{1}{n}\\sum\\limits_{i = 1}^{n} x_i\\) es un estimador insesgado de la media poblacional \\(\\bar{x}_{\\mathcal{U}} = \\frac{1}{N}\\sum\\limits_{i = 1}^{N} x_i\\). Se sigue de una factorización de \\(n\\) del total (\\(t\\) y \\(\\hat{t}\\) respectivamente). Se puede obtener \\(\\textrm{Var}(\\bar{x}_{\\mathcal{S}})\\) y \\(\\widehat{\\textrm{Var}}(\\bar{x}_{\\mathcal{S}})\\) factorizando las \\(n\\) de manera cuadrática del \\(\\hat{t}\\). 3.4.1 Ejercicio Definimos: \\[ s_{x,\\mathcal{U}}^2 = \\dfrac{1}{N-1} \\sum\\limits_{k = 1}^N \\big( x_k - \\bar{x}_{\\mathcal{U}})^2 \\] como la varianza poblacional ajustada y \\[ s_{x,\\mathcal{S}}^2 = \\dfrac{1}{n-1} \\sum\\limits_{k = 1}^n \\big( x_k - \\bar{x}_{\\mathcal{S}})^2 \\] como la varianza muestral ajustada. Sea \\(f = \\frac{n}{N}\\) la fracción muestral. Demuestra que en el caso de muestreo aleatorio simple sin reemplazo: \\[ \\textrm{Var}(\\hat{t}) = N^2\\dfrac{1-f}{n} s^2_{x,\\mathcal{U}} \\] mientras que el estimador insesgado se transforma en: \\[ \\widehat{\\textrm{Var}}(\\hat{t}) = N^2\\dfrac{1-f}{n} s^2_{x,\\mathcal{S}} \\] 3.5 Teorema del Límite Central (Aplicación) En esta sección hablaremos del teorema central del límite correspondiente a muestreo aleatorio simple con poblaciones finitas. Éste no es el mismo que el de Proba 2 (en términos de hipótesis) aunque las conclusiones sean las mismas. El teorema de Proba 2 establece que si se tiene una colección \\(\\{ X_i \\}\\) de variables aleatorias independientes idénticamente distribuidas (todas con distribución acumulada \\(F_X\\)) con media \\(\\mu\\) y varianza \\(\\sigma^2 &lt; \\infty\\), entonces, si definimos \\(Z\\) como: \\[ Z =\\lim_{n \\to \\infty} \\sqrt{\\dfrac{n}{\\sigma^2}} \\cdot \\Big( \\frac{1}{n}\\sum_{i = 1}^n X_i - \\mu\\Big) \\] se tiene que \\(Z \\sim \\textrm{Normal}(0,1)\\). En este teorema central podemos observar que hay algo muy parecido a la media muestral embebido en el teorema (la \\(\\frac{1}{n}\\sum_{i = 1}^n X_i\\)) pero no es exactamente la media muestral (aquí se supone que todas las \\(X_i\\) son independientes con distribución \\(F_X\\) y en el caso de muestreo aleatorio sin reemplazo se sabe que las indicadoras NO son independientes y que de hecho tampoco son idénticamente distribuidas cuando analizamos \\(\\sum_{i = 1}^{n} x_i \\mathbb{I}_{\\mathcal{S}}(x_i)\\)). Entonces técnicamente no podemos aplicar el teorema central del límite así como está a nuestra muestra. Sin embargo, Hàjek (y más tarde Rosen ) encontraron condiciones sin tener que pedir independencia ni distribución idéntica que permiten sustituir las \\(X_i\\) por las de la media muestral (\\(x_i \\mathbb{I}_{\\mathcal{S}}(x_i)\\)) y que, cuando \\(N\\) y \\(n\\) tienden a infinito “de buena manera”, se tiene algo similar a esta expresión (OJO no es una expresión correcta pero es la idea): \\[ Z =\\lim_{N, n \\to \\infty} \\sqrt{\\frac{1}{\\textrm{Var}(\\bar{x}_{\\mathcal{S}})}} \\cdot \\Big( \\frac{1}{n}\\sum_{i = 1}^N x_i \\mathbb{I}_{\\mathcal{S}}(x_i) - \\bar{x}_{\\mathcal{U}}\\Big) \\] donde \\(\\mu = \\sum_{k = 1}^N x_k\\) es la media poblacional y \\(\\sigma^2 = \\frac{1}{N} \\sum_{k = 1}^N (x_k - \\mu)^2\\) la varianza poblacional no ajustada. La demostración propia de este teorema la posponemos para una sección posterior. Por ahora, ejemplificaremos el teorema del límite central en R, utilizaremos la expresión anterior para deducir y explicar el concepto de intervalo de confianza y, finalmente, haremos un ejemplo de estimación de intervalo. 3.5.1 Estimación de intervalos de confianza para el total Un intervalo de confianza de \\((1 - \\alpha)\\times 100 \\%\\) de un estimador poblacional desconocido \\(\\theta = \\theta(x_1, x_2, \\dots, x_N)\\) (constante) es un intervalo aleatorio de la forma \\(\\big[ L(\\mathcal{S}), U(\\mathcal{S}) \\big]\\) (donde \\(L, U\\) son variables aleatorias que dependen de la muestra) tal que \\[ \\mathbb{P}\\Big( \\theta \\in \\big[ L(\\mathcal{S}), U(\\mathcal{S}) \\big]\\Big) = 1 - \\alpha \\] Notamos que lo aleatorio del intervalo son las cotas del mismo y que, dadas distintas muestras \\(\\mathcal{S}\\) el valor de interés \\(\\theta\\) no siempre va a caer ahí. La idea de un intervalo es poder dar una cota de más o menos dónde anda un valor. Veamos un ejemplo con el total. Recordamos que el estimador del total es insesgado \\(\\mathbb{E}\\big[ \\hat{t} \\big] = t\\) y que por definición: \\[ \\hat{t} = N \\frac{1}{n}\\sum\\limits_{i = 1}^N x_i \\cdot \\mathbb{I}_{\\mathcal{S}}(x_i) \\] luego usando la versión de muestreo finito del teorema central del límite (factorizando \\(N\\)) tenemos que: \\[ \\sqrt{\\frac{1}{\\textrm{Var}(\\bar{x}_{\\mathcal{S}})}} \\cdot \\Big( \\frac{1}{n}\\sum_{i = 1}^N x_i \\mathbb{I}_{\\mathcal{S}}(x_i) - \\bar{x}_{\\mathcal{U}}\\Big) = \\cdot N\\dfrac{\\Big( \\frac{1}{n}\\sum_{i = 1}^N x_i \\mathbb{I}_{\\mathcal{S}}(x_i) - \\bar{x}_{\\mathcal{U}}\\Big)}{N\\sqrt{\\textrm{Var}(\\bar{x}_{\\mathcal{S}})}} = \\dfrac{\\hat{t} - t}{\\sqrt{\\textrm{Var}(\\hat{t})}} \\mathrel{\\dot\\sim} \\textrm{Normal}(0,1) \\] De donde se sigue que si se desea tener un intervalo de tamali \\((1 - \\alpha) \\times 100 \\%\\) lo que hay que hacer es buscar \\(L(\\mathcal{S})\\) y \\(U(\\mathcal{S})\\) tales que: \\[ \\mathbb{P}\\Bigg( L(\\mathcal{S}) \\leq \\dfrac{\\hat{t} - t}{\\sqrt{\\textrm{Var}(\\hat{t})}} \\leq U(\\mathcal{S}) \\Bigg) = 1 - \\alpha \\] En este caso las probabilidades (por aproximación asintótica) se modelan bajo la hipótesis de normalidad. Y tomamos ventaja de que la normal es simétrica respecto a la media para proponer que \\(L(\\mathcal{S}) = -U(\\mathcal{S})\\) y ambas correspondan a \\(\\pm \\Phi^{-1}(\\alpha/2)\\) (la función de distirbución acumulada inversa de la normal). Es decir, ambos deben corresponder a los cuantiles con probabilidad \\(\\alpha/2\\) y \\(1 - \\alpha/2\\), denotados \\(z_{\\alpha/2}\\) y \\(z_{1 - \\alpha/2}\\). Por simetría de la normal tenemos que: \\(z_{\\alpha/2} = - z_{1 - \\alpha/2}\\) y por tanto: \\[ \\mathbb{P}\\Bigg( z_{\\alpha/2} \\leq \\dfrac{\\hat{t} - t}{\\sqrt{\\textrm{Var}(\\hat{t})}} \\leq z_{1 -\\alpha/2} \\Bigg) = 1 - \\alpha \\] de donde despejamos: \\[ \\mathbb{P}\\Bigg( z_{\\alpha/2}\\sqrt{\\textrm{Var}(\\hat{t})} \\leq \\hat{t} - t \\leq z_{1- \\alpha/2}\\sqrt{\\textrm{Var}(\\hat{t})} \\Bigg) = \\mathbb{P}\\Bigg( \\hat{t} - z_{1-\\alpha/2}\\sqrt{\\textrm{Var}(\\hat{t})} \\leq t \\leq \\hat{t} + z_{ \\alpha/2}\\sqrt{\\textrm{Var}(\\hat{t})} \\Bigg) = 1 - \\alpha \\] Notamos que como no conocemos \\(\\textrm{Var}(\\hat{t})\\) la podemos aproximar mediante \\(\\widehat{\\textrm{Var}}(\\hat{t})\\) (hay mejores aproximaciones mediante una \\(t\\) de Student asintótica pero no lo usaremos ahora) y tener intervalos aproximados de la forma: \\[\\begin{equation} \\begin{aligned} L(\\mathcal{S}) &amp; = \\hat{t} - z_{1-\\alpha/2}\\sqrt{\\widehat{\\textrm{Var}}(\\hat{t})} \\\\ U(\\mathcal{S}) &amp; = \\hat{t} + z_{1-\\alpha/2}\\sqrt{\\widehat{\\textrm{Var}}(\\hat{t})} \\end{aligned} \\end{equation}\\] de manera concisa muchas veces los escribimos como: \\[ \\hat{t} \\pm z_{1-\\alpha/2}\\sqrt{\\widehat{\\textrm{Var}}(\\hat{t})} \\] 3.5.2 Ejemplo con simulación: Veamos cómo se ven múltiples intervalos simulados con confianza del \\(90\\%\\) y suponiendo la varianza es conocida nsim &lt;- 100 n &lt;- 100 total.muestra &lt;- rep(NA, nsim) confianza.bajo &lt;- rep(NA, nsim) confianza.alto &lt;- rep(NA, nsim) f &lt;- n/N z &lt;- qnorm(1 - 0.1/2) var.total &lt;- N^2*(1 - f)/n*var(base.completa$x) for (i in 1:nsim){ muestra &lt;- sample(base.completa$x, n, replace = FALSE) total.muestra[i] &lt;- N*mean(muestra) #var.total[i] &lt;- N^2*(1 - f)/n*var(muestra) confianza.bajo[i] &lt;- total.muestra[i] - z*sqrt(var.total) confianza.alto[i] &lt;- total.muestra[i] + z*sqrt(var.total) } intervalos.simulados &lt;- data.frame( Simulacion = 1:nsim, Intervalo.Bajo = confianza.bajo, Total.Estimado = total.muestra, Intervalo.Alto = confianza.alto ) ggplot(intervalos.simulados) + geom_point(aes(x = Simulacion, y = Total.Estimado)) + geom_errorbar(aes(x = Simulacion, ymin = Intervalo.Bajo, ymax = Intervalo.Alto)) + geom_hline(aes(yintercept = sum(base.completa$x)), linetype = &quot;dashed&quot;, size = 1, color = &quot;red&quot;) + theme_bw() + ggtitle(&quot;Simulación de intervalos de confianza&quot;) Nota que estos intervalos son aproximados y no siempre van a funcionar. (¿Puedes hallar un ejemplo donde no sirvan a pesar de que \\(n\\) y \\(N\\) sean grandes?) Luego veremos correcciones a esto; por ahora, supondremos que la aproximación es buena. 3.6 Ejemplo Resumen: Estimación de una proporción bajo muestreo aleatorio simple sin reemplazo Se realiza una encuesta mediante muestreo aleatorio simple sin reemplazo a la población del ITAM \\(N = 5000\\) donde interesa conocer la proporción de gente que apoya al gobierno en turno \\(p\\). Implícitamente, se supone que alguien apoya (proporción \\(p\\) de toda la población) o no lo apoya (proporción \\(1-p\\)), que dichos conjuntos son disjuntos y que no hay una tercera opción (como NO RESPONDE / DESCONOCE QUIÉN GOBIERNA). La pregunta es: ¿a cuántas personas hay que encuestar si interesa estimar \\(p\\) con un error máximo de tamaño \\(\\epsilon = 0.05\\) al \\(99\\%\\) de confianza (es decir, que el estimador \\(\\hat{p}\\) de la proporción esté, a lo más, a \\(\\pm 0.05\\) de distancia del valor verdadero \\(p\\) con un intervalo de confianza al \\(99\\%\\))? Supongamos tomamos una muestra de tamaño \\(n\\) dada por \\(\\mathcal{S} = (x_1, x_2, \\dots, x_n)^T\\) de una población \\(\\mathcal{U} = (x_1, x_2, \\dots, x_N)^T\\) de tamaño \\(N\\). Pensemos, además, existen \\(N_1\\) personas que aprueban al gobierno actual y \\(N- N_1\\) que desaprueban del mismo y por tanto la proporción que nos interesa estimar es: \\[ p = \\dfrac{N_1}{N} \\] Por otro lado, la proporción muestral de personas que aprueban está dada por: \\[ \\hat{p} = \\dfrac{\\sum_{i = 1}^n \\mathbb{I}_{\\text{Aprueba}}(x_i)}{n} \\] donde si definimos \\(H = \\dfrac{\\sum_{i = 1}^n \\mathbb{I}_{\\text{Aprueba}}(x_i)}{n}\\) notamos que la distribución de \\(H\\) está dada por una variable Hipergeométrica (pues de una población de \\(N\\) se seleccionan \\(n\\) donde \\(N_1\\) cumplen la categoría deseada). Su media y varianza están dadas respectivamente por: \\[ \\mathbb{E}\\big[ H \\big] = n \\dfrac{N_1}{N} = np \\] así como por: \\[ \\textrm{Var}\\big[ H\\big] = n \\dfrac{N_1}{N} \\Big( 1 - \\dfrac{N_1}{N}\\Big) \\Big( \\dfrac{N-n}{N-1}\\Big) = np (1-p)\\Big( \\dfrac{N-n}{N-1}\\Big) \\] Se sigue entonces que \\(\\mathbb{E}\\big[\\hat{p}\\big] = p\\) y por tanto \\(\\hat{p}\\) es un estimador insesgado. La varianza por otro lado es: \\[ \\textrm{Var}\\big( \\hat{p} \\big) = \\dfrac{p(1-p)}{n}\\Big( \\dfrac{N-n}{N-1}\\Big) \\] Finalmente, el estimador de la varianza es: \\[ \\widehat{\\textrm{Var}}\\big( \\hat{p} \\big) = \\dfrac{\\hat{p}(1-\\hat{p})}{n}\\Big( \\dfrac{N-n}{N-1}\\Big) \\] el cual también cumple que es insesgado (demuéstralo). Podemos aplicar el Teorema Central del Límite para la proporción4 notando que la definición de \\(\\hat{p}\\) coincide con una media (de las indicadoras): \\[ \\underbrace{\\dfrac{\\hat{p} - p}{\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n} \\Big( \\frac{N-n}{N-1} \\Big)}}}_{\\widehat{\\text{Var}}(\\hat{p})}\\mathrel{\\dot\\sim} \\textrm{Normal}(0,1) \\] De donde se tiene que: \\[\\begin{equation} \\begin{aligned} &amp; \\mathbb{P}\\Bigg(- z_{\\alpha/2} \\leq \\dfrac{\\hat{p} - p}{\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n} \\Big( \\frac{N-n}{N-1} \\Big)}}\\leq z_{\\alpha/2}\\Bigg) \\approx 1 - \\alpha \\\\ \\Rightarrow &amp; \\mathbb{P}\\Bigg( \\hat{p} - z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n} \\Big( \\frac{N-n}{N-1} \\Big)} \\leq p \\leq \\hat{p} + z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n} \\Big( \\frac{N-n}{N-1} \\Big)}\\Bigg) \\approx 1 - \\alpha \\end{aligned} \\end{equation}\\] Nota Es común encontrar en Internet que para los intervalos de confianza la gente supone una población muy grande \\(N\\) respecto a la muestra \\(n\\) y entonces eliminan el término \\(\\frac{N-n}{N-1}\\) argumentando que \\(\\frac{N-n}{N-1} \\approx 1\\) y obtienen la siguiente fórmula: \\[ \\hat{p} \\pm z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\] esto simplifica algunos cálculos (a mano) pero nosotros tenemos R y podemos hacer cálculos más exactos sin tener que suponer semejantes atrocidades. Como el error deseado es de tamaño \\(\\epsilon\\) queremos \\(|p - \\hat{p} | \\leq \\epsilon\\) esto se traduce en: \\[\\begin{equation}\\nonumber |p - \\hat{p}| \\leq \\underbrace{z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n} \\Big( \\frac{N-n}{N-1} \\Big) }}_{\\epsilon} \\end{equation}\\] de donde igualamos para despejar la \\(n\\): \\[\\begin{equation}\\nonumber \\begin{aligned} \\epsilon &amp; = z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}\\Big( \\frac{N-n}{N-1} \\Big)} \\\\ &amp; = \\dfrac{\\epsilon^2 }{z_{\\alpha/2}^2} = \\frac{\\hat{p}(1-\\hat{p})}{n}\\Big( \\frac{N-n}{N-1} \\Big) \\\\ &amp; = \\frac{N-1}{\\hat{p}(1-\\hat{p})}\\dfrac{\\epsilon^2 }{z_{\\alpha/2}^2} = \\frac{N-n}{n} = \\frac{N}{n} - 1 \\\\ &amp; = \\frac{N-1}{\\hat{p}(1-\\hat{p})}\\dfrac{\\epsilon^2 }{z_{\\alpha/2}^2} + 1 = \\frac{N}{n} \\\\ &amp; \\Rightarrow n = \\dfrac{N}{\\frac{N-1}{\\hat{p}(1-\\hat{p})}\\frac{\\epsilon^2 }{z_{\\alpha/2}^2} + 1} = \\dfrac{\\frac{z^2_{\\alpha/2}}{\\epsilon^2}\\hat{p}(1-\\hat{p})}{\\frac{N-1}{N} + \\frac{1}{N}\\frac{z^2_{\\alpha/2}}{\\epsilon^2}\\hat{p}(1-\\hat{p})} = \\dfrac{m}{1 + \\frac{m-1}{N}} \\end{aligned} \\end{equation}\\] donde \\[ m = \\frac{z^2_{\\alpha/2}}{\\epsilon^2}\\hat{p}(1-\\hat{p}) \\] Ahora el problema es que el tamaño de muestra \\(n\\) depende de la muestra a través de \\(\\hat{p}\\) ¡y no hemos tomado la muestra! Para ello entonces analizamos el peor caso que puede ocurrir de \\(\\hat{p}\\) de tal forma que obtengamos la \\(n\\) que puede salir con la peor proporción \\(\\hat{p}\\) posible. Para ello maximizamos con derivadas: \\[\\begin{equation}\\nonumber \\begin{aligned} \\dfrac{\\partial n}{\\partial \\hat{p}} &amp; = \\dfrac{\\partial}{\\partial \\hat{p}} \\Bigg( \\dfrac{N}{\\frac{N-1}{\\hat{p}(1-\\hat{p})}\\frac{\\epsilon^2 }{z_{\\alpha/2}^2} + 1} \\Bigg) \\\\ &amp; = N \\Bigg( \\dfrac{1}{\\frac{N-1}{\\hat{p}(1-\\hat{p})}\\frac{\\epsilon^2 }{z_{\\alpha/2}^2} + 1} \\Bigg)^2 \\cdot \\dfrac{\\partial}{\\partial \\hat{p}} \\Bigg( \\frac{N-1}{\\hat{p}(1-\\hat{p})}\\frac{\\epsilon^2 }{z_{\\alpha/2}^2} + 1\\Bigg) \\\\ &amp; = \\underbrace{N (N-1)\\frac{\\epsilon^2 }{z_{\\alpha/2}^2}}_{C} \\Bigg( \\dfrac{1}{\\frac{N-1}{\\hat{p}(1-\\hat{p})}\\frac{\\epsilon^2 }{z_{\\alpha/2}^2} + 1} \\Bigg)^2 \\cdot \\dfrac{\\partial}{\\partial \\hat{p}} \\Bigg( \\frac{1}{\\hat{p}(1-\\hat{p})}\\Bigg) \\\\ &amp; = C \\Bigg( \\dfrac{1}{\\frac{N-1}{\\hat{p}(1-\\hat{p})}\\frac{\\epsilon^2 }{z_{\\alpha/2}^2} + 1} \\Bigg)^2 \\Bigg( \\frac{1}{\\hat{p}(1-\\hat{p})}\\Bigg)^2 \\dfrac{\\partial}{\\partial \\hat{p}} \\hat{p}(1-\\hat{p}) \\\\ &amp; = C \\Bigg( \\dfrac{1}{\\frac{N-1}{\\hat{p}(1-\\hat{p})}\\frac{\\epsilon^2 }{z_{\\alpha/2}^2} + 1} \\Bigg)^2 \\Bigg( \\frac{1}{\\hat{p}(1-\\hat{p})}\\Bigg)^2 (1-2\\hat{p}) = 0 \\end{aligned} \\end{equation}\\] de donde se sigue que \\(\\hat{p} = \\frac{1}{2}\\) es un punto crítico. De hecho puede verificarse que es el máximo (por ejemplo a través de la segunda derivada). Luego, podemos estimar la \\(n\\) de la muestra mediante: \\[ n = \\left\\lceil \\dfrac{m}{1 + \\frac{m-1}{N}} \\right\\rceil \\] donde \\(m = \\dfrac{1}{4}\\frac{z^2_{\\alpha/2}}{\\epsilon^2}\\). En el caso particular de este ejercicio, \\(N = 5000\\), \\(\\epsilon = 0.05\\), \\(\\alpha = 0.01\\) y \\(z^2_{\\alpha/2} \\approx\\) qnorm(0.9). Luego podemos calcular: alpha &lt;- 0.01 z &lt;- qnorm(1 - alpha/2) epsilon &lt;- 0.05 m &lt;- (1/4)*(z/epsilon)^2 N &lt;- 5000 n &lt;- ceiling(m/(1 + (m-1)/N)) print(paste0(&quot;El tamaño de muestra es &quot;, n)) ## [1] &quot;El tamaño de muestra es 586&quot; 3.7 Ejemplo Resumen: Estimación del total de individuos en una fotografía En este ejercicio vamos a determinar cuánta gente aparece en la siguiente foto: Figure 3.1: Imagen de un concierto extraída de https://www.youtube.com/watch?v=pJ1YKwyH5bk Hay varias opciones para determinar la cantidad de gente que está en dicha foto. Una sería contar todas las cabecitas que aparecen; otra, diseñar un modelo de redes neuronales (o de convolusión porque a la gente le encanta eso) que identifique una cabeza y la cuente. Nosotros lo que haremos (por ser un curso de estadística) será muestrear. Como investigador me interesa responder la siguiente pregunta: ¿Cuánta gente está en la fotografía con un intervalo de error de \\(\\pm 50\\) casos al 95%? Para ello dividiremos la fotografía en \\(N\\) pedazos (a determinar), muestrearemos \\(n\\) de ellos y contaremos la cantidad de personas que aparecen en cada pedazo. Finalmente, generamos intervalos de confianza y de muestreo. Para ello repetimos el ejercicio anterior de despejar la \\(n\\) del intervalo de confianza; por el teorema del límite central tenemos: \\[ \\dfrac{\\hat{t} - t}{\\sqrt{\\textrm{Var}(\\hat{t})}} ~\\sim \\textrm{Normal}(0,1) \\] de donde obtenemos intervalos (¡verifícalo!) de la forma: \\[ \\hat{t} \\pm z_{1-\\alpha/2}\\cdot\\sqrt{\\textrm{Var}(\\hat{t})} \\] Donde podemos aproximar la varianza mediante \\(\\widehat{\\text{Var}}(\\hat{t}) = N^2\\dfrac{1-f}{n} s^2_{x,\\mathcal{S}}\\) donde recordamos que \\(f = n/N\\) y \\(s^2_{x,\\mathcal{S}}\\) es la varianza muestral. Tomamos \\(\\epsilon = 50\\) y despejamos: \\[\\begin{equation}\\nonumber \\begin{aligned} \\epsilon &amp; = z_{1-\\alpha/2}\\cdot\\sqrt{\\textrm{Var}(\\hat{t})} \\\\ \\Rightarrow \\dfrac{\\epsilon^2}{z_{1-\\alpha/2}^2} &amp; = N^2\\dfrac{1-f}{n} s^2_{x,\\mathcal{S}} \\\\ \\Rightarrow \\dfrac{\\epsilon^2}{z_{1-\\alpha/2}^2 s^2_{x,\\mathcal{S}} N^2} &amp; = \\dfrac{1-\\frac{n}{N}}{n} \\\\ \\Rightarrow \\dfrac{\\epsilon^2}{z_{1-\\alpha/2}^2 s^2_{x,\\mathcal{S}} N^2} &amp; = \\dfrac{1}{n} - \\dfrac{1}{N} \\\\ \\Rightarrow \\dfrac{\\epsilon^2}{z_{1-\\alpha/2}^2 s^2_{x,\\mathcal{S}} N^2} + \\dfrac{1}{N} &amp; = \\dfrac{1}{n} \\\\ \\Rightarrow \\dfrac{1}{N} \\Bigg( \\dfrac{\\epsilon^2}{z_{1-\\alpha/2}^2 s^2_{x,\\mathcal{S}} N} + 1 \\Bigg) &amp; = \\dfrac{1}{n} \\\\ \\Rightarrow \\dfrac{1}{N} \\Bigg( \\dfrac{\\epsilon^2 + z_{1-\\alpha/2}^2 s^2_{x,\\mathcal{S}} N}{z_{1-\\alpha/2}^2 s^2_{x,\\mathcal{S}} N} \\Bigg) &amp; = \\dfrac{1}{n} \\\\ \\Rightarrow \\Bigg( \\dfrac{(z_{1-\\alpha/2} s_{x,\\mathcal{S}} N)^2} {\\epsilon^2 + z_{1-\\alpha/2}^2 s^2_{x,\\mathcal{S}} N}\\Bigg) &amp; = n \\\\ \\end{aligned} \\end{equation}\\] El problema aquí es que la \\(n\\) depende de la varianza muestral \\(s^2_{x,\\mathcal{S}}\\) (actualmente desconocida) así como de la cantidad de cuadritos originales \\(N\\) en los que dividimos la foto. Hay en la literatura varias técnicas que se pueden utilizar para estimar el \\(s^2_{x,\\mathcal{S}}\\): Realizar un estudio piloto (es decir un pequeño ejemplo de lo que vas a hacer en una población chica y de ahí tener la varianza). Esta es la mejor opción. Buscar otros estudios similares donde se analicen objetos similares de estudio y ver sus varianzas; suponer que la de este estudio es similar. Esta es la segunda mejor opción. Inventártela (sí, es una opción pero no la mejor). Vamos, ¿cuál es la probabilidad de que nadie en todo el mundo haya hecho un análisis similar al tuyo? Si realmente estás haciendo algo completamente nuevo sin estudio piloto pues… podrías inventarla. ¿Lo recomiendo? No; pero pasa. En nuestro caso utilizaremos la varianza estimada de este artículo reportada en \\(1.02\\); luego \\(s^2_{x,\\mathcal{S}} \\approx 1.02\\) para nuestro análisis. Finalmente, como éste es sólo un ejercicio de clase tomaremos \\(N = 100\\) (dividir la foto en \\(100\\) cuadritos). De manera profesional, de nuevo habría que ver diferencias en los resultados de las estimaciones en función de los cuadritos, o bien asignar un costo a la cantidad de cuadros. Concluimos entonces que para nuestro estudio: \\[ n = \\left\\lceil \\dfrac{(z_{1-\\alpha/2} s_{x,\\mathcal{S}} N)^2} {\\epsilon^2 + z_{1-\\alpha/2}^2 s^2_{x,\\mathcal{S}} N}\\right\\rceil = \\left\\lceil \\dfrac{(1.95\\cdot \\sqrt{1.02} \\cdot 100)^2} {50^2 + 1.95^2\\cdot 1.02 \\cdot 100}\\right\\rceil \\] Podemos calcular en R: n &lt;- ceiling((qnorm(0.975)*sqrt(1.02)*100)^2/(50^2 + (qnorm(0.975)^2*1.02*100))) print(paste0(&quot;El tamaño de muestra es &quot;, n)) ## [1] &quot;El tamaño de muestra es 14&quot; Podemos proceder a dividir la foto en los \\(N = 100\\) pedazos: #División con base en el siguiente link: #https://rpubs.com/issactoast/cutimage library(imager) #Cargamos la imagen img &lt;- load.image(&quot;images/concierto.jpg&quot;) #Función auxiliar del link superior make.vr &lt;- function( x, name ){ assign( name, x, envir = .GlobalEnv) } #División en N N &lt;- 100 par(mfrow=c(sqrt(N),sqrt(N)), mar = c(0.1,0.1,0.1,0.1)) k &lt;- 1 for (j in 1:sqrt(N)){ for (i in 1:sqrt(N)){ vr.name &lt;- paste0(&quot;sub&quot;, k) k &lt;- k + 1 imsub(img, (width/sqrt(N))*(i-1) &lt; x &amp; x &lt; i * (width/sqrt(N)), (height/sqrt(N))*(j-1) &lt; y &amp; y &lt; j * (height/sqrt(N))) %&gt;% make.vr(name = vr.name) %&gt;% # save.image( file = paste0(vr.name,&quot;.jpg&quot;)) %&gt;% plot(axes = FALSE, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, ann = FALSE ) } } Podemos acceder a cada una de las imágenes que se tienen a través de su nombre (sub seguido de un número entre \\(0\\) y \\(100\\)). Muestreamos entonces los nombres de las 15 imágenes: #Obtenemos los dígitos a muestrear imagenes.muestreadas &lt;- sample(1:100, n, replace = FALSE) #Agregamos el prefijo sub imagenes.muestreadas &lt;- paste0(&quot;sub&quot;, imagenes.muestreadas) Y graficamos cada una de ellas: par(mfrow = c(1,1)) for (imagen in imagenes.muestreadas){ plot(get(imagen), main = imagen, axes = FALSE) } Para cada una de las imágenes contamos las cabecitas que aparecen: datos &lt;- data.frame( Imagen = imagenes.muestreadas, Conteo = c(13, 11, 9, 14, 9, 15, 14, 10, 1, 22, 8, 9, 17, 16) ) kable(datos) %&gt;% kable_styling(latex_options = &quot;striped&quot;) Imagen Conteo sub52 13 sub43 11 sub34 9 sub35 14 sub54 9 sub23 15 sub83 14 sub14 10 sub79 1 sub5 22 sub30 8 sub36 9 sub39 17 sub16 16 Tenemos entonces que la estimación del total \\(\\hat{t}\\) es: 1200, por otro lado la varianza muestral es \\(s_{x,\\mathcal{S}}\\) está dada por: 25.2307692. Podemos entonces establecer un intervalo de confianza para el total: x &lt;- c(13, 11, 9, 14, 9, 15, 14, 10, 1, 22, 8, 9, 17, 16, 10) s2 &lt;- var(x) N &lt;- 100 n &lt;- 15 total.muestra &lt;- N*mean(x) ci &lt;- qnorm(0.975)*sqrt(N^2*(1 - n/N)/n*s2) ci_low &lt;- round(total.muestra - ci,2) ci_up &lt;- round(total.muestra + ci,2) print(paste0(&quot;Se estiman &quot;, round(total.muestra,2), &quot; personas con intervalo de &quot;, &quot;confianza al 95% de [&quot;, ci_low, &quot; ,&quot;, ci_up,&quot;]&quot;)) ## [1] &quot;Se estiman 1186.67 personas con intervalo de confianza al 95% de [959.55 ,1413.78]&quot; 3.8 Ejercicio: Cuando se resgistra un paquete de R en CRAN estos se registran junto con sus autores como muestra la imagen: La información de un paquete puede encontrarse en la página de CRAN dando clic en Packages y luego en Table of available packages, sorted by name y buscando el paquete deseado. Se desea conocer el número promedio de autores por paquete registrado en CRAN con un intervalo de confianza al 80% y un error de \\(\\pm 1\\). Obtén la \\(n\\) necesaria para muestrear, calcula un estimador de la media y obtén intervalos de confianza. Justifica tu elección de la varianza para la \\(n\\) mediante un estudio piloto (muestreando de manera inicial \\(10\\) y calculando la varianza de ellos). Hint Para obtener una lista (censo) de todos los paquetes de R puedes utilizar la función available.packages() la cual devuelve una matriz con todos los paquetes e incluye la url de donde se encuentra. 3.9 Ejemplo Resumen: Estimación de una región crítica En una elección existen dos candidatas \\(A\\) y \\(B\\). Se realiza una encuesta de opinión mediante muestreo aleatorio simple sin reemplazo donde se les pregunta a una cantidad suficiente de votantes por quién votarían de las dos. En este análisis no hay NO SABE / NO RESPONDE sino que todos los individuos indican su preferencia. Se desea determinar la cantidad de puntos porcentuales que debe haber de diferencia entre la proporción de individuos que reportan apoyan al candidato \\(A\\) y los que reportan que apoyan al \\(B\\) de tal forma que el \\(95\\%\\) de las veces podamos declarar de manera adecuada al ganador. Nota Si \\(A\\) no es el ganador entonces \\(p_A &lt; 50\\%\\) (la proporción de votantes que van a elegir a \\(A\\) es menor a la mitad) ¿cierto? Para ello el análisis es como sigue: sea \\(\\hat{p}_A\\) un estimador de la proporción de individuos que van a elegir a \\(A\\) y \\(p_A\\) la verdadera proporción. Sin pérdida de generalidad supondremos que \\(B\\) es el ganador; es decir que \\(p_A &lt; 0.5\\). El problema puede traducirse en determinar una \\(c\\) tal que: \\[ \\mathbb{P}\\big( \\hat{p}_A &gt; c | p_A &lt; 0.5 \\big) \\leq 0.05 \\] Notamos que el evento \\(\\{ p_A &lt; 50\\%\\}\\) es por definición conocido (con probabilidad \\(0\\) ó \\(1\\)) pues está dado por la población (constante). Notamos que por el teorema del límite central podemos escribir: \\[ \\dfrac{\\hat{p}_A - p_A}{\\sqrt{\\text{Var}(\\hat{p}_A )}}\\sim \\text{Normal}\\big(0, 1\\big) \\] donde \\(\\hat{p}_A = \\frac{1}{N} \\sum_{i = 1}^N x_i \\mathbb{I}_{\\mathcal{S}}(x_i)\\) como anteriormente hicimos para proporciones y su varianza está dada por: \\[ \\text{Var}(\\hat{p}_A ) = \\frac{p_A(1-p_A)}{n}\\Big( \\frac{N-1}{N-n}\\Big) \\] donde el cálculo se hizo en el primer ejemplo de esta sección. Podemos transformar el problema entonces en hallar \\(c\\) tal que: \\[ \\mathbb{P}\\bigg( \\underbrace{\\frac{\\hat{p}_A - p_A}{\\sqrt{\\text{Var}(\\hat{p}_A )}}}_{Z \\sim \\text{Normal}(0,1)} &gt; \\frac{c - p_A}{\\sqrt{\\text{Var}(\\hat{p}_A)}} \\bigg| p_A &lt; 0.5 \\bigg) \\leq 0.05 \\] Notamos que el lado izquierdo tiene una aproximación normal y entonces podemos reescribir el problema como hallar \\(c\\) tal que: \\[ \\mathbb{P}\\bigg( Z &gt; \\frac{c - p_A}{\\sqrt{\\text{Var}(\\hat{p}_A)}} \\bigg| p_A &lt; 0.5 \\bigg) \\leq 0.05 \\qquad \\text{ donde } Z \\sim \\text{Normal}(0,1). \\] Recordando la expresión para la varianza sustituyo: \\[ \\mathbb{P}\\left( Z &gt; \\dfrac{c - p_A}{\\sqrt{\\frac{p_A(1-p_A)}{n}\\Big( \\frac{N-1}{N-n}\\Big)}} \\Bigg| p_A &lt; 0.5 \\right) \\leq 0.05 \\qquad \\text{ donde } Z \\sim \\text{Normal}(0,1). \\] En función del análisis pasado, observamos que \\(\\dfrac{c - p_A}{\\sqrt{\\frac{p_A(1-p_A)}{n}\\Big( \\frac{N-1}{N-n}\\Big)}}\\) es una función decreciente en términos de \\(p_A\\) (¡compruébalo!) y que el mínimo valor se alcanza en el máximo de la \\(p_A\\) en el intervalo; es decir cuando \\(p_A = \\frac{1}{2}\\). Luego el problema se transforma en hallar \\(c\\) tal que: \\[ \\mathbb{P}\\left( Z &gt; \\dfrac{c - \\frac{1}{2}}{\\sqrt{\\frac{\\frac{1}{2}(1-\\frac{1}{2})}{n}\\Big( \\frac{N-1}{N-n}\\Big)}} \\right) \\leq 0.05 \\qquad \\text{ donde } Z \\sim \\text{Normal}(0,1). \\] donde eliminamos el evento \\(p_A &lt; 0.5\\) por ser un evento seguro. Reescribimos el evento: \\[ \\underbrace{\\mathbb{P}\\left( Z &lt; \\dfrac{c - \\frac{1}{2}}{\\sqrt{\\frac{\\frac{1}{2}(1-\\frac{1}{2})}{n}\\Big( \\frac{N-1}{N-n}\\Big)}} \\right)}_{\\Phi(x)} \\geq 0.95 \\qquad \\text{ donde } x = \\dfrac{c - \\frac{1}{2}}{\\sqrt{\\frac{\\frac{1}{2}(1-\\frac{1}{2})}{n}\\Big( \\frac{N-1}{N-n}\\Big)}} \\] de tal forma que descubrimos la acumulada de la normal; terminamos de escribir todo: \\[ \\Phi(x) \\geq 0.95 \\] donde aplicamos la función inversa de la acumulada de la normal para descubrir: \\[ \\dfrac{c - \\frac{1}{2}}{\\sqrt{\\frac{\\frac{1}{2}(1-\\frac{1}{2})}{n}\\Big( \\frac{N-1}{N-n}\\Big)}} \\geq \\phi^{-1}(0.95) \\Rightarrow c = \\frac{1}{2} + \\phi^{-1}(0.95)\\sqrt{\\frac{\\frac{1}{2}(1-\\frac{1}{2})}{n}\\Big( \\frac{N-1}{N-n}\\Big)} \\] de donde se sigue que: \\[ \\hat{p}_{A} &gt; \\frac{1}{2}\\Bigg(1 + \\phi^{-1}(0.95)\\sqrt{\\frac{N-1}{n(N-n)}} \\Bigg) \\Rightarrow 2\\hat{p}_A = 1 + \\phi^{-1}(0.95)\\sqrt{\\frac{N-1}{n(N-n)}} \\] Notando que los puntos porcentuales de \\(B\\) estimados mediante \\(\\hat{p}_B\\) tienen la forma: \\[ \\hat{p}_B = 1 - \\hat{p}_A \\] se tiene entonces que la diferencia entre puntos para determinar quien gana es: \\[ \\hat{p}_A - \\hat{p}_B = 2\\hat{p}_A - 1 \\geq \\phi^{-1}(0.95)\\sqrt{\\frac{N-1}{n(N-n)}} \\] El mismo análisis se seguiría bajo la hipótesis de que el perdedor es \\(B\\); por tanto se tiene que cumplir que: \\[ | \\hat{p}_A - \\hat{p}_B | \\geq \\phi^{-1}(0.95)\\sqrt{\\frac{N-1}{n(N-n)}} \\] para poder declarar como ganador a aquél con más puntos porcentuales de manera correcta con una confianza del \\(95\\%\\). 3.10 Ejemplo Resumen: Estimación del total de una población Consideremos una población de tiburones donde se desconoce el tamaño total de la población \\(N\\). Algunas veces para determinar el tamaño poblacional se utiliza un modelo de captura y recaptura. En él se capturan \\(\\ell\\) individuos los cuales se identifican (mediante etiquetas, por ejemplo) y se devuelven a convivir entre la población de \\(N\\) para mezclarse de vuelta. Una vez mezclados, seleccionamos \\(n\\) nuevos individuos por muestreo aleatorio simple sin reemplazo donde descubrimos que \\(K\\) están marcados. Suponiendo que \\(K \\neq 0\\), determinaremos un estimador \\(\\hat{N}\\) del total poblacional (en el caso \\(K = 0\\) tuvimos muy mala suerte y seguimos recapturando tiburones hasta encontrar alguno). En primer lugar notamos que los \\(K\\) marcados que surgen en la segunda muestra siguen una distribución hipergeométrica: \\[ \\mathbb{P}\\big( K = x) = \\dfrac{\\binom{\\ell}{x} \\binom{N-\\ell}{n-x}}{\\binom{N}{n}} \\] donde \\(x \\in \\big[ \\max\\{ 0, \\ell-N+n\\}, \\min\\{n,\\ell\\}\\big]\\cap\\mathbb{N}\\). Para construir el estimador notamos que: \\[ \\mathbb{E}(K) = n \\frac{\\ell}{N} \\] de donde podemos despejar \\(N\\): \\[ N= n \\frac{\\ell}{\\mathbb{E}(K) } \\] Ahora bien, dada una muestra donde se obtuvieron \\(K\\) (de \\(n\\)) marcados se propone un estimador de \\(N\\) dado por: \\[ \\hat{N} = \\ell \\cdot \\frac{n}{K} \\] donde \\(K = \\sum_{i = 1}^n x_i\\) donde las \\(x_i = 1\\) si estaba marcado y \\(x_i = 0\\) si no lo estaba. La \\(K\\) de hecho depende de la muestra y se puede escribir como: \\[ K = \\sum_{i = 1}^N x_i\\mathbb{I}_{\\mathcal{S}}(x_i) \\] Para estimar si \\(\\hat{N}\\) es insesgado, habría que calcular su valor esperado condicional en que \\(K &gt; 0\\). Para ello notamos que: \\[ \\mathbb{E}\\big[ \\hat{N} | K &gt; 0\\big] =(\\ell n) \\cdot \\mathbb{E}\\big[ \\frac{1}{K} \\big| K &gt; 0 \\big] \\] Sabemos (por la desigualdad de Jensen) que \\(\\mathbb{E}\\big[ \\frac{1}{K} \\big] \\neq \\dfrac{1}{\\mathbb{E}[K]}\\) por lo cual aproximamos el valor esperado mediante una expansión de Taylor; es decir para una función \\(f \\in \\mathcal{C}^2\\): \\[ \\mathbb{E}\\big[ f(X) \\big] \\approx \\mathbb{E}\\big[ f(\\mu) + (X - \\mu) f&#39;(\\mu) + (X - \\mu)^2 f&#39;&#39;(\\mu)\\big] = f(\\mu) + \\text{Var}\\big[X\\big] f&#39;&#39;(\\mu) \\] donde \\(\\mu = \\mathbb{E}\\big[X\\big]\\). En nuestro caso \\(f(k) = \\frac{1}{k}\\) y por tanto: \\[ \\mathbb{E}\\big[ \\frac{1}{K} \\big| K &gt; 0 \\big]\\approx \\dfrac{1}{\\mathbb{E}\\big[ K | K &gt; 0]} + 2 \\cdot \\dfrac{\\text{Var}\\big[K | K &gt; 0\\big] }{\\big(\\mathbb{E}\\big[ K | K &gt; 0]\\big)^3} = \\dfrac{1}{\\mu} + 2 \\dfrac{\\sigma^2}{\\mu^3} \\] Calculamos los valores esperados: \\[ \\mathbb{E}\\big[K\\big] = \\underbrace{\\mathbb{E}\\big[K | K = 0\\big]\\mathbb{P}(K = 0)}_{=0} + \\mathbb{E}\\big[K | K &gt; 0\\big]\\mathbb{P}(K &gt; 0) \\Rightarrow \\mathbb{E}\\big[K | K &gt; 0\\big] = \\frac{\\ell n}{N} \\dfrac{1}{\\mathbb{P}(K &gt; 0)} \\] de donde se sigue que: \\[ \\mathbb{E}\\big[K | K &gt; 0\\big] = \\frac{\\ell n}{N} \\dfrac{1}{1 - \\mathbb{P}(K = 0)} = \\dfrac{\\ell n}{N} \\dfrac{1}{1 - \\frac{\\binom{N-\\ell}{n}}{\\binom{N}{n}} } = \\dfrac{\\ell n}{N} \\cdot \\dfrac{\\binom{N}{n}}{\\binom{N}{n} - \\binom{N-\\ell}{n}} = \\mu \\] Por otro lado el cálculo de la varianza: \\[\\begin{equation}\\nonumber \\begin{aligned} \\text{Var}\\big[K | K &gt; 0\\big] &amp; =\\mathbb{E}\\big[K^2 | K &gt; 0] - \\mathbb{E}\\big[K | K &gt; 0]^2\\\\ &amp; = \\dfrac{\\mathbb{E}\\big[K^2]}{\\mathbb{P}(K &gt; 0)} - \\mu^2 \\\\ &amp; = \\dfrac{\\text{Var}[K] + \\mathbb{E}[K]^2}{1 - \\mathbb{P}(K = 0)} - \\mu^2 \\\\ &amp; = \\dfrac{\\text{Var}[K] + \\Big(n\\frac{\\ell}{N}\\Big)^2}{1 - \\mathbb{P}(K = 0)} - \\mu^2\\\\ &amp; = \\dfrac{\\frac{n\\ell}{N} \\cdot \\frac{(N-\\ell)}{N} \\cdot \\Big( \\frac{N-n}{N-1} \\Big) + \\Big(n\\frac{\\ell}{N}\\Big)^2}{1 - \\mathbb{P}(K = 0)} - \\mu^2 \\\\ &amp; = \\dfrac{\\frac{n\\ell}{N} \\cdot \\frac{(N-\\ell)}{N} \\cdot \\Big( \\frac{N-n}{N-1} \\Big) + \\Big(n\\frac{\\ell}{N}\\Big)^2}{1 - \\frac{\\binom{N-\\ell}{n}}{\\binom{N}{n}}} - \\mu^2 \\\\ &amp; = \\binom{N}{n} \\dfrac{\\frac{n\\ell}{N} \\cdot \\frac{(N-\\ell)}{N} \\cdot \\Big( \\frac{N-n}{N-1} \\Big) + \\Big(n\\frac{\\ell}{N}\\Big)^2}{\\binom{N}{n} - \\binom{N-\\ell}{n}}- \\mu^2 &amp; = \\sigma^2\\\\ \\end{aligned} \\end{equation}\\] Donde se tiene entonces que: \\[ \\mathbb{E}\\big[ \\hat{N} | K &gt; 0\\big] \\approx (\\ell n) \\Bigg[ \\cdot \\dfrac{1}{\\mathbb{E}\\big[ K | K &gt; 0]} + 2 \\cdot \\dfrac{\\text{Var}\\big[K | K &gt; 0\\big] }{\\big(\\mathbb{E}\\big[ K | K &gt; 0]\\big)^3} \\Bigg] \\] con los valores estimados en los renglones anteriores. En particular, \\(\\hat{N}\\) no es insesgado pero puede demostrarse que en el límite \\(\\lim_{\\substack{n \\to \\infty \\\\ N-n\\to\\infty}}\\) lo es. De manera similar puede obtenerse (ver Lohr capítulo 13): \\[ \\text{Var}\\big[ \\hat{N} | K &gt; 0\\big]\\approx \\Big(\\dfrac{n \\ell}{K}\\Big)^2 \\dfrac{\\ell - K}{K(\\ell - 1)} \\] Misma que puede utilizarse para los intervalos de confianza. 3.11 Demostración del Teorema del Límite Central para Muestras Finitas PRONTO 3.12 Muestreo Aleatorio Simple Bernoulli (BE) En un esquema de muestreo Bernoulli (BE) se tiene una población de tamaño \\(N\\in\\mathbb{N}\\) (constante) la cual se enlista de manera ordenada \\(U = (x_1,x_2,\\dots,x_N)^T\\). Se recorre la lista de \\(1\\) hasta \\(N\\). Cada elemento de la población, se selecciona y se mide con probabilidad \\(\\pi \\in (0,1)\\) para generar una muestra \\(\\mathcal{S} = (x_1, x_2, \\dots, x_n)^T\\) de tamaño \\(n = n(\\mathcal{S})\\) aleatorio (con \\(0 \\leq n(\\mathcal{S}) \\leq N\\)). Un ejemplo de muestreo Bernoulli ocurre en las aduanas del Sistema de Administración Tributaria (SAT) donde con probabilidad \\(\\pi\\) se revisa la mercancía de un viajero (de un total predefinido de \\(N\\) viajeros) para verificar no haya contrabando y con probabilidad \\(1-\\pi\\) se le deja entrar al país sin revisar su mercancía. Un muestreo Bernoulli no necesariamente tiene muestras del mismo tamaño: como el que cada elemento esté en la muestra depende de \\(\\pi\\) entonces \\(n(\\mathcal{S})\\) es una variable aleatoria con distribución Binomial: \\[ n(\\mathcal{S})\\sim \\textrm{Binomial}(N, \\pi) \\] con media y varianza dadas por: \\[ \\mathbb{E}\\Big[ n(\\mathcal{S})\\Big] = N\\pi \\quad \\text{ y } \\quad \\text{Var}\\Big[ n(\\mathcal{S})\\Big] = N\\pi(1 - \\pi) \\] Una forma de muestrear de un muestreo Bernoulli es recorrer uno a uno los elementos de la muestra y generar una variable aleatoria \\(B_i \\sim \\textrm{Bernoulli}(\\pi)\\) de tal forma que si \\(B_i = 1\\) se incluye el elemento en la muestra. Este esquema está programado en R como sigue: datos &lt;- data.frame(Edad = c(10, 12, 5, 4, 1, 3, 14), Raza = c(&quot;Labrador&quot;, &quot;Pomeranio&quot;,&quot;Labrador&quot;, &quot;Pastor Alemán&quot;, &quot;Bulldog&quot;,&quot;Bulldog&quot;, &quot;Chihuahua&quot;)) datos$en_muestra &lt;- 0 proba &lt;- 3/4 for (i in 1:nrow(datos)){ Bi &lt;- sample(c(0,1), 1, prob = c(1 - proba, proba)) datos$en_muestra[i] &lt;- Bi } muestra &lt;- datos %&gt;% filter(en_muestra == 1) Bajo este esquema se tiene que: \\[ \\pi_k = \\mathbb{P}(x_k \\in \\mathcal{S}) = \\pi \\qquad \\forall k \\] Además en este caso las \\(\\{ \\mathbb{I}_{\\mathcal{S}}(x_k) \\}_k\\) son independientes y por tanto: \\[ \\pi_{k,l} = \\pi^2 \\] En caso de muestreo aleatorio Bernoulli tenemos que un estimador del total es de la misma forma que en el caso de muestreo aleatorio simple: \\[ \\hat{t}_{\\pi} = \\frac{1}{\\pi} \\sum\\limits_{i = 1}^{n(\\mathcal{S})} x_i \\] El cual es insesgado pues usando indicadoras reescribimos \\(\\hat{t}_{\\pi} = \\frac{1}{\\pi} \\sum\\limits_{i = 1}^N x_i \\mathbb{I}_{\\mathcal{S}}(x_i)\\) y tomamos valor esperado: \\[ \\mathbb{E}\\Big[ \\hat{t}_{\\pi} \\Big] = \\frac{1}{\\pi} \\sum\\limits_{i = 1}^N x_i \\mathbb{E}\\Big[\\mathbb{I}_{\\mathcal{S}}(x_i)\\Big] =\\frac{1}{\\pi}\\sum\\limits_{i = 1}^N x_i\\pi = \\sum\\limits_{i = 1}^N x_i = t \\] por otro lado su varianza está dada por: \\[ \\textrm{Var}_{\\text{BE}}(\\hat{t}_{\\pi}) = \\Big( \\frac{1}{\\pi} - 1\\Big)\\sum\\limits_{i = 1}^N x_i^2 \\] la cual puede estimarse de manera insesgada mediante: \\[ \\widehat{\\textrm{Var}}_{\\text{BE}}(\\hat{t}_{\\pi}) = \\frac{1}{\\pi}\\Big( \\frac{1}{\\pi} - 1\\Big)\\sum\\limits_{i = 1}^{n(\\mathcal{S})} x_i^2 \\] 3.12.1 Ejercicio Demuestra la expresión para \\(\\textrm{Var}_{\\text{BE}}(\\hat{t}_{\\pi})\\) Demuestra que \\(\\widehat{\\textrm{Var}}_{\\text{BE}}(\\hat{t}_{\\pi})\\) es un estimador insesgado de \\(\\textrm{Var}_{\\text{BE}}(\\hat{t}_{\\pi})\\). 3.12.2 Ejemplo Consideraremos un ejemplo presentado por Särndal et al. Un profesor corrige 600 exámenes. Quiere tener un estimado de la calificación de sus alumnos y para ello cada que aparece un examen tira un dado justo de \\(6\\) caras y si sale un \\(6\\) corrige dicho examen; en caso contrario lo deja pasar. Al final del análisis el profe obtiene una muestra de \\(90\\) estudiantes de los cuales \\(60\\) pasaron. Asignamos \\(x_i = 0\\) si un alumno no pasó y \\(x_i = 1\\) si pasó; de esta forma la estimación de la cantidad de alumnos que pasaron es un total dado por: \\[ \\hat{t} = \\frac{1}{\\pi} \\sum\\limits_{i = 1}^{90} x_i = \\dfrac{1}{\\frac{1}{6}} 60 = 360 \\] El profe, después de pensarlo un rato se le ocurre otra manera de estimar la proporción de los alumnos que pasaron. Si pasaron \\(60/90\\) se tiene entonces que \\(2/3\\) de los alumnos pasan; aplicando el \\(2/3\\) a los \\(600\\) alumnos que tiene un estimador alternativo del total sería: \\[ \\hat{t}_{\\text{Alt}} = \\dfrac{2}{3}\\cdot 600 = 400 \\] El cual escrito en términos de las variables utilizadas es: \\[ \\hat{t}_{\\text{Alt}} = \\begin{cases} \\frac{N}{n(\\mathcal{S})} \\cdot \\sum\\limits_{i = 1}^{n(\\mathcal{S})} x_i &amp; \\text{ si } n(\\mathcal{S}) &gt; 0 \\\\ 0 &amp; \\text{ si } n(\\mathcal{S}) = 0 \\end{cases} \\] La pregunta obligada es ¿cuál es un mejor estimador si \\(\\hat{t}\\) o bien \\(\\hat{t}_{\\text{Alt}}\\)? 3.12.3 Un mejor estimador: el proporcional al tamaño Para decidir si \\(\\hat{t}_{\\text{Alt}}\\) es un mejor estimador que \\(\\hat{t}\\) calculemos su valor esperado y su varianza. En ambos casos tenemos dos cosas aleatorias: los elementos que sí quedaron en la muestra (las \\(x_i\\)) y el tamaño de muestra (la \\(n\\)). Para ello utilizamos la propiedad de torre de la esperanza condicional: \\[\\begin{equation}\\nonumber \\begin{aligned} \\mathbb{E}\\big[ \\hat{t}_{\\text{Alt}} \\big] &amp; = \\mathbb{E}\\Big[ \\mathbb{E}\\big[ \\hat{t}_{\\text{Alt}} \\big| n(\\mathcal{S}) = k \\big] \\Big] \\\\ &amp; = \\sum\\limits_{k = 0}^N \\mathbb{E}\\Big[ \\hat{t}_{\\text{Alt}} \\Big| n(\\mathcal{S}) = k \\Big] \\cdot \\mathbb{P}\\big(n(\\mathcal{S}) = k\\big) \\\\ &amp; = \\sum\\limits_{k = 1}^N \\mathbb{E}\\Big[ \\frac{N}{n(\\mathcal{S})} \\cdot \\sum\\limits_{i = 1}^{N} x_i \\mathbb{I}_{\\mathcal{S}}(x_i) \\Big| n(\\mathcal{S}) = k \\Big] \\cdot \\mathbb{P}\\big(n(\\mathcal{S}) = k\\big) \\\\ &amp; = \\sum\\limits_{k = 1}^N \\mathbb{E}\\Big[ \\frac{N}{k} \\cdot \\sum\\limits_{i = 1}^{N} x_i \\mathbb{I}_{\\mathcal{S}}(x_i) \\Big| n(\\mathcal{S}) = k \\Big] \\cdot \\mathbb{P}\\big(n(\\mathcal{S}) = k\\big) \\\\ &amp; = \\sum\\limits_{k = 1}^N \\frac{N}{k} \\mathbb{E}\\Big[ \\sum\\limits_{i = 1}^N x_i\\mathbb{I}_{\\mathcal{S}}(x_i) \\Big| n(\\mathcal{S}) = k \\Big] \\binom{N}{k} \\pi^k (1 - \\pi)^{N - k} \\\\ &amp; = \\sum\\limits_{k = 1}^N \\Bigg( \\frac{N}{k}\\sum\\limits_{i = 1}^N x_i \\mathbb{E}\\Big[\\mathbb{I}_{\\mathcal{S}}(x_i) \\Big| n(\\mathcal{S}) = k \\Big]\\Bigg) \\binom{N}{k} \\pi^k (1 - \\pi)^{N - k} \\\\ &amp; = \\sum\\limits_{k = 1}^N \\Bigg( \\frac{N}{k}\\sum\\limits_{i = 1}^N x_i \\frac{k}{N} \\Bigg) \\binom{N}{k} \\pi^k (1 - \\pi)^{N - k} \\\\ &amp; = \\Bigg( \\sum\\limits_{i = 1}^N x_i \\Bigg) \\cdot \\sum\\limits_{k = 1}^N \\Bigg( \\binom{N}{k} \\pi^k (1 - \\pi)^{N - k}\\Bigg) \\\\ &amp; = t \\cdot \\big( 1 - (1 - \\pi)^N\\big) \\end{aligned} \\end{equation}\\] en este caso el estimador no es insesgado y su sesgo es \\((1 - \\pi)^N\\). Este sesgo es prácticamente ignorable pues para aplicaciones con \\(N\\) grande \\((1 - \\pi)^N \\approx 0\\) y no habrá mucha variación en el resultado. Definición Dado \\(\\hat{\\theta}\\) estimador de \\(\\theta\\) definimos el sesgo de \\(\\hat{\\theta}\\) como: \\[ \\text{Sesgo}(\\hat{\\theta})= \\mathbb{E}\\Big[\\hat{\\theta} - \\theta \\Big] \\] Podemos calcular la varianza de nuestro estimador; para ello denotamos \\[ H(\\pi,N) = \\sum\\limits_{k = 1}^N \\dfrac{1}{k}\\binom{N}{k} \\pi^N (1 - \\pi)^{N - k} - \\dfrac{\\big( 1 - (1 - \\pi)^N\\big)}{N} \\] Luego: \\[\\begin{equation}\\nonumber \\begin{aligned} \\text{Var}\\big[ \\hat{t}_{\\text{Alt}} \\big] &amp; = \\mathbb{E}\\big[ \\hat{t}_{\\text{Alt}}^2 \\big] - \\mathbb{E}\\big[ \\hat{t}_{\\text{Alt}} \\big]^2 \\\\ &amp; = \\mathbb{E}\\big[ \\hat{t}_{\\text{Alt}}^2 \\big] - \\Big( t \\cdot \\big( 1 - (1 - \\pi)^N\\big)\\Big)^2 \\\\ &amp; = \\mathbb{E}\\Big[ \\mathbb{E}\\big[ \\hat{t}_{\\text{Alt}}^2 \\big| n(\\mathcal{S}) = k\\big]\\Big] - \\Big( t \\cdot \\big( 1 - (1 - \\pi)^N\\big)\\Big)^2 \\\\ &amp; = \\sum\\limits_{k = 1}^N \\mathbb{E}\\big[ \\hat{t}_{\\text{Alt}}^2 \\big| n(\\mathcal{S}) = k\\big]\\cdot \\mathbb{P}\\big(n(\\mathcal{S}) = k\\big) - \\Big( t \\cdot \\big( 1 - (1 - \\pi)^N\\big)\\Big)^2 \\\\ &amp; = \\sum\\limits_{k = 1}^N \\dfrac{N^2}{k^2} \\mathbb{E}\\bigg[ \\Big( \\sum\\limits_{i = 1}^N x_i \\mathbb{I}_{\\mathcal{S}}(x_i)\\Big)^2 \\bigg| n(\\mathcal{S}) = k\\bigg]\\cdot \\mathbb{P}\\big(n(\\mathcal{S}) = k\\big) - \\Big( t \\cdot \\big( 1 - (1 - \\pi)^N\\big)\\Big)^2 \\end{aligned} \\end{equation}\\] Notamos que: \\[\\begin{align*} \\mathbb{E}\\bigg[ \\Big( \\sum\\limits_{i = 1}^N x_i &amp; \\mathbb{I}_{\\mathcal{S}}(x_i)\\Big)^2 \\bigg| n(\\mathcal{S}) = k\\bigg] \\\\ &amp; = \\textrm{Var}\\bigg[ \\Big( \\sum\\limits_{i = 1}^N x_i \\mathbb{I}_{\\mathcal{S}}(x_i)\\Big) \\bigg| n(\\mathcal{S}) = k\\bigg] + \\mathbb{E}\\bigg[ \\Big( \\sum\\limits_{i = 1}^N x_i \\mathbb{I}_{\\mathcal{S}}(x_i)\\Big) \\bigg| n(\\mathcal{S}) = k\\bigg]^2 \\\\ &amp; = \\sum\\limits_{i = 1}^N x_i^2 \\textrm{Var}\\Big[ \\mathbb{I}_{\\mathcal{S}}(x_i) \\Big| n(\\mathcal{S}) = k\\Big] + \\sum\\limits_{i = 1}^N\\sum\\limits_{\\substack{j = 1 \\\\ j \\neq i}}^N x_i x_j \\textrm{Cov}\\Big[ \\mathbb{I}_{\\mathcal{S}}(x_i), \\mathbb{I}_{\\mathcal{S}}(x_j) \\Big| n(\\mathcal{S}) = k\\Big] \\\\ &amp; \\qquad + \\bigg( \\sum\\limits_{i = 1}^N x_i \\mathbb{E}\\Big[ \\mathbb{I}_{\\mathcal{S}}(x_i) \\Big| n(\\mathcal{S}) = k\\Big]\\bigg)^2 \\\\ &amp; = \\sum\\limits_{i = 1}^N x_i^2 \\dfrac{k}{N}\\Big( 1 - \\frac{k}{N}\\Big) + \\sum\\limits_{i = 1}^N\\sum\\limits_{\\substack{j = 1 \\\\ j \\neq i}}^N x_i x_j \\Big( \\dfrac{k(k-1)}{N(N-1)} - \\dfrac{k^2}{N^2}\\Big) + \\Bigg( \\dfrac{k}{N} \\sum\\limits_{i = 1}^N x_i \\Bigg)^2 \\\\ &amp; = \\dfrac{k}{N}\\Bigg[ \\sum\\limits_{i = 1}^N x_i^2\\Big( 1 - \\frac{k}{N}\\Big) + \\sum\\limits_{i = 1}^N\\sum\\limits_{\\substack{j = 1 \\\\ j \\neq i}}^N x_i x_j \\bigg( \\dfrac{k-1}{N-1} - \\dfrac{k}{N}\\bigg)\\Bigg] + k^2 \\bar{x}_{\\mathcal{U}}^2 \\\\ &amp; = \\dfrac{k}{N}\\Bigg[ \\sum\\limits_{i = 1}^N x_i^2\\Big(\\frac{N-k}{N}\\Big) - \\sum\\limits_{i = 1}^N\\sum\\limits_{\\substack{j = 1 \\\\ j \\neq i}}^N x_i x_j \\bigg( \\dfrac{N-k}{N(N-1)}\\bigg)\\Bigg] + k^2 \\bar{x}_{\\mathcal{U}}^2 \\\\ &amp; = \\dfrac{k}{N}(N-k) \\Bigg[ \\frac{1}{N}\\sum\\limits_{i = 1}^N x_i^2 - \\frac{1}{N}\\frac{1}{N-1}\\sum\\limits_{i = 1}^N\\sum\\limits_{\\substack{j = 1 \\\\ j \\neq i}}^N x_i x_j \\Bigg] + k^2 \\bar{x}_{\\mathcal{U}}^2 \\\\ &amp; = \\dfrac{k}{N}(N-k) \\frac{1}{N-1}\\sum\\limits_{i = 1}^N \\Bigg[ \\frac{N-1}{N} x_i^2 - \\frac{1}{N} x_i \\sum\\limits_{\\substack{j = 1 \\\\ j \\neq i}}^N x_j \\Bigg] + k^2 \\bar{x}_{\\mathcal{U}}^2 \\\\ &amp; = \\dfrac{k}{N}(N-k) \\frac{1}{N-1}\\sum\\limits_{i = 1}^N \\Bigg[ x_i^2 - \\frac{1}{N} x_i \\sum\\limits_{j = 1}^N x_j \\Bigg] + k^2 \\bar{x}_{\\mathcal{U}}^2 \\\\ &amp; = \\dfrac{k}{N}(N-k) \\frac{1}{N-1} \\Bigg[ \\sum\\limits_{i = 1}^N x_i^2 - \\frac{1}{N}\\bigg( \\sum\\limits_{i = 1}^N x_i \\bigg) \\bigg( \\sum\\limits_{j = 1}^N x_j\\bigg) \\Bigg] + k^2 \\bar{x}_{\\mathcal{U}}^2 \\\\ &amp; = \\dfrac{k}{N}(N-k) \\frac{1}{N-1} \\Bigg[ \\sum\\limits_{i = 1}^N x_i^2 - \\frac{1}{N}\\bigg( \\sum\\limits_{i = 1}^N x_i \\bigg)^2 \\Bigg] + k^2 \\bar{x}_{\\mathcal{U}}^2 \\\\ &amp; = \\dfrac{k}{N} (N-k) \\dfrac{1}{N-1}\\sum\\limits_{i = 1}^N \\Big( x_i - \\dfrac{1}{N}\\sum_{j = 1}^N x_j\\Big)^2 + k^2 \\bar{x}_{\\mathcal{U}}^2 \\\\ &amp; = k \\dfrac{(N- k)}{N} \\dfrac{1}{N-1}\\sum\\limits_{i = 1}^N \\Big( x_i - \\bar{x}_{\\mathcal{U}}\\Big)^2 + k^2 \\bar{x}_{\\mathcal{U}}^2 \\\\ &amp; = k \\dfrac{(N- k)}{N} s^2_{\\mathcal{U}} + k^2 \\bar{x}_{\\mathcal{U}}^2 \\end{align*}\\] por lo cual si sustituimos en la ecuación anterior: %\\[\\begin{equation}\\nonumber \\begin{aligned} \\text{Var}\\big[ \\hat{t}_{\\text{Alt}} \\big] &amp; = \\sum\\limits_{k = 1}^N \\dfrac{N^2}{k^2} \\mathbb{E}\\bigg[ \\Big( \\sum\\limits_{i = 1}^N x_i \\mathbb{I}_{\\mathcal{S}}(x_i)\\Big)^2 \\bigg| n(\\mathcal{S}) = k\\bigg]\\cdot \\mathbb{P}\\big(n(\\mathcal{S}) = k\\big) - \\Big( t \\cdot \\big( 1 - (1 - \\pi)^N\\big)\\Big)^2 \\\\ &amp; = \\sum\\limits_{k = 1}^N \\dfrac{N^2}{k^2} \\Big[ k \\dfrac{(N- k)}{N} s^2_{\\mathcal{U}} + k^2 \\bar{x}_{\\mathcal{U}}^2 \\Big] \\cdot \\binom{N}{k} \\pi^k (1 - \\pi)^{N-k} - \\Big( t \\cdot \\big( 1 - (1 - \\pi)^N\\big)\\Big)^2 \\\\ &amp; = N^2 \\sum\\limits_{k = 1}^N \\Big[ \\dfrac{(N- k)}{k N} s^2_{\\mathcal{U}} + \\bar{x}_{\\mathcal{U}}^2 \\Big] \\cdot \\binom{N}{k} \\pi^k (1 - \\pi)^{N-k} - t^2 \\cdot \\big( 1 - (1 - \\pi)^{2N}\\big) \\\\ &amp; = N^2 s^2_{\\mathcal{U}} \\sum\\limits_{k = 1}^N \\Big( \\frac{1}{k} - \\frac{1}{N} \\Big) \\binom{N}{k} \\pi^k (1 - \\pi)^{N-k} + N^2 \\bar{x}_{\\mathcal{U}}^2 \\sum\\limits_{k = 1}^N \\binom{N}{k} \\pi^k (1 - \\pi)^{N-k} \\\\ &amp; \\qquad - N^2 \\bar{x}_{\\mathcal{U}}^2\\big( 1 - (1 - \\pi)^{2N}\\big) \\\\ &amp; = N^2 s^2_{\\mathcal{U}} \\sum\\limits_{k = 1}^N \\frac{1}{k} \\binom{N}{k} \\pi^k (1 - \\pi)^{N-k} - \\frac{1}{N} \\big( 1 - (1 - p)^N \\big) + \\\\ &amp; \\qquad \\big( 1 - (1 - p)^N \\big) N^2\\bar{x}_{\\mathcal{U}}^2\\Big( 1 - \\big( 1 - (1 - p)^N \\big) \\Big) \\\\ &amp; = N^2[ H(N,\\pi) s^2_{\\mathcal{U}} + (1 - p)^N\\big( 1 - (1 - p)^N \\big) \\bar{x}_{\\mathcal{U}}^2 ] \\end{aligned} \\end{equation}\\] En nuestro caso para elegir el mejor estimador entre \\(\\hat{t}\\) y \\(\\hat{t}_{\\text{alt}}\\) se calculan las varianzas de ambos. Una posible elección es aquél que tiene menos varianza (podría estar más cercano al valor dado que el sesgo de \\(\\hat{t}_{\\text{alt}}\\) es pequeñísimo5). Se puede demostrar (ver Särndal) que en general \\[ \\text{Var}\\big[ \\hat{t}_{\\text{Alt}} \\big] \\ll \\text{Var}\\big[ \\hat{t} \\big] \\] Y usualmente se prefiere el estimador \\(\\hat{t}_{\\text{Alt}}\\). 3.13 Ejemplo Resumen: Aduana Se sabe que de manera diaria fluyen por un punto de la aduana 1000 cargamentos. Cada cargamento que entra debe ser analizado para buscar contrabando con probabilidad \\(p\\) (y con probabilidad \\((1 - p)\\) se deja pasar sin mayor análisis). Determina la probabilidad \\(p\\) si se desea estimar el total de cargamentos con contrabando que pasan por la aduana y, a la vez, se busca que el \\(75\\%\\) de las ocasiones no se analicen más de \\(200\\) cargamentos. Para encontrar la probabilidad \\(p\\) (correspondiente al \\(\\pi\\)) recordamos que el tamaño de la muestra \\(n\\) tiene una distribución Binomial: \\[ n(\\mathcal{S}) \\sim \\text{Binomial}(1000,p) \\] Buscamos entonces una \\(p\\) tal que \\[ \\mathbb{P}\\big( B \\leq 200) = 0.75 \\quad \\text{ donde } B \\sim \\text{Binomial}(1000,p) \\] En particular, notamos que del lado izquierdo tenemos a la función de distribución acumulada \\(F_B(200) = \\mathbb{P}(B\\leq 200)\\) la cual depende (de manera implícita) de \\(p\\). ¡Hagamos explícita la dependencia de los parámetros \\(p\\) y \\(N\\): \\[ F_B(200; 1000, p) = 0.75 \\quad \\text{ donde } B \\sim \\text{Binomial}(1000,p) \\] Podemos graficar la función de distribución acumulada como función de \\(p\\): p.val &lt;- seq(0, 0.5, length.out = 100) ggplot() + geom_line(aes(x = p.val, y = pbinom(200, 1000, p.val))) + theme_bw() + labs( x = &quot;p&quot;, y = &quot;F(200;1000,p)&quot;, title = &quot;Función de distribución acumulada como función de p&quot;, subtitle = &quot;Modelo Binomial(1000,p) evaluado en x = 200&quot; ) Notamos entonces que lo que necesitamos es hallar la \\(p\\) donde la función de distribución acumulada (como función de \\(p\\)) toca al \\(0.75\\). Para ello, como no podemos despejar, utilizamos un método numérico a través de uniroot para encontrar el \\(0\\) de la función \\(g(p) = F_B(200; 1000, p) - 0.75\\) (pues la \\(p^*\\) tal que \\(g(p^*)=0\\) es la respuesta): g.fun &lt;- function(p){pbinom(200, 1000, p) - 0.75} raiz &lt;- uniroot(g.fun, lower = 0, upper = 0.5) print(paste0(&quot;El valor de p es &quot;, raiz$root)) ## [1] &quot;El valor de p es 0.192159774829166&quot; De donde obtenemos el \\(p\\) necesario. 3.14 Muestreo Aleatorio Simple con Reemplazo (MAS/cR) El muestreo aleatorio simple con reemplazo es idéntico al muestreo aleatorio sin reemplazo pero en este caso no se extrae un elemento de la muestra sino que se permite que se seleccione múltiples veces. En cada selección hay una probabilidad \\(1/N\\) de que un individuo de la población sea seleccionado. Cada selección es independiente de la pasada. Aquí consideraremos un universo de tamaño constante \\(N\\in\\mathbb{R}\\) dado por \\(U =(x_1, x_2, \\dots, x_N)^T\\) y las variables aleatorias \\(N_k\\) que denotan la cantidad de veces que \\(x_k\\) fue seleccionado para incluirse en la muestra6. El orden en el que fueron seleccionados los elementos no importa. En el caso de muestreo aleatorio simple con reemplazo se fija un tamaño de muestra \\(m\\) y hay por tanto \\(N^m\\) muestras posibles. Cada una de las muestras sigue la siguiente función de probabilidad uniforme: \\[ \\mathbb{P}(\\mathcal{S} = S) = \\begin{cases} \\frac{1}{N^m} &amp; \\text{ si } \\#S = m\\\\ 0 &amp; \\text{ en otro caso} \\end{cases} \\] Dado un elemento \\(x_k\\) la probabilidad de que dicho \\(x_k\\) aparezca \\(r\\) veces en la muestra de tamaño \\(m\\) está dada por: \\[ \\binom{m}{r}\\Big(\\frac{1}{N}\\Big)^r \\Big( 1 - \\frac{1}{N}\\Big)^{m - r} \\] En particular se tiene que la probabilidad de que \\(x_k\\) no esté en la muestra es: \\[ \\Big( 1 - \\frac{1}{N}\\Big)^m \\] o bien de que esté en la muestra: \\[ \\pi_k = 1 - \\Big( 1 - \\frac{1}{N}\\Big)^m \\] lo cual se calcula por el complemento. Por otro lado, la probabilidad conjunta \\(\\pi_{k,l}\\) de que \\(x_k\\) y \\(x_l\\) estén en la muestra se puede computar usando inclusión exclusión: \\[ \\pi_{k,l} = 1 - \\underbrace{\\Big( 1 - \\frac{1}{N}\\Big)^m}_{\\text{No está }x_k} - \\underbrace{\\Big( 1 - \\frac{1}{N}\\Big)^m}_{\\text{No está }x_l} + \\underbrace{\\Big( 1 - \\frac{2}{N}\\Big)^m}_{\\text{No está ni }x_k\\text{ ni }x_l} \\] En R puedes obtener un muestreo aleatorio simple con reemplazo cambiando en sample el replace: sample(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;), 10, replace = TRUE) ## [1] &quot;B&quot; &quot;A&quot; &quot;C&quot; &quot;A&quot; &quot;C&quot; &quot;B&quot; &quot;B&quot; &quot;A&quot; &quot;A&quot; &quot;C&quot; Una observación es bastante relevante aquí: \\[\\begin{align*} \\pi_k &amp; = 1 - (1 - \\frac{1}{N})^m = 1 - \\sum\\limits_{j = 0}^m\\binom{m}{j}\\big( - \\frac{1}{N})^{m-j} \\\\ &amp; = 1 - \\bigg[ \\sum\\limits_{j = 0}^{m-2}\\binom{m}{j}\\Big(-\\frac{1}{N}\\Big)^{m-j} - \\frac{m}{N} + 1\\bigg] \\\\ &amp; = \\frac{m}{N} - \\sum\\limits_{j = 0}^{m-2}\\binom{m}{j}\\Big(-\\frac{1}{N}\\Big)^{m-j} \\\\ &amp; = \\frac{m}{N} + \\mathcal{O}\\Bigg( \\frac{m^2}{N^2}\\Bigg) \\end{align*}\\] donde \\(\\mathcal{O}\\Bigg( \\frac{m^2}{N^2}\\Bigg)\\) es notación que implica que una función \\(f(n)\\) es de orden \\(g(n)\\) (denotado \\(f(n) = \\mathcal{O}\\big(g(n)\\big)\\)) si y sólo si existe \\(M\\) tal que para cualquier \\(n \\in\\mathbb{N}\\)tal que \\(|f(n)|/g(n)\\leq M\\). Escrito con palabras en este caso esto significa que si \\(m/N\\) es pequeño entonces \\(\\frac{m^2}{N^2}\\) es caso \\(0\\) y entonces muestrear con reemplazo es casi lo mismo que muestrear sin reemplazo (lo cual tiene sentido: si tu población es muy grande \\(N \\gg 0\\) entonces está bien difícil que vuelvas a capturar a uno en tu encuesta y por tanto es casi lo mismo muestrear con que sin reemplazo en términos prácticos). Para el análisis del muestreo aleatorio simple con reemplazo podemos generalizar la idea de variables indicadoras. Como en este tipo de muestreo pueden aparecer varias veces los mismos valores \\(x_i\\) utilizaremos unas variables \\(\\mathbb{A}_{\\mathcal{S}}(x_i)\\) para denotar cuántas veces aparece el valor \\(x_i\\) en la muestra aleatoria \\(\\mathcal{S}\\); es decir: \\[ \\mathbb{A}_{\\mathcal{S}}(x_i) = \\begin{cases} 0 &amp; \\text{ si } x_i \\not\\in \\mathcal{S} \\\\ k &amp; \\text{ si } x_i \\in \\mathcal{S} \\quad k \\textrm{ veces} \\end{cases} \\] Observamos que la distribución de las \\(a_i\\) es multinomial: \\[\\begin{equation} \\mathbb{P}\\Big(\\mathbb{A}_{\\mathcal{S}}(x_1) = a_1,\\mathbb{A}_{\\mathcal{S}}(x_2) = a_2, \\dots,\\mathbb{A}_{\\mathcal{S}}(x_N) = a_N\\Big) &amp; = \\\\ &amp; = \\binom{m}{a_1}\\Big(\\frac{1}{N}\\Big)^{a_1} \\cdot \\binom{m - a_1}{a_2}\\Big(\\frac{1}{N}\\Big)^{a_2}\\cdot\\binom{m - a_1 - a_2}{a_3}\\Big(\\frac{1}{N}\\Big)^{a_3} \\cdots \\binom{m - \\sum_{l = 1}^{N-1} a_l}{a_N}\\Big(\\frac{1}{N}\\Big)^{a_N} \\\\ &amp; = \\dfrac{m!}{a_1!a_2!\\cdots a_N!}\\dfrac{1}{(m - \\sum_{l= 1}^N a_l)!} \\Big(\\frac{1}{N}\\Big)^{\\sum_{l= 1}^N a_l} \\\\ &amp; = \\dfrac{m!}{a_1!a_2!\\cdots a_N!} \\Big(\\frac{1}{N}\\Big)^{m} \\\\ \\end{equation}\\] donde tomamos que \\(\\sum\\limits_{l = 1}^N a_l = m\\). Al ser una distribución multinomial se tienen las marginales: \\[ \\mathbb{E}\\Big[\\mathbb{A}_{\\mathcal{S}}(x_i) \\Big] = \\dfrac{m}{N} \\] y por otro lado: \\[ \\textrm{Var}\\Big[\\mathbb{A}_{\\mathcal{S}}(x_i) \\Big] = \\dfrac{m(N-1)}{N^2} \\] con: \\[ \\textrm{Cov}\\Big[\\mathbb{A}_{\\mathcal{S}}(x_i) ,\\mathbb{A}_{\\mathcal{S}}(x_j) \\Big] = -\\dfrac{m}{N^2} \\] Un estimador de la media es la suma de los \\(N\\) valores únicos en el universo (i.e. \\(N = \\# \\mathcal{U}\\)) multiplicados por la cantidad de veces que aparecen en la muestra (las \\(\\mathbb{A}_{\\mathcal{S}}(x_i)\\)): \\[ \\bar{x}_{\\mathcal{S}} = \\frac{1}{m} \\sum\\limits_{i = 1}^m x_i = \\frac{1}{m} \\sum\\limits_{i = 1}^N x_i\\cdot \\mathbb{A}_{\\mathcal{S}}(x_i) \\] en este caso se tiene que el estimador es insesgado: \\[ \\mathbb{E}\\Big[\\bar{x}_{\\mathcal{S}}\\Big] = \\frac{1}{m} \\sum\\limits_{i = 1}^N x_i\\cdot \\mathbb{E}\\Big[ \\mathbb{A}_{\\mathcal{S}}(x_i)\\Big] = \\frac{1}{m}\\sum\\limits_{i = 1}^N x_i \\dfrac{m}{N}= \\bar{x}_{\\mathcal{U}} \\] Su varianza está dada por lo siguiente: \\[\\begin{align*} \\textrm{Var}\\Big[\\bar{x}_{\\mathcal{S}}\\Big] &amp; = \\frac{1}{m^2}\\bigg( \\sum\\limits_{i = 1}^N x_i^2\\text{Var}\\Big[ \\mathbb{A}_{\\mathcal{S}}(x_i)\\Big] + \\sum\\limits_{i= 1}^N \\sum\\limits_{\\substack{j = 1 \\\\ j \\neq i}}^N \\textrm{Cov}\\Big[\\mathbb{A}_{\\mathcal{S}}(x_i) ,\\mathbb{A}_{\\mathcal{S}}(x_j) \\Big] x_i x_k\\bigg) \\\\ &amp; = \\frac{1}{m^2}\\bigg( \\frac{m(N-1)}{N^2} \\sum\\limits_{i = 1}^N x_i^2 - \\frac{m}{N^2} \\sum\\limits_{i= 1}^N \\sum\\limits_{\\substack{j = 1 \\\\ j \\neq i}}^N x_i x_k\\bigg) \\\\ &amp; = \\frac{N-1}{mN}\\bigg( \\frac{1}{N} \\sum\\limits_{i = 1}^N x_i^2 - \\frac{1}{N} \\sum\\limits_{i= 1}^N \\sum\\limits_{\\substack{j = 1 \\\\ j \\neq i}}^N x_i x_k\\bigg) \\\\ &amp; = \\frac{N-1}{mN}\\Bigg( \\frac{1}{N-1} \\sum\\limits_{i = 1}^N \\bigg[ \\frac{N-1}{N} x_i^2 - \\frac{1}{N} x_i \\sum\\limits_{\\substack{j = 1 \\\\ j \\neq i}}^N x_k\\bigg]\\Bigg) \\\\ &amp; = \\frac{N-1}{mN}\\Bigg( \\frac{1}{N-1} \\sum\\limits_{i = 1}^N \\bigg[ x_i^2 - \\frac{1}{N} x_i \\sum\\limits_{j = 1}^N x_k\\bigg]\\Bigg) \\\\ &amp; = \\frac{N-1}{mN}\\Bigg( \\frac{1}{N-1} \\bigg[ \\sum\\limits_{i = 1}^N x_i^2 - \\frac{1}{N}\\sum\\limits_{i = 1}^N x_i \\sum\\limits_{j = 1}^N x_k\\bigg]\\Bigg) \\\\ &amp; = \\frac{N-1}{mN}\\Bigg( \\frac{1}{N-1} \\bigg[ \\sum\\limits_{i = 1}^N x_i^2 - \\Big(\\sum\\limits_{i = 1}^N x_i\\Big)^2\\bigg]\\Bigg) \\\\ &amp; = \\frac{N-1}{mN} s^2_{\\mathcal{U}} \\end{align*}\\] Donde la última igualdad se sigue de la misma que se hizo con Bernoulli. Un estimador de la varianza es: \\[ \\widehat{\\textrm{Var}}\\Big[\\bar{x}_{\\mathcal{S}}\\Big] = \\frac{N-1}{mN} s^2_{\\mathcal{S}} \\] 3.15 Ejemplo Resumen: Proporción de trabajadores enfermos con o sin reemplazo Nos interesa estimar la proporción de trabajadores \\(P\\) afectados por una enfermedad en su trabajo en un negocio que emplea a 1500 personas. Además sabemos que en población general \\(3\\) de cada \\(10\\) personas enferman. Para ello obtenemos una muestra aleatoria con reemplazo donde además buscamos un intervalo de confianza al \\(0.95\\) con un error a lo más de \\(0.01\\). Proponemos el estimador de la proporción dado por: \\[ \\bar{x}_m = \\frac{1}{m}\\sum\\limits_{i = 1}^m x_i \\] donde \\(x_i = 1\\) si el trabajador tiene la enfermedad (\\(0\\) en otro caso). En este caso tenemos que la varianza está dada por: \\[ \\widehat{\\text{Var}}(\\bar{x}_m) =\\frac{N-1}{N\\cdot m} s^2_{\\mathcal{U}} \\] Tenemos entonces que el error es: \\[ 0.01 \\geq \\epsilon = Z_{1 - \\alpha/2} \\sqrt{\\widehat{\\text{Var}}(\\bar{x}_m)} \\] de donde se tiene: \\[ \\dfrac{0.01^2}{Z_{1 - \\alpha/2}^2} \\geq \\frac{N-1}{N\\cdot m} s^2_{\\mathcal{U}} \\] o de manera equivalente: \\[ \\dfrac{0.01^2}{Z_{1 - \\alpha/2}^2\\cdot s^2_{\\mathcal{U}}}\\dfrac{N}{N-1} \\geq \\frac{1}{m} \\] de donde se sigue que: \\[ \\Big(\\dfrac{Z_{1 - \\alpha/2}s_{\\mathcal{U}}}{0.01}\\Big)^2\\cdot \\dfrac{N-1}{N} \\leq m \\] donde finalmente como sabemos que \\(3\\) de cada \\(10\\) personas lo padecen notamos que la probabilidad en el mundo real de obtener a una persona que lo tenga es \\(p = \\frac{1}{3}\\) y podemos modelar la variable estar enfermo mediante una Bernoulli y por tanto una buena aproximación a \\(s^2_{\\mathcal{U}}\\) es: \\[ s^2_{\\mathcal{U}} \\approx \\underbrace{\\frac{1}{3}\\Big( 1 - \\frac{1}{3}\\Big)}_{p\\cdot(1-p)} \\] donde sustituimos: \\[ \\frac{1}{3}(1 - \\frac{1}{3})\\cdot \\Bigg(\\dfrac{Z_{1 - \\alpha/2}}{0.01}\\Bigg)^2\\cdot \\dfrac{N-1}{N} \\leq m \\] Concluimos que: m &lt;- ceiling(1/3*(1 - 1/3)*(qnorm(0.975)/0.01)^2*(1500 - 1)/1500) print(paste0(&quot;m = &quot;, m)) ## [1] &quot;m = 8531&quot; Lo cual es un sinsentido estadístico: ¿para qué muestrear más de lo que se tiene en población total? En este ejercicio lo mejor sería hacer un censo. Nota Si se repite el ejercicio con muestreo aleatorio simple sin reemplazo acabamos con \\(n \\approx 1300\\) lo cual sí tiene sentido como muestreo. En general la \\(m\\) de muestreo con reemplazo es mayor que la \\(n\\) de sin reemplazo (las varianzas son mayores). Intuitivamente esto tiene sentido pues si estás muestrando con la posibilidad de repetidos a fuerza necesitas muestrear más para obtener la misma cantidad de datos únicos. 3.16 Ejemplo Resumen: Captura-Recaptura con reemplazo Se realiza un estudio para determinar la cantidad de ratas en la CDMX. Para ello se pone una trampa en algún lugar aleatorio de la ciudad. Si se atrapa una rata se le marca y se le deja ir. Si para \\(50\\) ratas capturadas, contamos \\(42\\) marcadas determina el número de ratas en la isla suponiendo que las \\(50\\) fueron con reemplazo. Para ello denotamos \\(p_N(r)\\) a la probabilidad de tener \\(r\\) ratas distintas en \\(m\\) intentos con reemplazos (\\(m = 50\\) es determinado por nosotros y no es aleatorio) en una población de tamaño desconocido \\(N\\). Una vez que se fijan las \\(r\\) ratas que van a salir hay \\(\\binom{N}{r}\\) formas de elegirlas. Entonces: \\[ p_N(r) = \\dfrac{N!}{r!(N-r)!} q_N(r) \\] donde \\(q_N(r)\\) es la probabilidad de obtener \\(r\\) distintas ratas en \\(m\\) intentos con reemplazo. Fijado el número de ratas, el universo \\(\\Omega\\) de posibilidades se forma por el grupo de mapeos de \\(\\{1,2,\\dots, m\\}\\to\\{1,2,\\dots, N\\}\\) (todas las formas de haber acomodado las ratas). Tenemos \\(m \\geq r\\) y de hecho: \\[ q_N(r) = \\sum_{\\omega\\in\\text{Fav}} p(\\omega) \\] dpmde \\(p(\\omega)\\) es la probabilidad de obtener un mapeo favorable (\\(w\\in\\text{Fav}\\)). Tenemos que \\(p(\\omega) = N^{-m}\\) para toda \\(\\omega\\). La cantidad de casos favrables es lo mismo que preguntarse por la cantidad de mapeos suprayectivos del conjunto \\(\\{1,2,\\dots, m\\}\\) en \\(\\{1,2,\\dots, r\\}\\) lo cual está dado por \\(r!\\) multiplicado por el número de Stirling de segundo tipo \\(\\mathfrak{s}_m^{(r)}\\): \\[ \\mathfrak{s}_m^{(r)} = \\frac{1}{r!} \\sum\\limits_{i = 1}^r \\binom{r}{i} i^m (-1)^{r-i} \\] donde \\(\\mathfrak{s}_m^{(r)}\\) es la forma de encontrar de un grupo de \\(m\\) elementos \\(r\\) partes no vacías. Tenemos entonces: \\[ p_N(r) = \\dfrac{N!}{(N-r)!N^m} \\mathfrak{s}_m^{(r)} \\qquad \\text{para} r = 1,2,\\dots,\\min\\{m,N\\}. \\] Lo que vamos a hacer es pensar que la \\(N\\) que generó los datos es la \\(N\\) máxima (criterio de máxima verosimilitud) y entonces lo que hay que maximizar es: \\[ \\dfrac{N!}{(N-r)!N^m} = \\dfrac{\\prod_{i = 0}^{r -1} (N-i)}{N^m} \\] Hay dos formas de hacer esta maximización: enlistando todas las \\(N\\) y \\(r\\) para mi población o bien derivando el logaritmo: \\[ \\dfrac{d}{dN} \\ln \\Bigg( \\dfrac{\\prod_{i = 0}^{r -1} (N-i)}{N^m}\\Bigg) = \\dfrac{d}{dN}\\Bigg[ \\sum\\limits_{i = 0}^{r-1} \\ln(N-i) - m\\ln (N) \\Bigg] = \\sum\\limits_{i = 0}^{r-1} \\dfrac{1}{N-i} - \\dfrac{m}{N} = 0 \\] de donde se sigue que: \\[ \\sum\\limits_{i = 0}^{r-1} \\dfrac{N}{N-i} = m \\] La cual es una ecuación no lineal que se puede resolver mediante uniroot como la pasada. m.val &lt;- function(N){ r &lt;- 42 m &lt;- 50 suma &lt;- 0 N &lt;- floor(N) #Esto es para garantizar sale un entero; no es la mejor opción de optimizar for (i in 1:r){ suma &lt;- suma + N/(N - (i - 1)) } return(suma - m) } floor(uniroot(m.val, lower = 100, upper = 200)$root) ## [1] 136 3.17 Muestreo Aleatorio Simple Ponderado (MAS/P) En el caso más general posible cada uno de los elementos \\(x_k\\) tiene una probabilidad \\(\\pi_k\\) de aparecer en la muestra. Análogamente se tienen probabilidades conjuntas de la forma \\(\\pi_{k,l}\\) donde no necesariamente hay independencia. El estimador del total está dado por el estimador Horvitz Thompson: \\[ \\hat{t} = \\sum\\limits_{k = 1}^n \\dfrac{x_k}{\\pi_k} \\] donde su varianza y sus estimadores fueron ya calculados desde muestreo aleatorio simple. En el siguiente capítulo comenzaremos a variar mucho más las \\(\\pi_k\\) cuando entremos a muestreo estratificado. ¡Nos vemos pronto! 3.18 Ejercicios Bajo muestreo aleatorio Bernoulli se propone el siguiente estimador para el total: \\[ \\hat{t}_{\\text{BE}} = \\dfrac{N}{\\mathbb{E}[n(\\mathcal{S})]} \\sum\\limits_{i = 1}^{n(\\mathcal{S})} x_i \\] Demuestra que es insesgado Obtén su varianza Para el ejemplo del profesor con los exámenes ¿es \\(\\hat{t}_{\\text{BE}}\\) una mejor opción que \\(\\hat{t}_{\\text{Alt}}\\) ? Justifica calculando en R todos los posibles casos (desde que \\(0\\) pasan hasta que los \\(600\\) pasan el examen) y analizando cuántas de esas veces la varianza de \\(\\hat{t}_{\\text{Alt}}\\) es menor que la de \\(\\hat{t}_{\\text{BE}}\\). Se tiene una muestra aleatoria simple con reemplazo y se calcula la varianza del estimador de la media como sigue: \\[ \\text{Var}(\\bar{x}_{\\mathcal{S}}) = \\frac{1}{n}\\sum\\limits_{i = 1}^N x_i^2 \\cdot \\text{Var}(\\mathbb{I}_{\\mathcal{S}}(x_i)) = \\frac{1}{n}\\sum\\limits_{i = 1}^N x_i^2 \\cdot \\frac{1}{N}\\Big(1 - \\frac{1}{N}\\Big) = \\frac{1}{N\\cdot n}\\sum\\limits_{i = 1}^N x_i^2 \\cdot \\underbrace{\\Big(1 - \\frac{1}{N}\\Big)}_{(N-1)/N} = \\frac{N-1}{N\\cdot n} s^2_{\\mathcal{U}} \\] El resultado está bien pero el razonamiento tiene un error. Identifica los errores. Sea \\(\\mathcal{S}\\) una muestra bajo diseño Bernoulli con parámetro \\(\\pi\\). Sea \\(n(\\mathcal{S}) = \\#\\mathcal{S}\\) la variable aleatoria del tamaño de \\(\\mathcal{S}\\). Demuestra que condicional en que \\(n(\\mathcal{S}) = n\\) la probabilidad de que \\(\\mathcal{S} = S\\) es la misma que bajo muestreo aleatorio simple. Encuentra un estimador insesgado de la varianza poblacional bajo muestreo aleatorio simple sin reemplazo. Hint Demuestra que: \\[ \\dfrac{1}{N} \\sum\\limits_{k = 1}^N (x_k - \\bar{x}_{\\mathcal{U}})^2 =\\dfrac{1}{2N^2} \\sum\\limits_{k = 1}^N\\sum\\limits_{\\substack{l = 1 \\\\ l \\neq k}}^N (x_k - \\bar{x}_{l})^2 \\] Considera la población de la tabla : Se sabe que en una muestra aleatoria sin reemplazo que se obtenga de dicha población es dos veces más factible seleccionar a alguien de menos de \\(20\\) años que a alguien de más de \\(20\\) años. Determina un estimador insesgado de la media de peso para dicha población. Se obtiene la muestra aleatoria simple de peso dada por \\(S = \\{50, 35, 85 \\}\\). Determina la probabilidad de haber obtenido dicha muestra. A partir de la muestra \\(S\\) del inciso anterior, estima el peso total poblacional (la suma de los pesos) y da un intervalo asintótico de confianza de 75% para dicho total. De una población de \\(N = 1000\\) personas se entrevistaron a \\(n = 100\\). Se les preguntó el tipo de música que preferían; \\(30\\) personas respondieron reguetón. Estima el total de personas de la población que prefieren . Genera un intervalo de confianza asintótico de 50% para dicha estimación. Un estimador \\(\\hat{\\theta}\\) de \\(\\theta\\) es Fisher consistente si cuando \\(n = N\\) el estimador \\(\\hat{\\theta} = \\theta\\). Demuestra que \\(\\bar{x}_{\\mathcal{S}}\\) es Fisher consistente. Da un ejemplo de un estimador que no sea Fisher consistente. ¿Cuál de los siguientes diseñois de muestreo simple sin reemplazo tiene la mayor precisión para estimar la media poblacional? Suponiendo que todas las poblaciones tienen la misma varianza \\(\\sigma^2\\): Tomar \\(n = 400\\) de \\(N = 4000\\) Tomar \\(n = 300\\) de \\(N = 4000\\) Tomar \\(n = 3000\\) de \\(N = 300,000,000\\) En una ciudad hay \\(10\\) millones de habitantes. Interesa saber el porcentaje de personas que utilizan bicicleta en dicha ciudad con un margen de error de \\(4\\) puntos porcentuales y una confianza del \\(75\\%\\). Determina el tamaño de muestra mínimo para hacer una encuesta de usuarios de bicicleta. Bajo muestreo Bernoulli, define el total de la población como \\(t = \\sum_{i=1}^{N} x_i\\). Un estimador del total es: \\[ \\hat{t}_1 = \\dfrac{1}{p} \\sum_{k=1}^{N} x_k \\mathbb{I}_{\\mathcal{S}}(x_k) \\] Obtén \\(\\mathbb{E}[\\hat{t}_1]\\) Demuestra que \\(\\textrm{Var}[\\hat{t}_1] = \\big(\\frac{1}{p}-1 \\big) \\sum_{k=1}^{N} x_k^2\\). Obtén un estimador de \\(\\textrm{Var}[\\hat{t}_1]\\). ¿Es insesgado? Un estimador distinto del total de la población (bajo Bernoulli) está dado por: \\[ \\hat{t}_2 = N \\bar{x} \\] donde \\(\\bar{x} = \\frac{1}{\\# S} \\sum\\limits_{k=1}^{N} x_k \\mathbb{I}_k\\). Demuestra que \\(\\mathbb{E}[\\hat{t}_2] = \\big(1 - (1-p)^N\\big) \\cdot t\\) Hint Utiliza la propiedad de torre de proba 2: \\(\\mathbb{E}[X] = \\mathbb{E}\\big[\\mathbb{E}[X| Y ]\\big]\\) condicionando en el tamaño de la muestra. Demuestra que \\(\\textrm{Var}[\\hat{t}_2] = \\frac{N^3}{N-1} \\Big[ H \\sigma^2 + (1-p)^N \\Big( 1 - (1 - p)^N \\Big) \\bar{x}^2 \\Big]\\) donde \\(H = \\sum_{k=1}^{N} \\frac{1}{k} \\binom{N}{k} p^k (1-p)^{N-k} - \\big(1 - (1-p)^N\\big)/N\\). En muestreo aleatorio simple con reemplazo sea \\(n_{\\text{únicos}}\\) la cardinalidad del conjunto \\(\\{ x_k | x_k \\in \\mathcal{S}\\}\\); es decir, el número de valores únicos que se obtuvieron en dicho muestreo. Determina \\(\\mathbb{E}\\big[n_{\\text{únicos}}\\big]\\) En un esquema de muestreo Poisson (PO) se tiene una población de tamaño \\(N\\in\\mathbb{N}\\) (constante) la cual se enlista de manera ordenada \\(U = \\{x_1,x_2,\\dots,x_N\\}\\). Se recorre la lista de \\(1\\) hasta \\(N\\). Cada elemento de la población, se selecciona y se mide con probabilidad \\(\\pi_k \\in (0,1)\\) para \\(k = 1,2,\\dots, N\\) a fin de generar una muestra \\(S = \\{x_1, x_2, \\dots, x_n\\}\\) de tamaño \\(\\# S\\) (con \\(0 \\leq \\# S \\leq N\\)) donde en este caso la cardinalidad de \\(S\\), \\(\\# S\\), es una variable aleatoria. En este caso se asume que \\(\\mathbb{I}_k\\) es independiente de \\(\\mathbb{I}_j\\) para \\(k\\neq j\\). Responde los siguientes incisos considerando que la muestra \\(S\\) fue generada por un esquema Poisson. Demuestra que \\(\\mathbb{E}[\\mathbb{I}_k] = \\pi_k\\). ¿Cuánto vale es \\(\\textrm{Cov}(\\mathbb{I}_k, \\mathbb{I}_j)\\)? Demuestra \\(\\mathbb{E}[\\# S ] = \\sum_{k=1}^N \\pi_k\\) y \\(\\textrm{Var}[\\# S ] = \\sum_{k=1}^N \\pi_k (1 - \\pi_k)\\). Define la media de la población como \\(\\mu_X = \\frac{1}{N} \\sum_{i=1}^{N} x_i\\). Se propone un estimador de la media como: \\[ \\hat{\\mu} = \\frac{1}{n} \\sum_{k=1}^{n} \\dfrac{x_k}{\\pi_k} \\] Determina el sesgo de \\(\\hat{\\mu}\\) ¿Es \\(\\hat{\\mu}\\) Fisher-consistente? Obtén el error cuadrático medio de \\(\\hat{\\mu}\\) Demuestra que \\(\\widehat{\\textrm{Var}}[\\hat{\\mu}] = \\frac{1}{N^2} \\sum_{i \\in S} (1 - \\pi_i) \\pi_i^{-2} x_i^2\\) es estimador insesgado de la varianza \\(\\textrm{Var}[\\hat{\\mu}]\\). Una aplicación de el muestreo Poisson es en estimación de cantidad de madera en un bosque. Los investigadores van al bosque y estiman a ojo de buen cubero el tamaño de un árbol en una de las categorías: , , . Una vez que a ojo se estimaron los tamaños se seleccionan los árboles pequeños con probabilidad \\(p_1\\) para cada árbol, los medianos con probabilidad \\(p_2\\) y los grandes con probabilidad \\(p_3\\) (\\(0 &lt; p_1 &lt; p_2 &lt; p_3 &lt; 1\\)). La selección de cada árbol es independiente de que otro haya sido seleccionado. Se busca estimar el total de madera midiendo los árboles (área de la base por altura). Supongamos que un bosque tiene \\(N\\) árboles de los cuales \\(N_1\\) son pequeños, \\(N_2\\) son medianos y \\(N_3\\) son grandes (\\(\\sum N_i = N\\)). Supongamos además que medir un árbol pequeño cuesta \\(C_1\\), un árbol mediano cuesta \\(C_2\\) y un árbol grande cuesta \\(C_3\\) (\\(0 &lt; C_1 &lt; C_2 &lt; C_3\\)). Específicamente, supongamos que \\(p_i = C_i / \\sum_k C_k\\). En promedio para \\(N_1 = N_2 = 300\\) y \\(N_3 = 400\\) y \\(C_1 = 10\\), \\(C_2 = 20\\) y \\(C_3 = 70\\) ¿es más barato un muestreo Poisson (parámetros \\(p_i\\)) o un muestreo aleatorio simple sin reemplazo de los árboles del bosque de tamaño \\(n = \\lceil \\sum_k n_k \\pi_k \\rceil\\)? Supongamos que se tienen dos muestras aleatorias simples sin reemplazo \\(\\mathcal{S}\\) para \\(\\mathcal{U}\\) y \\(\\mathcal{S}_{-}\\) para \\(\\mathcal{U}\\setminus\\mathcal{S}\\) (de tamaños \\(n\\) y \\(n_{-}\\). Encuentra la covarianza entre \\(\\bar{x}_{\\mathcal{S}}\\) y \\(\\bar{x}_{\\mathcal{S}_{-}}\\). Queremos estimar el área cultivada por granjas en una localidad. De \\(N = 2010\\) granjas seleccionamos \\(100\\) mediante muestreo aleatorio simple sin reemplazo. Medimos \\(x_k\\) el área cultivada en hectáreas y obtenemos que: \\[ \\sum_{i = 1}^n x_k = 2907 \\qquad \\text{y} \\qquad \\sum_{i = 1}^n x_k^2= 154593 \\] Estima el promedio de hectáreas cultivadas y da un intervalo al \\(90\\%\\). Determina el tamaño de muestra bajo muestreo aleatorio simple para hallar con una precisión de al menos dos puntos porcentuales y un intervalo al \\(95\\%\\) la proporción de personas que usan lentes en una población de tamaño \\(N\\) De una población de 4000 individuos nos interesan dos proporciones: \\(P_1\\) la proporción de individuos con lavadora y \\(P_2\\) la proporción de individuos con laptop. Se sabe además de un estudio previo que que: \\[ 45\\% \\leq P_1 \\leq 65\\% \\quad \\text{y} \\quad 5\\% \\leq P_2 \\leq 10\\% \\] De qué tamaño tiene que ser la muestra si queremos conocer a la vez \\(P_1\\) con una precisión \\(\\pm 2\\%\\) y \\(P_2\\) con un error de \\(1\\%\\) y una confianza de \\(95\\%\\) Nos interesa conocer el precio por litro en una población de \\(10\\) gasolineras. Los precios para mayo y junio de las mismas aparecen en la tabla 2. En particular, queremos estimar la evolución del precio por litro entre dichos meses. Para ello se proponen dos métodos Muestrear \\(n\\) estaciones en mayo y \\(n\\) en junio de manera independiente y calcular la diferencia en precios. Muestrear \\(n\\) estaciones en mayo e ir a verlas en junio de nuevo (a las mismas) y calcular la diferencia en precios. Determina cuál de los métodos es mejor (en términos de varianza) En una población de tamaño \\(N\\) consideramos \\(n\\) individuos mediante muestreo aleatorio simple sin reemplazo. Consideremos \\(D\\) una subpoblación de tamaño \\(N_D\\) de donde \\(n_D\\) es el tamaño de los muestreados que son elementos de \\(D\\). La muestra la podemos dividir en dos partes: \\(\\mathcal{S} = (\\mathcal{S}_{D}, \\mathcal{S}_{\\bar{D}})^T\\) según se esté o no en \\(D\\). Encuentra la distribución de \\(\\mathcal{S}_{D}\\) condicional en \\(n_D\\): Dada una población \\(\\mathcal{U} = \\{1,2,3\\}\\) se tiene el diseño: \\[ p(\\{1,2\\}) = 1/2, \\quad p(\\{1,3\\}) = 1/4, \\quad p(\\{2,3\\}) = 1/4 \\] determina los \\(\\pi_k\\) los \\(\\pi_{k,l}\\) y los \\(\\Delta_{k,l}\\) En la literatura muchas referencias establecen una muestra aleatoria como un conjunto de valores. Yo utilizo vectores para poder hablar de repeticiones (por ejemplo si extraes el mismo valor varias veces en la muestra).↩︎ Enumero las submatrices para luego poder hablar de ellas↩︎ Una mejor distribución sería una \\(t\\) de Student; empero eso lo verás en Estadística Matemática.↩︎ Calcúlalo.↩︎ Observa que las variables aleatorias \\(N_k\\) generalizan a las variables indicadoras.↩︎ "],
["muestreo-aleatorio-estratificado.html", "Capítulo 4 Muestreo Aleatorio Estratificado 4.1 Introducción a Muestreo Aleatorio Estratificado (MAE) 4.2 Alocación 4.3 Ejercicio de clase: 4.4 Ejercicio en R tipo control", " Capítulo 4 Muestreo Aleatorio Estratificado 4.1 Introducción a Muestreo Aleatorio Estratificado (MAE) Vamos a considerar una población \\(\\mathcal{U}\\) la cual suponemos podemos particionar en una cantidad finita (no vacía) de subpoblaciones: \\(\\mathcal{U}_1, \\mathcal{U}_2, \\dots, \\mathcal{U}_H\\) de tamaños \\(N_1, N_2, \\dots, N_H\\) con \\(\\bigcup\\limits_{h=1}^H \\mathcal{U}_h = \\mathcal{U}\\) (lo que se traduce en \\(\\sum_{h = 1}^H N_h = N\\)). Lo que busca el muestreo aleatorio estratificado es estimar en cada uno de los estratos así como de manera global. Por ejemplo, puede interesarnos conocer la estatura en hombres y mujeres, las ganancias en empresas agrícolas, ganaderas, de servicios y de transformación, la cantidad de enfermos que hay en cada entidad federativa, etc. En cada uno de estos casos estamos hablando de estratos de en los cuales interesa realizar la estimación. Un punto importante aquí es que los estratos son conocidos y decididos por la investigadora. La extracción en cada uno de los estratos se realiza de manera independiente obteniéndose muestras \\(\\mathcal{S}_1, \\mathcal{S}_2, \\dots, \\mathcal{S}_H\\) de tamaños \\(n_1, n_2, \\dots, n_H\\) para cada uno de ellos. Notación Si \\(x_i \\in \\mathcal{U}_h\\) lo denotaremos como \\(x_{i,h}\\) para de esta manera distinguir el \\(x_i\\) que está en \\(\\mathcal{U}_h\\) del que está en \\(\\mathcal{U}_k\\) La muestra total es: \\[ \\mathcal{S} = (\\mathcal{S}_1, \\mathcal{S}_2, \\dots, \\mathcal{S}_H)^T \\] un vector de tamaño \\(n = \\sum_{h = 1}^H n_h\\). Por independencia, se tiene: \\[ \\mathbb{P}(\\mathcal{S} = S) = \\mathbb{P}_1(\\mathcal{S}_1 = S_1)\\cdot\\mathbb{P}_2(\\mathcal{S}_2 = S_2) \\cdots \\mathbb{P}_H(\\mathcal{S}_H = S_H) \\] donde cada \\(\\mathbb{P}_h\\) es un esquema muestral para el estrato \\(h\\). En el caso del muestreo aleatorio estratificado tenemos un estimador de la media dada por la media ponderada de las medias: \\[ \\bar{x}_{\\mathcal{S}} = \\sum\\limits_{h = 1}^{H} \\dfrac{N_h}{N}\\cdot\\bar{x}_{\\mathcal{S}_h} \\] donde por comodidad denotaremos \\[ \\bar{x}_h = \\bar{x}_{\\mathcal{S}_h} \\] En particular, por la independencia se tiene: \\[ \\textrm{Var}\\big(\\bar{x}_{\\mathcal{S}}\\big) = \\sum\\limits_{h = 1}^{H} \\dfrac{N_h^2}{N^2}\\cdot\\textrm{Var}\\big(\\bar{x}_{h}\\big) \\] donde un estimador de la varianza es: \\[ \\widehat{\\textrm{Var}}\\big(\\bar{x}_{\\mathcal{S}}\\big) = \\sum\\limits_{h = 1}^{H} \\dfrac{N_h^2}{N^2}\\cdot\\widehat{\\textrm{Var}}\\big(\\bar{x}_{h}\\big) \\] En particular tenemos que para muestreo aleatorio simple tenemos un estimador insesgado: \\[ \\mathbb{E}\\Big[\\bar{x}_{\\mathcal{S}} \\Big] = \\sum\\limits_{h = 1}^{H} \\dfrac{N_h}{N} \\mathbb{E}\\big[\\bar{x}_h\\big]= \\sum\\limits_{h = 1}^{H} \\dfrac{N_h}{N}\\cdot \\dfrac{1}{N_h}\\sum\\limits_{i = 1}^{N_h}x_{i,h} =\\frac{1}{N} \\sum\\limits_{h = 1}^{H}\\sum\\limits_{i = 1}^N x_{i,h} = \\bar{x}_{\\mathcal{U}} \\] Su varianza es: \\[ \\textrm{Var}\\big(\\bar{x}_{\\mathcal{S}}\\big) = \\sum\\limits_{h = 1}^H N_h^2\\dfrac{1 - f_h}{n_h} s^2_{\\mathcal{U}_h} \\] donde \\(f_h = n_h/N_h\\) es la fracción de muestreo del estrato \\(h\\) y: \\[ s^2_{\\mathcal{U}_h} = \\dfrac{1}{N_h - 1}\\sum\\limits_{\\mathcal{U}_h}(x_k - \\bar{x}_{\\mathcal{U}_h})^2 \\] es la varianza del estrato con \\[ \\bar{x}_{\\mathcal{U}_h} = \\sum_{\\mathcal{U_h}} x_k \\] siendo la media del mismo. El estimador insesgado de la varianza en este caso es: \\[ \\widehat{\\textrm{Var}}\\big(\\bar{x}_{\\mathcal{S}}\\big) = \\sum\\limits_{h = 1}^H N_h^2\\dfrac{1 - f_h}{n_h} s^2_{\\mathcal{S}_h} \\] donde \\[ s^2_{\\mathcal{S}_h} = \\dfrac{1}{n_h - 1}\\sum\\limits_{\\mathcal{S}_h}(x_k - \\bar{x}_{\\mathcal{S}_h})^2 \\] es la varianza muestral ajustada. 4.1.1 Ejemplo Dada la población \\(\\mathcal{U} = \\{ x_1, x_2, x_3, x_4 \\}\\) con \\(x_1 = x_2 = 0\\), \\(x_3 = 1\\), \\(x_4 = -1\\): Calcula la varianza del estimador de la media de un muestreo aleatorio simple sin reemplazo de tamaño \\(2\\) Calcula la varianza del estimador de la media de un muestreo estratificado de donde se selecciona una unidad por cada estrato y los estratos están dados por \\(U_1 = \\{ x_1, x_2\\}\\) y \\(U_2 = \\{ x_3, x_4\\}\\). Solución Tenemos que: \\[ \\bar{x} = 0 \\] mientras que por otro lado, \\[ s^2_{\\mathcal{S}} = \\frac{1}{4 - 1} \\big( 1^2 + (-1)^2) = \\frac{2}{3} \\] Finalmente: \\[ \\textrm{Var}\\big( \\bar{x}\\big) = \\frac{N - n}{N} \\frac{s^2_{\\mathcal{S}}}{n} = \\frac{1}{6} \\] Por otro lado para resolver 2: \\[ \\bar{x}_1 = \\bar{x}_2 = 0 \\] Además de que: \\[ s^2_{\\mathcal{S}_1} = 0 \\] y: \\[ s^2_{\\mathcal{S}_2} = 2 \\] Tenemos entonces que: \\[ \\textrm{Var}( \\bar{x} ) = \\dfrac{N - n}{nN}\\sum_{h = 1}^2\\frac{N_h}{N}s^2_{\\mathcal{S}_h} = \\frac{1}{4} \\] Notamos que la varianza del muestreo estratificado es mayor a la varianza del muestreo simple. Por lo que concluimos que estratificar no necesariamente reduce la varianza. 4.1.2 Ejercicio de clase De entre 7500 empleados de una compañía deseamos conocer la proporción \\(P\\) que tiene un vehículo por lo menos. Se construyeron \\(3\\) estratos para la población según el ingreso (bajo, medio, alto). Se tiene \\(N_h\\) el total del estrato, \\(n_h\\) el total muestreado y \\(p_h\\) el estimador del total de vehículos para cada estrato \\(h = 1,2,3\\) según la muestra. Bajo Medio Alto \\(N_h\\) 3500.00 2000.00 2e+03 \\(n_h\\) 500.00 300.00 2e+02 \\(p_h\\) 0.13 0.45 5e-01 Determina un estimador \\(\\hat{p}\\) y su intervalo de confianza. 4.2 Alocación Una pregunta importante para el caso de muestreo estratificado es el cálculo de la(s) \\(n\\). En este caso ¿cómo determinar cuánto muestrear de cada población? Veamos un ejemplo: Supongamos que se desean muestrear hombres y mujeres en México. En este país el 48% de los habitantes son hombres y el 52% son mujeres. Una opción en este caso podría ser tomar una muestra que refleje exactamente esas proporciones. Ésta se conoce como proporcional al tamaño. 4.2.1 Alocación proporcional al tamaño Dada una población de tamaño \\(N\\) con \\(H\\) estratos de tamaños \\(N_1, N_2, \\dots, N_H\\) para \\(h = 1,2,\\dots, H\\) la alocación proporcional consiste en tomar \\(n_h\\) como: \\[ n_h = n\\cdot \\dfrac{N_h}{N} \\] Ésta forma de asignar variables no necesariamente es la mejor (mucho menos para estudios con costo) por lo cual se tienen otras alocaciones. Un alocación proporcional al tamaño representa usualmente una ganancia en la precisión (ver último ejemplo, el de los doctores) 4.2.2 Alocación óptima Si consideramos muestreo aleatorio simple sin reemplazo y analizamos su varianza podemos reescribirla: \\[ V = \\textrm{Var}\\big(\\bar{x}_{\\mathcal{S}}\\big) = \\sum\\limits_{h = 1}^H N_h^2\\dfrac{1 - f_h}{n_h} s^2_{\\mathcal{U}_h} = \\sum\\limits_{h = 1}^H \\frac{A_h}{n_h} + B \\] donde \\[ A_h = N^2_h s^2_{\\mathcal{U}_h} \\] y \\[ B = -\\sum\\limits_{h = 1}^H N_h s^2_{\\mathcal{U}_h} \\] (Ésta no es la única que se puede escribir de esta forma, también la de la Bernoulli, por ejemplo). Supongamos, además que asociado a muestrear cada estrato \\(h\\) hay un costo diferenciado \\(c_h\\) para cada elemento muestreado de \\(h\\). El costo total sería: \\[ C =c_0 + \\sum\\limits_{h = 1}^H n_h c_h \\] El problema de alocación de muestras es determinar las \\(n_h\\) que minimizan las varianzas \\(V\\) sujetas a los costos \\(C\\) (o puede verse de igual forma como hallar aquellas \\(n_h\\) que dadas varianzas predefinidas \\(V\\) minimizan los costos \\(C\\)). Teorema Bajo un muestreo aleatorio estratificado donde \\(V\\) puede escribirse como: \\[ V = \\sum\\limits_{h = 1}^H \\frac{A_h}{n_h} + B \\] y con una función de costo lineal \\(C =c_0 + \\sum_{h = 1}^H n_h c_h\\) la muestra óptima se alcanza tomando \\(n_h\\) proporcional a \\((A_h/c_h)^{1/2}\\). Demostración Sea \\(V^* = V - B\\) y \\(C^* = C - c_0\\). El problema de optimización se puede reescribir como minimizar el producto: \\[ V^* C^* = \\Big( \\sum\\limits_{h = 1}^H \\frac{A_h}{n_h}\\Big)\\cdot\\Big(\\sum\\limits_{h = 1}^H n_h c_h\\Big) \\] Utilizamos la desigualdad de Cauchy \\[ \\big(\\sum_h a_h^2\\big)\\big(\\sum_h b_h^2\\big) \\geq \\big(\\sum_h a_hb_h\\big)^2 \\] con \\(a_h = (A_h/n_h)^2\\) y \\(b_h = (n_h c_h)^{1/2}\\) tenemos: \\[ V^* C^* \\geq \\Big[ \\sum\\limits_{h = 1}^H (A_h c_h)^{1/2}\\Big]^2 \\] Recordamos que la igualdad en el caso de Cauchy se mantiene cuando \\(b_h/a_h\\) es constante para toda \\(h\\): \\[ \\Big( \\frac{n_h c_h}{A_h/n_h}\\Big)^{1/2} = \\text{Constante} \\] De donde se sigue el resultado que \\(n_h \\propto (A_h/c_h)^{1/2}\\) Nota Minimizar la varianza para un costo fijo \\(C\\) nos lleva a : \\[ n_h = \\dfrac{(C - c_0)(A_h/c_h)^{1/2}}{\\sum_{h = 1}^H (A_h c_h)^{1/2}} \\] en particular para muestreo aleatorio simple sin reemplazo puede demostrarse: \\[ n_h = \\dfrac{(C - c_0) N_h s_{\\mathcal{S}_h} / c_h^{1/2}}{\\sum_{h = 1}^H N_h s_{\\mathcal{S}_h} c_h^{1/2} } \\] Por otro lado, minimizar el costo para una varianza fija \\(V\\) nos lleva a: \\[ n_h = \\Big(\\frac{A_h}{c_h}\\Big)^{1/2}\\Big[ \\sum_{h = 1}^H (A_h c_h)^{1/2}\\Big] / (V - B) \\] Nota 2 Cuando se toman todas las \\(c_h\\) idénticas y constantes se le conoce como alocación de Neymann o sólo alocación óptima. 4.2.3 Ejemplo Se quiere estimar las ventas promedio de una población de empresas. Las empresas se enlistan según tres clases: según sus ventas en la siguiente tabla: Se sabe que se quieren estimar \\(111\\) empresas. Se supone, además que dentro de cada clase la distribución de ventas es uniforme. Obtén las varianzas de los estimadores cuando se toma alocación proporcional y cuando se toma óptima con costos constantes (Neyman). Solución Como la distribución intra-clase es uniforme podemos completar la tabla recordando que la varianza de una variable uniforme es: \\[ \\frac{(b-a)^2}{12} \\] de donde obtenemos la tabla actualizada: de donde se sigue que (para convertir a \\(1/N-1\\) de \\(1/N\\)): \\[\\begin{equation}\\nonumber \\begin{aligned} s^2_{h_1} &amp; = \\frac{1}{12} \\cdot \\frac{1000}{999} \\approx 0.0834168 \\\\ s^2_{h_2} &amp; = \\frac{81}{12} \\cdot \\frac{100}{99} \\approx 0.81818 \\\\ s^2_{h_3} &amp; = \\frac{8100}{12} \\cdot \\frac{10}{9} \\approx 750 \\\\ \\end{aligned} \\end{equation}\\] Luego: Estratificación proporcional al tamaño \\[ \\textrm{Var}(\\bar{x}) = \\dfrac{N - n}{nN} \\sum\\limits_{h = 1}^3 \\dfrac{N_h}{N} s^2_{h} \\approx 0.0604 \\] Estratificación óptima Por un lado tenemos que: \\[\\begin{equation}\\nonumber \\begin{aligned} N_1 s^2_1 &amp; = 288.82 \\\\ N_2 s^2_2 &amp; = 261.116 \\\\ N_3 s^2_3 &amp; = 273.861 \\\\ \\end{aligned} \\end{equation}\\] Lo que nos da las alocaciones óptimas: \\[\\begin{equation}\\nonumber \\begin{aligned} n_1 &amp; = \\dfrac{n N_1 s_1}{\\sum_{h = 1}^3 N_h s_h} &amp; = 38.9161 \\\\ n_2 &amp; = \\dfrac{n N_2 s_2}{\\sum_{h = 1}^3 N_h s_h} &amp; = 35.18 \\\\ n_3 &amp; = \\dfrac{n N_3 s_3}{\\sum_{h = 1}^3 N_h s_h} &amp; = 36.90 \\\\ \\end{aligned} \\end{equation}\\] En el caso del tercer estrato \\(n_3 &gt; N_3\\) por lo que seleccionamos \\(n_3 = 10\\). En este caso es necesario redistribuir de manera óptima los restantes \\(101\\) entre los estratos \\(1\\) y \\(2\\) por lo que recalculamos las \\(n\\): \\[\\begin{equation}\\nonumber \\begin{aligned} n_1 &amp; = \\dfrac{101 N_1 s_1}{N_1 s_1 + N_2 s_2} &amp; = 53.0439 \\\\ n_2 &amp; = \\dfrac{101 N_2 s_2}{N_1 s_1 + N_2 s_2} &amp; = 47.9561 \\\\ \\end{aligned} \\end{equation}\\] La distribución óptima entonces es \\(n_1 = 53, n_2 = 48, n_3 = 10\\). Finalmente la varianza está dada por: \\[ \\text{Var}(\\bar{x}_{\\mathcal{S}}) = \\sum\\limits_{h = 1}^3 \\dfrac{N_h^2}{N^2} \\dfrac{1 - f_h}{n_h} s^2_h = 0.0018 \\] 4.3 Ejercicio de clase: En una ciudad grande se estudia el número promedio de pacientes que ven los médicos en su día laboral. Comenzamos con algunas hipótesis a priori: entre más experiencia tiene un médico más pacientes ve. Esto nos lleva a clasificar a la población de médicos en tres grupos: recién graduados (grupo 1), intermedios (grupo 2) y experimentados (grupo 3). Tenemos una lista de \\(500\\) doctores en el grupo \\(1\\), \\(1000\\) en el grupo \\(2\\) y \\(2500\\) en el grupo \\(3\\). Seleccionamos mediante muestreo aleatorio simple sin reemplazo \\(200\\) doctores por cada clase y calculamos el número de pacientes por día y por doctor: \\(10\\) para el grupo \\(1\\), \\(15\\) para el grupo \\(2\\) y \\(20\\) para el grupo \\(3\\). Finalmente calculamos las varianzas del número de pacientes por doctor en cada una de las siguientes muestras y encontramos respectivamente que son \\(4\\) (grupo 1), \\(7\\) (grupo 2) y \\(10\\) (grupo 3). Estima la media del número de pacientes que ve un doctor por día y obtén un intervalo de confianza. Si al año siguiente se volviera a repetir el mismo análisis con \\(600\\) médicos (de nuevo) una hipótesis usual es que las varianzas no cambian. Determina la alocación de Neyman y la proporcional en este caso. Determina la ganancia en precisión de hacer alocación proporcional al tamaño por encima de hacer muestreo aleatorio simple Solución 1. Consideramos el estimador de la media: \\[ \\bar{x}_{\\mathcal{S}} = \\sum\\limits_{h = 1}^3 \\dfrac{N_h}{N} \\bar{x}_h = 17.5 \\] Por otro lado, su varianza está estimada por: \\[\\begin{equation}\\nonumber \\begin{aligned} \\widehat{\\textrm{Var}}(\\bar{x}_{\\mathcal{S}}) &amp; = \\sum\\limits_{h = 1}^3 \\Big( \\dfrac{N_h}{N}\\Big)^2 \\Big( 1 - \\frac{n_h}{N_h}\\Big) \\frac{s^2_h}{n_h} \\approx 0.0199 \\end{aligned} \\end{equation}\\] De donde tenemos el intervalo de confianza: \\[ \\bar{Y} \\pm 1.95\\sqrt{\\widehat{\\textrm{Var}}} \\Rightarrow \\text{IC}_{95\\%} = [17.5 - 0.28, 17.5 + 0.28] \\] 2. Una alocación proporcional al tamaño nos lleva a que \\(n_h = N_h / N\\) en este caso, \\(n_1 = 75\\), \\(n_2 = 150\\), \\(n_3 = 375\\). Por otro lado, si utilizamos para la de Neyman las varianzas del actual y suponemos serán similares el próximoa ño podemos estimar: \\(N_1 s_1 = 1000\\), \\(N_2 s_2 = 2646\\) y \\(N_3 s_3 = 7906\\). En este caso, \\(n_1 = 52\\), \\(n_2 = 137\\) y \\(n_3 = 411\\). A partir de la fórmula de descomposición de la varianza (demuestra) podemos aproximar la varianza poblacional a partir de las de la muestra: \\[ s_{\\mathcal{U}}^2 \\approx \\sum\\limits_{h = 1}^3 \\dfrac{N_h}{N} s^2_{\\mathcal{U}_h} + \\sum\\limits_{h = 1}^3 \\dfrac{N_h}{N} (\\bar{x}_{\\mathcal{U}_h} - \\bar{x}_{\\mathcal{U}})^2 \\] Sabemos que \\(\\mathbb{E}[s^2_{\\mathcal{S}_h}] = s^2_{\\mathcal{U}_h}\\) (el estimador es insesgado en cada estrato). Nos interesa el valor esperado de: \\[ A = \\sum\\limits_{h = 1}^3 \\dfrac{N_h}{N} (\\bar{x}_{\\mathcal{S}_h} - \\bar{x}_{\\mathcal{S}})^2 = \\sum\\limits_{h = 1}^3 \\dfrac{N_h}{N} \\bar{x}_{\\mathcal{S}_h}^2 - \\bar{x}_{\\mathcal{S}}^2 \\] Luego: \\[\\begin{equation}\\nonumber \\begin{aligned} \\mathbb{E}\\big[A\\big] &amp; = \\sum\\limits_{h = 1}^3 \\dfrac{N_h}{N} \\mathbb{E}\\big[\\bar{x}_{\\mathcal{S}_h}^2\\big] - \\mathbb{E}\\big[\\bar{x}_{\\mathcal{S}}^2\\big] \\\\ &amp; = \\sum\\limits_{h = 1}^3 \\dfrac{N_h}{N} \\Big( \\textrm{Var}(\\bar{x}_{\\mathcal{S}_h}^2) + \\bar{x}_{\\mathcal{S}_h}^2 \\Big) - \\Big( \\textrm{Var}\\big[\\bar{x}_{\\mathcal{S}}^2\\big] + \\bar{x}_{\\mathcal{S}}^2\\Big)\\\\ &amp; = \\sum\\limits_{h = 1}^3 \\dfrac{N_h}{N} \\Big( \\bar{x}_{\\mathcal{U}_h} - \\bar{x}_{\\mathcal{U}}\\Big)^2 + \\sum\\limits_{h = 1}^3 \\dfrac{N_h}{N} \\textrm{Var}(\\bar{x}_{\\mathcal{S}_h}) - \\textrm{Var}(\\bar{x}_{\\mathcal{S}}) \\end{aligned} \\end{equation}\\] En nuestro caso \\[ \\widehat{\\text{Var}}(\\bar{x}_{\\mathcal{S}_h}) = \\Big( 1 - \\frac{n_h}{N_h}\\Big) \\frac{s^2_{\\mathcal{S}_h}}{n_h} \\] es un estimador de \\(\\textrm{Var}(\\bar{x}_{\\mathcal{S}_h})\\). Si juntamos todo tenemos un estimador insesgado de \\(s^2_{\\mathcal{U}}\\) dado por: \\[ \\hat{S}^2_{\\mathcal{U}} = \\sum\\limits_{h = 1}^3 \\dfrac{N_h}{N} s^2_{\\mathcal{S}_h} + \\sum\\limits_{h = 1}^3 \\dfrac{N_h}{N} (\\bar{x}_{\\mathcal{S}_h} - \\bar{x}_{\\mathcal{S}})^2 - \\sum\\limits_{h = 1}^3 \\dfrac{N_h}{N} \\widehat{\\textrm{Var}}(\\bar{x}_{\\mathcal{S}_h}) + \\widehat{\\textrm{Var}}(\\bar{x}_{\\mathcal{S}}) = 20.983 \\] La varianza estimada entonces con muestreo aleatorio simple sin reemplazo es: \\[ \\widehat{V}_{\\text{MAS}} = \\frac{1-f}{n} \\hat{S}^2_{\\mathcal{U}} \\] Mientras que la estimada con alocación proporcional: \\[ \\widehat{V}_{\\text{Prop}} = \\frac{1-f}{n} \\Big( \\sum\\limits_{h = 1}^3 \\frac{N_h}{N} s^2_{\\mathcal{S}_h}\\Big) \\] de donde la ganancia de la proporcional está dada por: \\[ \\dfrac{\\widehat{V}_{\\text{Prop}}}{\\widehat{V}_{\\text{MAS}}} = \\dfrac{\\sum\\limits_{h = 1}^3 \\frac{N_h}{N} s^2_{\\mathcal{S}_h}}{\\hat{S}^2_{\\mathcal{U}}} \\approx 40.5\\% \\] Lo cual nos muestra que, en este caso, estratificar sí resulta en estimaciones más precisas. 4.4 Ejercicio en R tipo control La base de datos Base_a_estratificar (en este link) contiene una base con un millón de entradas correspondientes a los registros de un millón de clientes de una empresa. Se registró el grupo de edad, la entidad federativa y el género de la persona. Interesa realizar un muestreo aleatorio simple para estudiar el ingreso promedio de los clientes estratificando por grupo de edad, entidad y género. Se sabe además que los costos de muestreo por cada persona muestreada varían según el estado y puedes encontrarlos en la base Costos_x_entidad este link. Determina las \\(n\\) óptimas para el muestreo suponiendo que la varianza sólo varía por edad de acuerdo a la siguiente tabla (varianza son las \\(s^2\\)) pensando, además que nos interesa un error de \\(\\pm 50\\) al \\(95\\%\\). Edad Varianza &lt; 20 100 [20,60] 200 &gt;60 500 Suponiendo un costo basal de \\(500,000\\), ¿cuánto es el costo total de la encuesta? La base de datos Muestra link contiene una muestra estratificada por muestreo aleatorio simple sin reemplazo de los datos. Obtén el estimador del ingreso promedio y su intervalo de confianza para el total y para cada uno de los estratos. Solución En primer lugar calculamos \\(N_h\\) de cada estrato así como el \\(N\\): base.datos &lt;- read_rds(&quot;Base_a_estratificar.RDS&quot;) base.nh &lt;- base.datos %&gt;% group_by(Género, Entidad, Edad) %&gt;% tally() Por otro lado calculamos la varianza a partir del error usando que \\[ \\epsilon = Z_{1 - \\alpha/2}\\sqrt{\\textrm{Var}(\\bar{x}_{\\mathcal{S}})} \\qquad \\text{con} \\qquad \\alpha = 0.05 \\] Entonces: eps.error &lt;- 50 zalpha &lt;- qnorm(0.975) var.x &lt;- (eps.error/zalpha)^2 print(paste0(&quot;La varianza es &quot;, var.x)) ## [1] &quot;La varianza es 650.794429067515&quot; De la ecuación calculamos el término \\(V^* = V - B\\). Para ello recordamos que: \\[ B = -\\sum\\limits_{h = 1}^H N_h s^2_{\\mathcal{U}_h} \\] Entonces: edad &lt;- c(&quot;&lt; 20&quot;,&quot;[20,60]&quot;,&quot;&gt;60&quot;) dats &lt;- data.frame(Edad = edad, Varianza = c(100, 200, 500)) base.nh &lt;- base.nh %&gt;% left_join(dats, by = &quot;Edad&quot;) base.nh &lt;- base.nh %&gt;% mutate(BSumandos = Varianza*n) B &lt;- -sum(base.nh$BSumandos) print(paste0(&quot;B = &quot;, B)) ## [1] &quot;B = -21616500&quot; Por otro lado, obtenemos los costos: base.costos &lt;- read_rds(&quot;Costos_x_entidad.RDS&quot;) base.nh &lt;- base.nh %&gt;% left_join(base.costos, by = &quot;Entidad&quot;) Y calculamos los \\(A_h\\): base.nh &lt;- base.nh %&gt;% mutate(Ah = Varianza*n^2) Finalmente obtenemos los \\(n_h\\): sumaAh &lt;- sum(sqrt(base.nh$Ah*base.nh$Costo)) base.nh &lt;- base.nh %&gt;% mutate(nh = sqrt(Ah/Costo)*sumaAh/(var.x - B)) base.nh &lt;- base.nh %&gt;% mutate(nh = ceiling(nh)) Verificamos que no haya ningún \\(n_h &gt; N_h\\): base.nh &lt;- base.nh %&gt;% mutate(nh = ifelse(nh &gt; n, n, nh)) Para determinar el costo total de la encuesta, base.nh &lt;- base.nh %&gt;% mutate(Costo_estrato = Costo*nh) costo &lt;- 500000 + sum(base.nh$Costo_estrato) print(paste0(&quot;El costo es de $&quot;, scales::comma(costo))) ## [1] &quot;El costo es de $985,409&quot; Analizamos la base de datos muestra: muestra &lt;- read_rds(&quot;Muestra_estratificada.RDS&quot;) Obtenemos los estimadores puntuales de cada uno promedios.muestra &lt;- muestra %&gt;% group_by(Género, Entidad, Edad) %&gt;% summarise(Media = mean(Ingreso), S_h = var(Ingreso), n = n()) Agrego los \\(N_h\\) y los \\(N\\): promedios.muestra &lt;- promedios.muestra %&gt;% left_join(base.nh, by = c(&quot;Género&quot;, &quot;Entidad&quot;, &quot;Edad&quot;)) %&gt;% rename(`N_mayusc_h` = n.y) %&gt;% rename(`n_minusc_h` = n.x) Ntotal &lt;- sum(promedios.muestra$N_mayusc_h) El estimador total es el promedio ponderado de los de cada grupo: promedios.muestra &lt;- promedios.muestra %&gt;% mutate(factor_pop = N_mayusc_h/!!Ntotal) promedios.muestra &lt;- promedios.muestra %&gt;% mutate(sumando_media = factor_pop*Media) xbarra &lt;- sum(promedios.muestra$sumando_media) print(paste0(&quot;La media se estima con &quot;, xbarra)) ## [1] &quot;La media se estima con 1193.65573405234&quot; Mientras que la varianza se estima mediante: promedios.muestra &lt;- promedios.muestra %&gt;% mutate(varianza_intra_clase = (1 - n_minusc_h/N_mayusc_h)/n_minusc_h*S_h) promedios.muestra &lt;- promedios.muestra %&gt;% mutate(sumando_var = (factor_pop^2)*varianza_intra_clase) varianza.est &lt;- sum(promedios.muestra$sumando_var) Luego el intervalo está dado por: c( Lower = xbarra - zalpha*sqrt(varianza.est), Upper = xbarra + zalpha*sqrt(varianza.est) ) ## Lower Upper ## 1178.268 1209.043 Para los intervalos de cada estrato usamos la varianza específica de los mismos: promedios.muestra &lt;- promedios.muestra %&gt;% mutate(ic_lower = Media - !!zalpha*sqrt(varianza_intra_clase)) %&gt;% mutate(ic_upper = Media + !!zalpha*sqrt(varianza_intra_clase)) kable(promedios.muestra) %&gt;% kable_styling(latex_options = &quot;striped&quot;) Género Entidad Edad Media S_h n_minusc_h N_mayusc_h Varianza BSumandos Costo Ah nh Costo_estrato factor_pop sumando_media varianza_intra_clase sumando_var ic_lower ic_upper Hombre Aguascalientes [20,60] 1160.9109 64549.920 7 766 200 153200 34.32 117351200 294 10090.08 0.00766 8.8925778 9137.1484 0.5361277 973.5610 1348.2608 Hombre Aguascalientes &lt; 20 1157.7719 72988.914 11 535 100 53500 34.32 28622500 145 4976.40 0.00535 6.1940799 6498.9279 0.1860156 999.7676 1315.7763 Hombre Aguascalientes &gt;60 1215.9932 115311.993 20 260 500 130000 34.32 33800000 158 5422.56 0.00260 3.1615823 5322.0920 0.0359773 1073.0086 1358.9778 Hombre Baja California Norte [20,60] 1249.1890 111605.495 13 751 200 150200 2.11 112800200 751 1584.61 0.00751 9.3814091 8436.4289 0.4758154 1069.1662 1429.2117 Hombre Baja California Norte &lt; 20 1225.9103 194636.825 14 475 100 47500 2.11 22562500 475 1002.25 0.00475 5.8230741 13492.8686 0.3044328 998.2433 1453.5774 Hombre Baja California Norte &gt;60 1227.2721 102351.266 12 248 500 124000 2.11 30752000 248 523.28 0.00248 3.0436348 8116.5654 0.0499201 1050.6951 1403.8491 Hombre Baja California Sur [20,60] 1207.0631 105388.939 15 752 200 150400 6.92 113100800 642 4442.64 0.00752 9.0771148 6885.7844 0.3893939 1044.4241 1369.7022 Hombre Baja California Sur &lt; 20 1330.1951 151332.205 9 537 100 53700 6.92 28836900 325 2249.00 0.00537 7.1431476 16532.8790 0.4767570 1078.1824 1582.2077 Hombre Baja California Sur &gt;60 1169.8939 105405.420 11 239 500 119500 6.92 28560500 239 1653.88 0.00239 2.7960464 9141.2840 0.0522159 982.5016 1357.2862 Hombre Campeche [20,60] 1190.7680 109617.289 10 793 200 158600 13.89 125769800 478 6639.42 0.00793 9.4427901 10823.4977 0.6806346 986.8611 1394.6749 Hombre Campeche &lt; 20 1296.5288 183909.715 15 536 100 53600 13.89 28729600 229 3180.81 0.00536 6.9493944 11917.5325 0.3423859 1082.5645 1510.4931 Hombre Campeche &gt;60 1100.7067 77398.895 6 275 500 137500 13.89 37812500 262 3639.18 0.00275 3.0269435 12618.3652 0.0954264 880.5411 1320.8724 Hombre Chiapas [20,60] 1058.5052 112526.529 11 758 200 151600 3.47 114912800 758 2630.26 0.00758 8.0234693 10081.2326 0.5792313 861.7143 1255.2960 Hombre Chiapas &lt; 20 1182.7344 108902.265 11 499 100 49900 3.47 24900100 426 1478.22 0.00499 5.9018447 9681.9649 0.2410819 989.8799 1375.5889 Hombre Chiapas &gt;60 1186.5285 146464.343 15 248 500 124000 3.47 30752000 248 860.56 0.00248 2.9425907 9173.7075 0.0564220 998.8042 1374.2528 Hombre Chihuahua [20,60] 1221.0971 179155.101 17 726 200 145200 5.24 105415200 713 3736.12 0.00726 8.8651647 10291.7652 0.5424542 1022.2620 1419.9322 Hombre Chihuahua &lt; 20 1196.7915 66692.794 10 540 100 54000 5.24 29160000 375 1965.00 0.00540 6.4626743 6545.7742 0.1908748 1038.2188 1355.3643 Hombre Chihuahua &gt;60 1273.7945 62308.032 8 242 500 121000 5.24 29282000 242 1268.08 0.00242 3.0825826 7531.0327 0.0441047 1103.7058 1443.8831 Hombre Ciudad de México [20,60] 1201.1904 81969.714 14 772 200 154400 2.43 119196800 772 1875.96 0.00772 9.2731897 5748.8012 0.3426194 1052.5842 1349.7965 Hombre Ciudad de México &lt; 20 1206.2439 132304.246 10 532 100 53200 2.43 28302400 532 1292.76 0.00532 6.4172176 12981.7324 0.3674142 982.9307 1429.5571 Hombre Ciudad de México &gt;60 1247.1323 228599.912 10 227 500 113500 2.43 25764500 227 551.61 0.00227 2.8309903 21852.9431 0.1126060 957.3959 1536.8687 Hombre Coahuila [20,60] 1167.4726 170040.368 15 753 200 150600 5.22 113401800 740 3862.80 0.00753 8.7910683 11110.2073 0.6299588 960.8826 1374.0625 Hombre Coahuila &lt; 20 1173.5195 153515.205 20 514 100 51400 5.22 26419600 358 1868.76 0.00514 6.0318904 7377.0925 0.1948998 1005.1782 1341.8608 Hombre Coahuila &gt;60 1170.0741 124586.496 16 249 500 124500 5.22 31000500 249 1299.78 0.00249 2.9134845 7286.3086 0.0451758 1002.7718 1337.3764 Hombre Colima [20,60] 1061.6194 81084.975 4 745 200 149000 8.74 111005000 566 4946.84 0.00745 7.9090649 20162.4047 1.1190639 783.3156 1339.9233 Hombre Colima &lt; 20 1214.0360 275520.034 14 459 100 45900 8.74 21068100 247 2158.78 0.00459 5.5724252 19079.7409 0.4019739 943.3073 1484.7647 Hombre Colima &gt;60 1208.3185 152782.922 11 256 500 128000 8.74 32768000 256 2237.44 0.00256 3.0932953 13292.5483 0.0871140 982.3477 1434.2892 Hombre Durango [20,60] 1143.4237 102923.725 16 783 200 156600 3.46 122617800 783 2709.18 0.00783 8.9530077 6301.2849 0.3863248 987.8405 1299.0069 Hombre Durango &lt; 20 1212.6097 98808.579 14 512 100 51200 3.46 26214400 438 1515.48 0.00512 6.2085617 6864.7702 0.1799558 1050.2190 1375.0004 Hombre Durango &gt;60 1244.8630 13953.458 7 251 500 125500 3.46 31500500 251 868.46 0.00251 3.1246062 1937.7597 0.0122081 1158.5854 1331.1406 Hombre Estado de México [20,60] 1030.0824 109373.665 14 762 200 152400 5.14 116128800 755 3880.70 0.00762 7.8492277 7668.8696 0.4452883 858.4443 1201.7205 Hombre Estado de México &lt; 20 1281.5619 167622.673 12 487 100 48700 5.14 23716900 342 1757.88 0.00487 6.2412065 13624.3616 0.3231276 1052.7882 1510.3357 Hombre Estado de México &gt;60 1285.7485 123599.194 10 240 500 120000 5.14 28800000 240 1233.60 0.00240 3.0857963 11844.9227 0.0682268 1072.4370 1499.0599 Hombre Guanajuato [20,60] 1068.8483 70920.056 11 752 200 150400 10.81 113100800 514 5556.34 0.00752 8.0377396 6352.9693 0.3592630 912.6284 1225.0683 Hombre Guanajuato &lt; 20 1359.0351 232779.860 10 504 100 50400 10.81 25401600 244 2637.64 0.00504 6.8495371 22816.1212 0.5795660 1062.9824 1655.0878 Hombre Guanajuato &gt;60 1321.4735 91641.590 10 250 500 125000 10.81 31250000 250 2702.50 0.00250 3.3036839 8797.5927 0.0549850 1137.6378 1505.3093 Hombre Guerrero [20,60] 1215.0994 159936.739 12 738 200 147600 0.49 108928800 738 361.62 0.00738 8.9674336 13111.3451 0.7141015 990.6742 1439.5247 Hombre Guerrero &lt; 20 1278.6104 282360.300 8 488 100 48800 0.49 23814400 488 239.12 0.00488 6.2396189 34716.4303 0.8267510 913.4232 1643.7977 Hombre Guerrero &gt;60 1201.3422 124844.360 13 231 500 115500 0.49 26680500 231 113.19 0.00231 2.7751005 9062.9605 0.0483609 1014.7545 1387.9300 Hombre Hidalgo [20,60] 1129.2966 195408.174 14 739 200 147800 25.66 109224200 328 8416.48 0.00739 8.3455019 13693.3043 0.7478201 899.9448 1358.6484 Hombre Hidalgo &lt; 20 1175.5184 117419.642 15 518 100 51800 25.66 26832400 163 4182.58 0.00518 6.0891856 7601.2973 0.2039611 1004.6382 1346.3987 Hombre Hidalgo &gt;60 1161.8690 127316.410 10 255 500 127500 25.66 32512500 179 4593.14 0.00255 2.9627660 12232.3609 0.0795409 945.0970 1378.6410 Hombre Jalisco [20,60] 1188.7770 6525.069 7 811 200 162200 4.41 131544200 811 3576.51 0.00811 9.6409812 924.1070 0.0607805 1129.1958 1248.3582 Hombre Jalisco &lt; 20 1154.1446 74633.458 9 482 100 48200 4.41 23232400 365 1609.65 0.00482 5.5629770 8137.7652 0.1890598 977.3371 1330.9521 Hombre Jalisco &gt;60 1422.8185 82379.428 14 243 500 121500 4.41 29524500 243 1071.63 0.00243 3.4574489 5545.2349 0.0327441 1276.8672 1568.7698 Hombre Michoacán [20,60] 1237.4226 76144.062 16 720 200 144000 5.03 103680000 720 3621.60 0.00720 8.9094428 4653.2482 0.2412244 1103.7242 1371.1210 Hombre Michoacán &lt; 20 1162.9977 143248.797 11 537 100 53700 5.03 28836900 381 1916.43 0.00537 6.2452979 12755.8604 0.3678395 941.6358 1384.3597 Hombre Michoacán &gt;60 1035.5259 122454.904 9 274 500 137000 5.03 37538000 274 1378.22 0.00274 2.8373409 13159.1848 0.0987939 810.6916 1260.3602 Hombre Morelos [20,60] 1096.9048 66995.607 12 752 200 150400 2.35 113100800 752 1767.20 0.00752 8.2487240 5493.8774 0.3106810 951.6309 1242.1787 Hombre Morelos &lt; 20 1234.5305 76673.538 8 561 100 56100 2.35 31472100 561 1318.35 0.00561 6.9257160 9447.5193 0.2973333 1044.0252 1425.0357 Hombre Morelos &gt;60 1522.8539 169507.504 8 246 500 123000 2.35 30258000 246 578.10 0.00246 3.7462205 20499.3832 0.1240541 1242.2340 1803.4738 Hombre Nayarit [20,60] 1052.8052 202408.640 7 709 200 141800 5.44 100536200 683 3715.52 0.00709 7.4643889 28630.0353 1.4391775 721.1712 1384.4392 Hombre Nayarit &lt; 20 1293.7826 65185.724 8 514 100 51400 5.44 26419600 350 1904.00 0.00514 6.6500427 8021.3950 0.2119220 1118.2439 1469.3214 Hombre Nayarit &gt;60 1173.6317 27434.478 3 258 500 129000 5.44 33282000 258 1403.52 0.00258 3.0279698 9038.4907 0.0601638 987.2960 1359.9674 Hombre Nuevo León [20,60] 1299.0163 52305.706 10 702 200 140400 9.19 98560800 520 4778.80 0.00702 9.1190942 5156.0611 0.2540928 1158.2797 1439.7529 Hombre Nuevo León &lt; 20 1178.5272 96404.869 12 474 100 47400 9.19 22467600 249 2288.31 0.00474 5.5862188 7830.3532 0.1759292 1005.0914 1351.9630 Hombre Nuevo León &gt;60 1220.8625 126759.649 10 251 500 125500 9.19 31500500 251 2306.69 0.00251 3.0643649 12170.9463 0.0766782 1004.6354 1437.0897 Hombre Oaxaca [20,60] 1125.0004 33352.194 10 741 200 148200 4.80 109816200 741 3556.80 0.00741 8.3362530 3290.2097 0.1806592 1012.5762 1237.4246 Hombre Oaxaca &lt; 20 1232.2177 127816.584 13 510 100 51000 4.80 26010000 370 1776.00 0.00510 6.2843103 9581.4242 0.2492128 1040.3671 1424.0683 Hombre Oaxaca &gt;60 1059.7098 173983.284 13 270 500 135000 4.80 36450000 270 1296.00 0.00270 2.8612164 12738.9470 0.0928669 838.4946 1280.9249 Hombre Puebla [20,60] 1063.0416 90856.176 19 736 200 147200 1.87 108339200 736 1376.32 0.00736 7.8239864 4658.4581 0.2523468 929.2684 1196.8148 Hombre Puebla &lt; 20 1213.3631 181598.117 11 506 100 50600 1.87 25603600 506 946.22 0.00506 6.1396172 16150.0302 0.4134989 964.2854 1462.4407 Hombre Puebla &gt;60 1250.2833 134213.189 17 229 500 114500 1.87 26220500 229 428.23 0.00229 2.8631488 7308.8097 0.0383281 1082.7229 1417.8437 Hombre Querétaro [20,60] 1359.3707 120580.634 10 777 200 155400 1.99 120745800 777 1546.23 0.00777 10.5623105 11902.8760 0.7186111 1145.5381 1573.2034 Hombre Querétaro &lt; 20 1116.7279 41082.151 11 501 100 50100 1.99 25100100 501 996.99 0.00501 5.5948070 3652.7407 0.0916842 998.2718 1235.1841 Hombre Querétaro &gt;60 1141.2693 47084.848 11 227 500 113500 1.99 25764500 227 451.73 0.00227 2.5906813 4073.0185 0.0209879 1016.1840 1266.3546 Hombre Quintana Roo [20,60] 1280.4653 215367.867 13 712 200 142400 0.44 101388800 712 313.28 0.00712 9.1169130 16264.2760 0.8245077 1030.5082 1530.4224 Hombre Quintana Roo &lt; 20 1235.3861 117640.820 13 514 100 51400 0.44 26419600 514 226.16 0.00514 6.3498848 8820.4207 0.2330320 1051.3120 1419.4603 Hombre Quintana Roo &gt;60 1291.3932 223337.149 13 245 500 122500 0.44 30012500 245 107.80 0.00245 3.1639134 16268.2005 0.0976499 1041.4060 1541.3805 Hombre San Luis Potosí [20,60] 1098.4050 70820.668 16 727 200 145400 2.36 105705800 727 1715.72 0.00727 7.9854043 4328.8768 0.2287937 969.4507 1227.3593 Hombre San Luis Potosí &lt; 20 1143.8343 71902.444 16 500 100 50000 2.36 25000000 500 1180.00 0.00500 5.7191717 4350.0978 0.1087524 1014.5644 1273.1043 Hombre San Luis Potosí &gt;60 1386.1919 222858.410 10 236 500 118000 2.36 27848000 236 556.96 0.00236 3.2714129 21341.5257 0.1188638 1099.8659 1672.5179 Hombre Sinaloa [20,60] 1407.6534 86372.912 11 758 200 151600 3.79 114912800 758 2872.82 0.00758 10.6700128 7738.1345 0.4446053 1235.2419 1580.0649 Hombre Sinaloa &lt; 20 1052.3875 97665.413 18 490 100 49000 3.79 24010000 400 1516.00 0.00490 5.1566986 5226.5391 0.1254892 910.6923 1194.0827 Hombre Sinaloa &gt;60 1252.7122 49642.740 12 257 500 128500 3.79 33024500 257 974.03 0.00257 3.2194702 3943.7326 0.0260480 1129.6281 1375.7962 Hombre Sonora [20,60] 1237.2947 135232.992 10 815 200 163000 7.76 132845000 657 5098.32 0.00815 10.0839517 13357.3691 0.8872299 1010.7736 1463.8157 Hombre Sonora &lt; 20 1241.1958 105801.839 13 467 100 46700 7.76 21808900 267 2071.92 0.00467 5.7963844 7912.0466 0.1725530 1066.8576 1415.5340 Hombre Sonora &gt;60 1199.9155 131649.271 14 235 500 117500 7.76 27612500 235 1823.60 0.00235 2.8198014 8843.3097 0.0488372 1015.6027 1384.2283 Hombre Tabasco [20,60] 1203.0331 173122.677 15 816 200 163200 4.59 133171200 816 3745.44 0.00816 9.8167502 11329.3516 0.7543717 994.4157 1411.6506 Hombre Tabasco &lt; 20 1391.9049 129236.343 10 481 100 48100 4.59 23136100 357 1638.63 0.00481 6.6950626 12654.9517 0.2927862 1171.4203 1612.3896 Hombre Tabasco &gt;60 1186.6347 79131.793 10 238 500 119000 4.59 28322000 238 1092.42 0.00238 2.8241907 7580.6928 0.0429401 1015.9862 1357.2833 Hombre Tamaulipas [20,60] 1214.2472 110305.480 12 783 200 156600 6.52 122617800 689 4492.28 0.00783 9.5075553 9051.2479 0.5549221 1027.7800 1400.7143 Hombre Tamaulipas &lt; 20 1093.0011 112312.774 12 525 100 52500 6.52 27562500 327 2132.04 0.00525 5.7382557 9145.4687 0.2520720 905.5659 1280.4363 Hombre Tamaulipas &gt;60 1286.6968 307350.631 11 270 500 135000 6.52 36450000 270 1760.40 0.00270 3.4740813 26802.6308 0.1953912 965.8211 1607.5725 Hombre Tlaxcala [20,60] 1315.8986 88261.199 12 778 200 155600 4.60 121056800 778 3578.80 0.00778 10.2376912 7241.6536 0.4383257 1149.1098 1482.6874 Hombre Tlaxcala &lt; 20 1177.2379 99981.371 7 465 100 46500 4.60 21622500 345 1587.00 0.00465 5.4741564 14068.0393 0.3041862 944.7690 1409.7068 Hombre Tlaxcala &gt;60 1152.5891 66395.157 11 259 500 129500 4.60 33540500 259 1191.40 0.00259 2.9852057 5779.5714 0.0387699 1003.5858 1301.5924 Hombre Veracruz [20,60] 1135.6109 57279.644 15 752 200 150400 16.65 113100800 414 6893.10 0.00752 8.5397938 3742.4732 0.2116384 1015.7086 1255.5132 Hombre Veracruz &lt; 20 1042.2512 94394.733 11 539 100 53900 16.65 29052100 210 3496.50 0.00539 5.6177342 8406.2100 0.2442181 862.5512 1221.9513 Hombre Veracruz &gt;60 1220.8847 117878.884 16 258 500 129000 16.65 33282000 225 3746.25 0.00258 3.1498826 6910.5353 0.0459993 1057.9537 1383.8158 Hombre Yucatán [20,60] 1162.5284 138436.551 9 742 200 148400 1.15 110112800 742 853.30 0.00742 8.6259605 15195.2669 0.8365967 920.9254 1404.1313 Hombre Yucatán &lt; 20 1240.4180 110683.751 8 486 100 48600 1.15 23619600 486 558.90 0.00486 6.0284314 13607.7245 0.3214090 1011.7840 1469.0520 Hombre Yucatán &gt;60 1148.7365 244082.698 12 249 500 124500 1.15 31000500 249 286.35 0.00249 2.8603538 19359.9730 0.1200338 876.0269 1421.4461 Hombre Zacatecas [20,60] 1049.8101 98177.645 14 771 200 154200 22.18 118888200 368 8162.24 0.00771 8.0940361 6885.3509 0.4092935 887.1762 1212.4441 Hombre Zacatecas &lt; 20 1360.0553 344291.959 15 489 100 48900 22.18 23912100 165 3659.70 0.00489 6.6506702 22248.7237 0.5320137 1067.7069 1652.4036 Hombre Zacatecas &gt;60 1118.4467 149792.162 15 239 500 119500 22.18 28560500 181 4014.58 0.00239 2.6730875 9359.3987 0.0534618 928.8319 1308.0614 Mujer Aguascalientes [20,60] 965.3583 52564.222 12 731 200 146200 34.32 106872200 281 9643.92 0.00731 7.0567691 4308.4445 0.2302265 836.7087 1094.0079 Mujer Aguascalientes &lt; 20 1185.4574 187007.463 11 558 100 55800 34.32 31136400 152 5216.64 0.00558 6.6148526 16665.5396 0.5189049 932.4357 1438.4792 Mujer Aguascalientes &gt;60 1238.9959 114471.167 9 259 500 129500 34.32 33540500 157 5388.24 0.00259 3.2089994 12277.0450 0.0823556 1021.8283 1456.1635 Mujer Baja California Norte [20,60] 1314.3148 128584.459 13 782 200 156400 2.11 122304800 782 1650.02 0.00782 10.2779415 9726.6819 0.5948099 1121.0154 1507.6141 Mujer Baja California Norte &lt; 20 965.4953 63432.983 12 537 100 53700 2.11 28836900 537 1133.07 0.00537 5.1847099 5167.9572 0.1490279 824.5965 1106.3942 Mujer Baja California Norte &gt;60 1374.7139 135736.320 6 274 500 137000 2.11 37538000 274 578.14 0.00274 3.7667162 22127.3319 0.1661232 1083.1642 1666.2636 Mujer Baja California Sur [20,60] 1377.1076 153555.858 12 832 200 166400 6.92 138444800 711 4920.12 0.00832 11.4575348 12611.7592 0.8730162 1156.9995 1597.2156 Mujer Baja California Sur &lt; 20 984.7081 30715.165 13 560 100 56000 6.92 31360000 338 2338.96 0.00560 5.5143651 2307.8565 0.0723744 890.5511 1078.8650 Mujer Baja California Sur &gt;60 1052.1851 35582.574 14 266 500 133000 6.92 35378000 266 1840.72 0.00266 2.7988124 2407.8433 0.0170369 956.0101 1148.3601 Mujer Campeche [20,60] 1274.1003 164331.489 15 764 200 152800 13.89 116739200 461 6403.29 0.00764 9.7341264 10740.3390 0.6269093 1070.9783 1477.2224 Mujer Campeche &lt; 20 1277.0535 132777.783 11 545 100 54500 13.89 29702500 233 3236.37 0.00545 6.9599414 11827.0786 0.3512938 1063.9027 1490.2042 Mujer Campeche &gt;60 1243.5654 85593.134 8 264 500 132000 13.89 34848000 252 3500.28 0.00264 3.2830126 10374.9253 0.0723091 1043.9286 1443.2022 Mujer Chiapas [20,60] 1284.0826 57843.092 7 776 200 155200 3.47 120435200 776 2692.72 0.00776 9.9644806 8188.7588 0.4931074 1106.7220 1461.4431 Mujer Chiapas &lt; 20 1072.4410 101692.179 11 548 100 54800 3.47 30030400 468 1623.96 0.00548 5.8769766 9059.1738 0.2720506 885.8922 1258.9898 Mujer Chiapas &gt;60 1347.3974 174708.366 18 292 500 146000 3.47 42632000 292 1013.24 0.00292 3.9344003 9107.7040 0.0776559 1160.3496 1534.4452 Mujer Chihuahua [20,60] 1080.2059 74935.992 13 760 200 152000 5.24 115520000 746 3909.04 0.00760 8.2095651 5665.7071 0.3272512 932.6777 1227.7342 Mujer Chihuahua &lt; 20 1218.0873 66183.184 12 538 100 53800 5.24 28944400 374 1959.76 0.00538 6.5533098 5392.2483 0.1560754 1074.1634 1362.0113 Mujer Chihuahua &gt;60 1216.0253 84414.653 9 227 500 113500 5.24 25764500 227 1189.48 0.00227 2.7603774 9007.5351 0.0464149 1030.0090 1402.0416 Mujer Ciudad de México [20,60] 1364.6948 133859.424 8 780 200 156000 2.43 121680000 780 1895.40 0.00780 10.6446196 16560.8134 1.0075599 1112.4694 1616.9203 Mujer Ciudad de México &lt; 20 1099.9886 85086.993 12 543 100 54300 2.43 29484900 543 1319.49 0.00543 5.9729382 6933.8848 0.2044449 936.7825 1263.1947 Mujer Ciudad de México &gt;60 1248.8304 121270.983 19 249 500 124500 2.43 31000500 249 605.07 0.00249 3.1095877 5895.6512 0.0365536 1098.3382 1399.3226 Mujer Coahuila [20,60] 1038.2508 50143.475 10 799 200 159800 5.22 127680200 786 4102.92 0.00799 8.2956235 4951.5897 0.3161100 900.3329 1176.1686 Mujer Coahuila &lt; 20 1165.5764 111466.593 14 527 100 52700 5.22 27772900 367 1915.74 0.00527 6.1425876 7750.3879 0.2152507 993.0284 1338.1243 Mujer Coahuila &gt;60 1118.8744 125381.144 9 281 500 140500 5.22 39480500 281 1466.82 0.00281 3.1440370 13485.0420 0.1064792 891.2733 1346.4754 Mujer Colima [20,60] 1150.8040 117251.228 14 765 200 153000 8.74 117045000 581 5077.94 0.00765 8.8036504 8221.8182 0.4811614 973.0857 1328.5222 Mujer Colima &lt; 20 1186.7753 111051.961 16 510 100 51000 8.74 26010000 274 2394.76 0.00510 6.0525540 6722.9986 0.1748652 1026.0702 1347.4804 Mujer Colima &gt;60 1250.1292 93845.599 12 299 500 149500 8.74 44700500 299 2613.26 0.00299 3.7378863 7506.6017 0.0671098 1080.3167 1419.9418 Mujer Durango [20,60] 1157.1173 156285.900 12 837 200 167400 3.46 140113800 837 2896.02 0.00837 9.6850715 12837.1035 0.8993277 935.0515 1379.1830 Mujer Durango &lt; 20 1144.9842 79820.760 9 515 100 51500 3.46 26522500 440 1522.40 0.00515 5.8966686 8713.9815 0.2311166 962.0241 1327.9443 Mujer Durango &gt;60 1087.1909 201026.067 12 266 500 133000 3.46 35378000 266 920.36 0.00266 2.8919278 15996.4351 0.1131844 839.3005 1335.0813 Mujer Estado de México [20,60] 1083.0558 78052.824 11 824 200 164800 5.14 135795200 817 4199.38 0.00824 8.9243799 7000.9870 0.4753502 919.0619 1247.0497 Mujer Estado de México &lt; 20 1294.7424 183961.387 17 551 100 55100 5.14 30360100 386 1984.04 0.00551 7.1340309 10487.3898 0.3183982 1094.0265 1495.4584 Mujer Estado de México &gt;60 1082.5636 180452.421 14 256 500 128000 5.14 32768000 256 1315.84 0.00256 2.7713629 12184.5664 0.0798528 866.2155 1298.9118 Mujer Guanajuato [20,60] 1054.8199 78970.971 8 793 200 158600 10.81 125769800 542 5859.02 0.00793 8.3647219 9771.7863 0.6144978 861.0729 1248.5669 Mujer Guanajuato &lt; 20 1225.2195 82062.886 11 535 100 53500 10.81 28622500 259 2799.79 0.00535 6.5549244 7306.8738 0.2091410 1057.6813 1392.7577 Mujer Guanajuato &gt;60 1117.4307 44693.930 16 276 500 138000 10.81 38088000 276 2983.56 0.00276 3.0841088 2631.4361 0.0200452 1016.8894 1217.9720 Mujer Guerrero [20,60] 1206.3988 133543.349 15 773 200 154600 0.49 119505800 773 378.77 0.00773 9.3254624 8730.1301 0.5216506 1023.2692 1389.5283 Mujer Guerrero &lt; 20 1049.4879 108092.102 10 541 100 54100 0.49 29268100 541 265.09 0.00541 5.6777296 10609.4097 0.3105173 847.6077 1251.3681 Mujer Guerrero &gt;60 1111.1607 199336.081 13 231 500 115500 0.49 26680500 231 113.19 0.00231 2.5667813 14470.6180 0.0772167 875.3891 1346.9324 Mujer Hidalgo [20,60] 1055.1255 205299.784 7 797 200 159400 25.66 127041800 354 9083.64 0.00797 8.4093502 29070.9498 1.8466129 720.9476 1389.3034 Mujer Hidalgo &lt; 20 1222.8737 171575.389 19 506 100 50600 25.66 25603600 159 4079.94 0.00506 6.1877409 8691.2018 0.2225261 1040.1529 1405.5945 Mujer Hidalgo &gt;60 1346.5189 137331.811 10 251 500 125500 25.66 31500500 176 4516.16 0.00251 3.3797623 13186.0424 0.0830734 1121.4552 1571.5825 Mujer Jalisco [20,60] 1104.4133 148679.413 14 818 200 163600 4.41 133824800 818 3607.38 0.00818 9.0341011 10438.1984 0.6984449 904.1687 1304.6580 Mujer Jalisco &lt; 20 1215.2296 114986.138 15 558 100 55800 4.41 31136400 422 1861.02 0.00558 6.7809813 7459.6742 0.2322674 1045.9487 1384.5106 Mujer Jalisco &gt;60 1116.7456 65404.837 15 263 500 131500 4.41 34584500 263 1159.83 0.00263 2.9370409 4111.6349 0.0284398 991.0687 1242.4225 Mujer Michoacán [20,60] 1247.2035 157379.983 14 791 200 158200 5.03 125136200 791 3978.73 0.00791 9.8653794 11042.4641 0.6909060 1041.2443 1453.1626 Mujer Michoacán &lt; 20 1292.4362 165911.805 8 534 100 53400 5.03 28515600 379 1906.37 0.00534 6.9016096 20428.2794 0.5825246 1012.3034 1572.5691 Mujer Michoacán &gt;60 1317.3918 50253.875 10 275 500 137500 5.03 37812500 275 1383.25 0.00275 3.6228274 4842.6462 0.0366225 1180.9996 1453.7840 Mujer Morelos [20,60] 1160.1294 120041.394 17 757 200 151400 2.35 114609800 757 1778.95 0.00757 8.7821795 6902.6833 0.3955576 997.2909 1322.9679 Mujer Morelos &lt; 20 1088.7616 37922.594 11 538 100 53800 2.35 28944400 538 1264.30 0.00538 5.8575372 3377.0205 0.0977458 974.8639 1202.6593 Mujer Morelos &gt;60 1208.4049 53829.591 13 255 500 127500 2.35 32512500 255 599.25 0.00255 3.0814324 3929.6413 0.0255525 1085.5409 1331.2688 Mujer Nayarit [20,60] 1296.3370 110945.038 20 740 200 148000 5.44 109520000 713 3878.72 0.00740 9.5928936 5397.3262 0.2955576 1152.3453 1440.3287 Mujer Nayarit &lt; 20 1186.8195 73140.226 15 551 100 55100 5.44 30360100 376 2045.44 0.00551 6.5393755 4743.2742 0.1440063 1051.8340 1321.8050 Mujer Nayarit &gt;60 1234.4286 155231.987 12 251 500 125500 5.44 31500500 251 1365.44 0.00251 3.0984158 12317.5448 0.0776018 1016.9031 1451.9541 Mujer Nuevo León [20,60] 1120.3470 106538.635 14 794 200 158800 9.19 126087200 589 5412.91 0.00794 8.8955555 7475.7228 0.4712965 950.8841 1289.8100 Mujer Nuevo León &lt; 20 1179.5186 136438.203 10 533 100 53300 9.19 28408900 280 2573.20 0.00533 6.2868343 13387.8387 0.3803338 952.7394 1406.2979 Mujer Nuevo León &gt;60 1225.8209 171933.640 19 280 500 140000 9.19 39200000 280 2573.20 0.00280 3.4322986 8435.0902 0.0661311 1045.8124 1405.8294 Mujer Oaxaca [20,60] 1190.3074 183562.640 13 758 200 151600 4.80 114912800 758 3638.40 0.00758 9.0225300 13878.0360 0.7973820 959.4137 1421.2011 Mujer Oaxaca &lt; 20 1186.7941 111090.949 16 559 100 55900 4.80 31248100 406 1948.80 0.00559 6.6341792 6744.4527 0.2107513 1025.8328 1347.7554 Mujer Oaxaca &gt;60 1460.3492 128729.203 11 258 500 129000 4.80 33282000 258 1238.40 0.00258 3.7677009 11203.7045 0.0745763 1252.8918 1667.8066 Mujer Puebla [20,60] 1099.8245 38675.632 14 771 200 154200 1.87 118888200 771 1441.77 0.00771 8.4796473 2712.3822 0.1612351 997.7486 1201.9005 Mujer Puebla &lt; 20 1071.0788 36600.802 11 545 100 54500 1.87 29702500 545 1019.15 0.00545 5.8373792 3260.1882 0.0968357 959.1686 1182.9889 Mujer Puebla &gt;60 1122.4992 63759.916 6 274 500 137000 1.87 37538000 274 512.38 0.00274 3.0756477 10393.9522 0.0780336 922.6794 1322.3189 Mujer Querétaro [20,60] 1155.5605 104804.463 9 786 200 157200 1.99 123559200 786 1564.14 0.00786 9.0827058 11511.6013 0.7111821 945.2718 1365.8492 Mujer Querétaro &lt; 20 1307.5487 238972.845 10 528 100 52800 1.99 27878400 528 1050.72 0.00528 6.9038571 23444.6844 0.6536003 1007.4457 1607.6517 Mujer Querétaro &gt;60 1075.0890 184933.233 12 276 500 138000 1.99 38088000 276 549.24 0.00276 2.9672457 14741.0548 0.1122915 837.1244 1313.0536 Mujer Quintana Roo [20,60] 1112.4654 63133.779 10 768 200 153600 0.44 117964800 768 337.92 0.00768 8.5437344 6231.1725 0.3675295 957.7502 1267.1806 Mujer Quintana Roo &lt; 20 1144.9299 76227.677 12 476 100 47600 0.44 22657600 476 209.44 0.00476 5.4498663 6192.1642 0.1402996 990.6997 1299.1601 Mujer Quintana Roo &gt;60 1313.5124 128859.091 14 286 500 143000 0.44 40898000 286 125.84 0.00286 3.7566453 8753.6645 0.0716015 1130.1361 1496.8886 Mujer San Luis Potosí [20,60] 1081.7185 138706.952 11 787 200 157400 2.36 123873800 787 1857.32 0.00787 8.5131249 12433.4752 0.7700909 863.1718 1300.2653 Mujer San Luis Potosí &lt; 20 1204.4747 56247.479 13 542 100 54200 2.36 29376400 542 1279.12 0.00542 6.5282528 4222.9515 0.1240551 1077.1079 1331.8415 Mujer San Luis Potosí &gt;60 1185.1372 189856.212 10 260 500 130000 2.36 33800000 260 613.60 0.00260 3.0813568 18255.4050 0.1234065 920.3215 1449.9530 Mujer Sinaloa [20,60] 1278.7543 77572.167 8 823 200 164600 3.79 135465800 823 3119.17 0.00823 10.5241481 9602.2655 0.6503893 1086.6952 1470.8135 Mujer Sinaloa &lt; 20 1018.3889 94721.993 11 548 100 54800 3.79 30030400 447 1694.13 0.00548 5.5807714 8438.2399 0.2534037 838.3469 1198.4310 Mujer Sinaloa &gt;60 1257.0014 140769.422 17 267 500 133500 3.79 35644500 267 1011.93 0.00267 3.3561936 7753.3279 0.0552727 1084.4207 1429.5820 Mujer Sonora [20,60] 1643.4982 264318.242 7 771 200 154200 7.76 118888200 622 4826.72 0.00771 12.6713715 37416.9236 2.2242153 1264.3735 2022.6230 Mujer Sonora &lt; 20 1177.1543 136815.282 9 526 100 52600 7.76 27667600 300 2328.00 0.00526 6.1918318 14941.5929 0.4133980 937.5766 1416.7321 Mujer Sonora &gt;60 1340.4347 120658.300 12 281 500 140500 7.76 39480500 281 2180.56 0.00281 3.7666216 9625.4694 0.0760037 1148.1437 1532.7258 Mujer Tabasco [20,60] 1351.6654 162449.032 12 744 200 148800 4.59 110707200 744 3414.96 0.00744 10.0563907 13319.0739 0.7372587 1125.4693 1577.8615 Mujer Tabasco &lt; 20 1252.2089 102273.299 17 536 100 53600 4.59 28729600 398 1826.82 0.00536 6.7118398 5825.2680 0.1673576 1102.6177 1401.8001 Mujer Tabasco &gt;60 1133.4492 119793.415 17 258 500 129000 4.59 33282000 258 1184.22 0.00258 2.9242991 6582.3559 0.0438148 974.4340 1292.4645 Mujer Tamaulipas [20,60] 1328.8109 162751.909 15 799 200 159800 6.52 127680200 703 4583.56 0.00799 10.6171994 10646.4327 0.6796693 1126.5788 1531.0431 Mujer Tamaulipas &lt; 20 1240.3795 68961.715 15 561 100 56100 6.52 31472100 349 2275.48 0.00561 6.9585289 4474.5212 0.1408226 1109.2738 1371.4851 Mujer Tamaulipas &gt;60 1363.8311 126943.754 10 246 500 123000 6.52 30258000 246 1603.92 0.00246 3.3550245 12178.3439 0.0736985 1147.5382 1580.1240 Mujer Tlaxcala [20,60] 1320.8632 80375.886 6 833 200 166600 4.60 138777800 833 3831.80 0.00833 11.0027900 13299.4914 0.9228371 1094.8334 1546.8929 Mujer Tlaxcala &lt; 20 1325.2656 275040.792 17 507 100 50700 4.60 25704900 376 1729.60 0.00507 6.7190966 15636.3833 0.4019317 1080.1809 1570.3503 Mujer Tlaxcala &gt;60 1118.4119 100080.901 10 264 500 132000 4.60 34848000 264 1214.40 0.00264 2.9526075 9628.9958 0.0671102 926.0857 1310.7382 Mujer Veracruz [20,60] 851.7864 23447.337 3 807 200 161400 16.65 130249800 445 7409.25 0.00807 6.8739164 7786.7239 0.5071096 678.8345 1024.7384 Mujer Veracruz &lt; 20 1036.0357 25495.699 8 516 100 51600 16.65 26625600 201 3346.65 0.00516 5.3459440 3137.5520 0.0835392 926.2505 1145.8208 Mujer Veracruz &gt;60 1275.4856 159518.668 15 266 500 133000 16.65 35378000 232 3862.80 0.00266 3.3927918 10034.8836 0.0710028 1079.1477 1471.8236 Mujer Yucatán [20,60] 1236.5699 63524.903 13 761 200 152200 1.15 115824200 761 875.15 0.00761 9.4102970 4803.0554 0.2781550 1100.7364 1372.4034 Mujer Yucatán &lt; 20 1305.0264 113671.759 12 520 100 52000 1.15 27040000 520 598.00 0.00520 6.7861371 9254.0470 0.2502294 1116.4818 1493.5709 Mujer Yucatán &gt;60 1126.7606 121551.782 13 270 500 135000 1.15 36450000 270 310.50 0.00270 3.0422535 8899.9453 0.0648806 941.8585 1311.6626 Mujer Zacatecas [20,60] 1432.2067 241640.916 7 798 200 159600 22.18 127360800 381 8450.58 0.00798 11.4290093 34217.3228 2.1789728 1069.6540 1794.7593 Mujer Zacatecas &lt; 20 1005.1855 81747.219 13 511 100 51100 22.18 26112100 173 3837.14 0.00511 5.1364981 6128.2726 0.1600221 851.7531 1158.6180 Mujer Zacatecas &gt;60 1158.9556 155064.150 13 269 500 134500 22.18 36180500 203 4502.54 0.00269 3.1175907 11351.5649 0.0821411 950.1338 1367.7775 Otro Aguascalientes [20,60] 1197.6385 240355.401 5 12 200 2400 34.32 28800 5 171.60 0.00012 0.1437166 28041.4635 0.0004038 869.4310 1525.8459 Otro Aguascalientes &lt; 20 1297.3356 40317.755 8 8 100 800 34.32 6400 3 102.96 0.00008 0.1037869 0.0000 0.0000000 1297.3356 1297.3356 Otro Aguascalientes &gt;60 989.1890 179995.357 4 4 500 2000 34.32 8000 3 102.96 0.00004 0.0395676 0.0000 0.0000000 989.1890 989.1890 Otro Baja California Norte [20,60] 887.7728 36973.725 8 16 200 3200 2.11 51200 16 33.76 0.00016 0.1420436 2310.8578 0.0000592 793.5546 981.9910 Otro Baja California Norte &lt; 20 1397.9326 151161.376 10 10 100 1000 2.11 10000 10 21.10 0.00010 0.1397933 0.0000 0.0000000 1397.9326 1397.9326 Otro Baja California Norte &gt;60 1379.4369 1932.935 3 3 500 1500 2.11 4500 3 6.33 0.00003 0.0413831 0.0000 0.0000000 1379.4369 1379.4369 Otro Baja California Sur [20,60] 1079.0216 115502.593 11 18 200 3600 6.92 64800 16 110.72 0.00018 0.1942239 4083.4250 0.0001323 953.7766 1204.2666 Otro Baja California Sur &lt; 20 1171.7196 94202.661 12 12 100 1200 6.92 14400 8 55.36 0.00012 0.1406063 0.0000 0.0000000 1171.7196 1171.7196 Otro Baja California Sur &gt;60 890.3546 26107.667 2 2 500 1000 6.92 2000 2 13.84 0.00002 0.0178071 0.0000 0.0000000 890.3546 890.3546 Otro Campeche [20,60] 1211.9838 95001.959 11 24 200 4800 13.89 115200 15 208.35 0.00024 0.2908761 4678.1268 0.0002695 1077.9285 1346.0391 Otro Campeche &lt; 20 1142.6506 23269.303 10 10 100 1000 13.89 10000 5 69.45 0.00010 0.1142651 0.0000 0.0000000 1142.6506 1142.6506 Otro Campeche &gt;60 1188.6141 55861.792 4 4 500 2000 13.89 8000 4 55.56 0.00004 0.0475446 0.0000 0.0000000 1188.6141 1188.6141 Otro Chiapas [20,60] 1269.2610 52793.179 11 15 200 3000 3.47 45000 15 52.05 0.00015 0.1903891 1279.8346 0.0000288 1199.1437 1339.3782 Otro Chiapas &lt; 20 1157.4273 87329.133 12 12 100 1200 3.47 14400 11 38.17 0.00012 0.1388913 0.0000 0.0000000 1157.4273 1157.4273 Otro Chiapas &gt;60 949.7062 43392.402 4 4 500 2000 3.47 8000 4 13.88 0.00004 0.0379882 0.0000 0.0000000 949.7062 949.7062 Otro Chihuahua [20,60] 1166.5268 135018.895 12 12 200 2400 5.24 28800 12 62.88 0.00012 0.1399832 0.0000 0.0000000 1166.5268 1166.5268 Otro Chihuahua &lt; 20 1189.1130 90206.049 10 10 100 1000 5.24 10000 7 36.68 0.00010 0.1189113 0.0000 0.0000000 1189.1130 1189.1130 Otro Chihuahua &gt;60 1417.6045 395701.236 6 6 500 3000 5.24 18000 6 31.44 0.00006 0.0850563 0.0000 0.0000000 1417.6045 1417.6045 Otro Ciudad de México [20,60] 1030.8943 93614.572 12 17 200 3400 2.43 57800 17 41.31 0.00017 0.1752520 2294.4748 0.0000663 937.0107 1124.7779 Otro Ciudad de México &lt; 20 824.6764 18754.642 3 3 100 300 2.43 900 3 7.29 0.00003 0.0247403 0.0000 0.0000000 824.6764 824.6764 Otro Ciudad de México &gt;60 1011.2821 46507.895 4 4 500 2000 2.43 8000 4 9.72 0.00004 0.0404513 0.0000 0.0000000 1011.2821 1011.2821 Otro Coahuila [20,60] 1153.8240 133184.968 18 21 200 4200 5.22 88200 21 109.62 0.00021 0.2423030 1057.0236 0.0000466 1090.1018 1217.5461 Otro Coahuila &lt; 20 1281.1743 26724.389 8 8 100 800 5.22 6400 6 31.32 0.00008 0.1024939 0.0000 0.0000000 1281.1743 1281.1743 Otro Coahuila &gt;60 1400.4608 360005.268 3 3 500 1500 5.22 4500 3 15.66 0.00003 0.0420138 0.0000 0.0000000 1400.4608 1400.4608 Otro Colima [20,60] 1203.9958 48933.453 8 23 200 4600 8.74 105800 18 157.32 0.00023 0.2769190 3989.1402 0.0002110 1080.2051 1327.7864 Otro Colima &lt; 20 1152.7899 70420.029 10 13 100 1300 8.74 16900 7 61.18 0.00013 0.1498627 1625.0776 0.0000275 1073.7793 1231.8005 Otro Colima &gt;60 1013.3670 14855.926 3 3 500 1500 8.74 4500 3 26.22 0.00003 0.0304010 0.0000 0.0000000 1013.3670 1013.3670 Otro Durango [20,60] 990.9619 90532.440 5 17 200 3400 3.46 57800 17 58.82 0.00017 0.1684635 12781.0504 0.0003694 769.3815 1212.5423 Otro Durango &lt; 20 1410.2571 210571.895 11 11 100 1100 3.46 12100 10 34.60 0.00011 0.1551283 0.0000 0.0000000 1410.2571 1410.2571 Otro Durango &gt;60 1706.5995 51537.321 4 4 500 2000 3.46 8000 4 13.84 0.00004 0.0682640 0.0000 0.0000000 1706.5995 1706.5995 Otro Estado de México [20,60] 1117.6846 121512.225 8 12 200 2400 5.14 28800 12 61.68 0.00012 0.1341221 5063.0094 0.0000729 978.2237 1257.1455 Otro Estado de México &lt; 20 1228.9336 164030.546 12 12 100 1200 5.14 14400 9 46.26 0.00012 0.1474720 0.0000 0.0000000 1228.9336 1228.9336 Otro Estado de México &gt;60 1393.3237 211705.207 5 5 500 2500 5.14 12500 5 25.70 0.00005 0.0696662 0.0000 0.0000000 1393.3237 1393.3237 Otro Guanajuato [20,60] 1211.0489 103149.713 11 11 200 2200 10.81 24200 8 86.48 0.00011 0.1332154 0.0000 0.0000000 1211.0489 1211.0489 Otro Guanajuato &lt; 20 1399.6811 242433.423 5 5 100 500 10.81 2500 3 32.43 0.00005 0.0699841 0.0000 0.0000000 1399.6811 1399.6811 Otro Guanajuato &gt;60 1235.4232 141243.737 3 3 500 1500 10.81 4500 3 32.43 0.00003 0.0370627 0.0000 0.0000000 1235.4232 1235.4232 Otro Guerrero [20,60] 1135.7179 129377.467 17 17 200 3400 0.49 57800 17 8.33 0.00017 0.1930720 0.0000 0.0000000 1135.7179 1135.7179 Otro Guerrero &lt; 20 1213.8758 86267.562 11 16 100 1600 0.49 25600 16 7.84 0.00016 0.1942201 2450.7830 0.0000627 1116.8470 1310.9046 Otro Guerrero &gt;60 1276.3713 264670.167 6 6 500 3000 0.49 18000 6 2.94 0.00006 0.0765823 0.0000 0.0000000 1276.3713 1276.3713 Otro Hidalgo [20,60] 1187.2688 89959.761 5 22 200 4400 25.66 96800 10 256.60 0.00022 0.2611991 13902.8721 0.0006729 956.1686 1418.3690 Otro Hidalgo &lt; 20 1279.9488 87426.438 8 13 100 1300 25.66 16900 5 128.30 0.00013 0.1663933 4203.1941 0.0000710 1152.8803 1407.0173 Otro Hidalgo &gt;60 1568.8472 5022.877 2 2 500 1000 25.66 2000 2 51.32 0.00002 0.0313769 0.0000 0.0000000 1568.8472 1568.8472 Otro Jalisco [20,60] 1178.5785 122876.524 8 16 200 3200 4.41 51200 16 70.56 0.00016 0.1885726 7679.7827 0.0001966 1006.8183 1350.3387 Otro Jalisco &lt; 20 1320.0498 173996.945 7 7 100 700 4.41 4900 6 26.46 0.00007 0.0924035 0.0000 0.0000000 1320.0498 1320.0498 Otro Jalisco &gt;60 1198.4388 92224.354 7 7 500 3500 4.41 24500 7 30.87 0.00007 0.0838907 0.0000 0.0000000 1198.4388 1198.4388 Otro Michoacán [20,60] 1136.4214 135045.156 13 18 200 3600 5.03 64800 18 90.54 0.00018 0.2045559 2885.5803 0.0000935 1031.1369 1241.7060 Otro Michoacán &lt; 20 1456.6225 171872.942 8 8 100 800 5.03 6400 6 30.18 0.00008 0.1165298 0.0000 0.0000000 1456.6225 1456.6225 Otro Michoacán &gt;60 1292.6526 29312.132 6 6 500 3000 5.03 18000 6 30.18 0.00006 0.0775592 0.0000 0.0000000 1292.6526 1292.6526 Otro Morelos [20,60] 1302.2474 103070.789 11 12 200 2400 2.35 28800 12 28.20 0.00012 0.1562697 780.8393 0.0000112 1247.4791 1357.0156 Otro Morelos &lt; 20 1397.6639 127386.632 9 11 100 1100 2.35 12100 11 25.85 0.00011 0.1537430 2573.4673 0.0000311 1298.2362 1497.0916 Otro Morelos &gt;60 1500.1497 219864.684 5 5 500 2500 2.35 12500 5 11.75 0.00005 0.0750075 0.0000 0.0000000 1500.1497 1500.1497 Otro Nayarit [20,60] 994.4397 93499.908 10 16 200 3200 5.44 51200 16 87.04 0.00016 0.1591104 3506.2466 0.0000898 878.3833 1110.4962 Otro Nayarit &lt; 20 1021.0080 94536.075 9 9 100 900 5.44 8100 7 38.08 0.00009 0.0918907 0.0000 0.0000000 1021.0080 1021.0080 Otro Nayarit &gt;60 1184.8444 31569.884 5 5 500 2500 5.44 12500 5 27.20 0.00005 0.0592422 0.0000 0.0000000 1184.8444 1184.8444 Otro Nuevo León [20,60] 1223.1507 132877.998 12 12 200 2400 9.19 28800 9 82.71 0.00012 0.1467781 0.0000 0.0000000 1223.1507 1223.1507 Otro Nuevo León &lt; 20 1372.9317 261615.765 9 9 100 900 9.19 8100 5 45.95 0.00009 0.1235639 0.0000 0.0000000 1372.9317 1372.9317 Otro Nuevo León &gt;60 1152.0131 7908.662 2 2 500 1000 9.19 2000 2 18.38 0.00002 0.0230403 0.0000 0.0000000 1152.0131 1152.0131 Otro Oaxaca [20,60] 1162.7544 144581.354 9 15 200 3000 4.80 45000 15 72.00 0.00015 0.1744132 6425.8380 0.0001446 1005.6411 1319.8677 Otro Oaxaca &lt; 20 1147.9198 68487.589 5 5 100 500 4.80 2500 4 19.20 0.00005 0.0573960 0.0000 0.0000000 1147.9198 1147.9198 Otro Oaxaca &gt;60 1138.1790 113042.454 9 9 500 4500 4.80 40500 9 43.20 0.00009 0.1024361 0.0000 0.0000000 1138.1790 1138.1790 Otro Puebla [20,60] 1060.4885 52973.460 12 14 200 2800 1.87 39200 14 26.18 0.00014 0.1484684 630.6364 0.0000124 1011.2690 1109.7081 Otro Puebla &lt; 20 1107.7481 79003.026 12 16 100 1600 1.87 25600 16 29.92 0.00016 0.1772397 1645.8964 0.0000421 1028.2331 1187.2632 Otro Puebla &gt;60 902.3671 67325.056 4 4 500 2000 1.87 8000 4 7.48 0.00004 0.0360947 0.0000 0.0000000 902.3671 902.3671 Otro Querétaro [20,60] 993.5867 58302.001 8 8 200 1600 1.99 12800 8 15.92 0.00008 0.0794869 0.0000 0.0000000 993.5867 993.5867 Otro Querétaro &lt; 20 1072.7032 50821.194 8 8 100 800 1.99 6400 8 15.92 0.00008 0.0858163 0.0000 0.0000000 1072.7032 1072.7032 Otro Querétaro &gt;60 1240.5791 55436.766 10 10 500 5000 1.99 50000 10 19.90 0.00010 0.1240579 0.0000 0.0000000 1240.5791 1240.5791 Otro Quintana Roo [20,60] 1053.2675 77931.393 14 16 200 3200 0.44 51200 16 7.04 0.00016 0.1685228 695.8160 0.0000178 1001.5669 1104.9681 Otro Quintana Roo &lt; 20 1374.7510 32431.258 7 10 100 1000 0.44 10000 10 4.40 0.00010 0.1374751 1389.9110 0.0000139 1301.6806 1447.8214 Otro Quintana Roo &gt;60 1044.5242 174719.858 4 4 500 2000 0.44 8000 4 1.76 0.00004 0.0417810 0.0000 0.0000000 1044.5242 1044.5242 Otro San Luis Potosí [20,60] 1045.9068 145157.376 12 12 200 2400 2.36 28800 12 28.32 0.00012 0.1255088 0.0000 0.0000000 1045.9068 1045.9068 Otro San Luis Potosí &lt; 20 1194.7173 167401.204 6 10 100 1000 2.36 10000 10 23.60 0.00010 0.1194717 11160.0803 0.0001116 987.6642 1401.7704 Otro San Luis Potosí &gt;60 1289.9168 18361.259 2 2 500 1000 2.36 2000 2 4.72 0.00002 0.0257983 0.0000 0.0000000 1289.9168 1289.9168 Otro Sinaloa [20,60] 1151.6349 90412.338 9 16 200 3200 3.79 51200 16 60.64 0.00016 0.1842616 4395.0442 0.0001125 1021.6988 1281.5709 Otro Sinaloa &lt; 20 1202.7303 152418.845 15 15 100 1500 3.79 22500 13 49.27 0.00015 0.1804096 0.0000 0.0000000 1202.7303 1202.7303 Otro Sinaloa &gt;60 1286.5936 111536.235 5 5 500 2500 3.79 12500 5 18.95 0.00005 0.0643297 0.0000 0.0000000 1286.5936 1286.5936 Otro Sonora [20,60] 1259.2915 165221.079 13 13 200 2600 7.76 33800 11 85.36 0.00013 0.1637079 0.0000 0.0000000 1259.2915 1259.2915 Otro Sonora &lt; 20 1042.4590 44264.919 13 13 100 1300 7.76 16900 8 62.08 0.00013 0.1355197 0.0000 0.0000000 1042.4590 1042.4590 Otro Sonora &gt;60 1102.7487 125961.459 4 5 500 2500 7.76 12500 5 38.80 0.00005 0.0551374 6298.0729 0.0000157 947.2052 1258.2922 Otro Tabasco [20,60] 1290.6955 23624.871 8 13 200 2600 4.59 33800 13 59.67 0.00013 0.1677904 1135.8111 0.0000192 1224.6412 1356.7499 Otro Tabasco &lt; 20 1395.8767 120227.601 6 6 100 600 4.59 3600 5 22.95 0.00006 0.0837526 0.0000 0.0000000 1395.8767 1395.8767 Otro Tabasco &gt;60 1042.9406 37449.781 4 4 500 2000 4.59 8000 4 18.36 0.00004 0.0417176 0.0000 0.0000000 1042.9406 1042.9406 Otro Tamaulipas [20,60] 1299.6833 294788.098 6 18 200 3600 6.52 64800 16 104.32 0.00018 0.2339430 32754.2331 0.0010612 944.9664 1654.4001 Otro Tamaulipas &lt; 20 1091.9465 122412.133 13 14 100 1400 6.52 19600 9 58.68 0.00014 0.1528725 672.5941 0.0000132 1041.1160 1142.7771 Otro Tamaulipas &gt;60 1253.1113 59013.632 2 2 500 1000 6.52 2000 2 13.04 0.00002 0.0250622 0.0000 0.0000000 1253.1113 1253.1113 Otro Tlaxcala [20,60] 1143.1954 74966.602 7 16 200 3200 4.60 51200 16 73.60 0.00016 0.1829113 6024.1019 0.0001542 991.0726 1295.3182 Otro Tlaxcala &lt; 20 1185.4372 137159.022 10 10 100 1000 4.60 10000 8 36.80 0.00010 0.1185437 0.0000 0.0000000 1185.4372 1185.4372 Otro Tlaxcala &gt;60 1130.8778 69284.546 3 3 500 1500 4.60 4500 3 13.80 0.00003 0.0339263 0.0000 0.0000000 1130.8778 1130.8778 Otro Veracruz [20,60] 1233.5080 172326.711 13 13 200 2600 16.65 33800 8 133.20 0.00013 0.1603560 0.0000 0.0000000 1233.5080 1233.5080 Otro Veracruz &lt; 20 1492.7512 242721.582 6 6 100 600 16.65 3600 3 49.95 0.00006 0.0895651 0.0000 0.0000000 1492.7512 1492.7512 Otro Veracruz &gt;60 1252.2549 142100.917 8 8 500 4000 16.65 32000 7 116.55 0.00008 0.1001804 0.0000 0.0000000 1252.2549 1252.2549 Otro Yucatán [20,60] 1378.3297 89575.764 14 18 200 3600 1.15 64800 18 20.70 0.00018 0.2480993 1421.8375 0.0000461 1304.4248 1452.2346 Otro Yucatán &lt; 20 1235.8338 114156.706 10 10 100 1000 1.15 10000 10 11.50 0.00010 0.1235834 0.0000 0.0000000 1235.8338 1235.8338 Otro Yucatán &gt;60 1055.5209 7548.192 4 4 500 2000 1.15 8000 4 4.60 0.00004 0.0422208 0.0000 0.0000000 1055.5209 1055.5209 Otro Zacatecas [20,60] 1308.0441 178838.247 11 15 200 3000 22.18 45000 8 177.44 0.00015 0.1962066 4335.4727 0.0000975 1178.9916 1437.0966 Otro Zacatecas &lt; 20 1402.5301 255927.588 12 12 100 1200 22.18 14400 5 110.90 0.00012 0.1683036 0.0000 0.0000000 1402.5301 1402.5301 Otro Zacatecas &gt;60 1260.5067 107021.370 6 6 500 3000 22.18 18000 5 110.90 0.00006 0.0756304 0.0000 0.0000000 1260.5067 1260.5067 "],
["referencias.html", "Referencias", " Referencias "],
["programación-en-r.html", "A Programación en R A.1 Algunas ventajas de R y cosas no tan padres A.2 Bienvenidx a R, Taking Off Again (sí, así se llama esta versión) A.3 Instalando cosas A.4 Instalación de RStudio A.5 Primeros pasos en R usando RStudio A.6 Cálculos numéricos A.7 Variables A.8 Observaciones sobre la aritmética de punto flotante A.9 Leer y almacenar variables en R A.10 Instalación de paquetes A.11 Comentarios adicionales sobre el formato", " A Programación en R Figure A.1: R es un programa chido de estadística. FIN. Una de las primeras cosas que necesitamos saber es que R (por más que sus más ávidos defensores digan lo contrario) no es para todo. Si tú ya conoces otro lenguaje (sea Stata, Excel, SAS, Python, Matlab, Julia, etc) sabrás utilizar muchas de sus opciones. Estoy seguro que, de conocer uno de estos, te será muchísimo más fácil seguir sacando promedios en tu lenguaje favorito que en R, realizar regresiones lineales es probablemente más sencillo en Stata mientras que las gráficas de barras para mí son más simples en Excel, Python excede en aplicaciones de inteligencia artificial mientras que Matlab es más veloz que R, Julia tiene muchas cosas de ecuaciones diferenciales que nadie más. Lo que probablemente no sea más sencillo de hacer en otro lenguaje es realizar análisis estadístico, gráficas de todo tipo y modelos de simulación. Para eso, R es, indiscutiblemente, una de las mejores opciones para quienes no conocen de programación7. Finalmente, uno de los consejos más importantes que te puedo dar es que este curso no te va a servir si no practicas. Igual que como pasa con los idiomas uno no aprende R en una semana sin practicarlo después. Mi sugerencia es que, a la vez que sigues estas notas comiences a trabajar un proyecto tuyo específico junto con el buscador de Internet de tu preferencia a la mano y empieces a usar R en él. Practica8. A.1 Algunas ventajas de R y cosas no tan padres A.1.1 Puntos a favor de R Todo el mundo lo usa. Quizá éste es el punto más a favor. Si mucha gente lo conoce y lo utiliza, hay más opciones de ayuda. Los sitios de StackOverflow en inglés y en español son excelentes para pedir apoyo en R; los grupos de usuarios de Google son otra fuente muy buena. Entre más gente usa el programa; es más fácil obtener ayuda porque seguro alguien más tuvo hace ya tiempo el mismo problema que tú. Todas las personas que trabajan en estadística publican sus métodos y su código en R (eso, claro, cuando publican sus métodos). Es raro encontrar un nuevo método estadístico en el mundo y que no se pueda usar, de alguna forma, en R. Dentro de los lenguajes de programación R es de los más sencillos. Quienes lo hicieron realmente se preocuparon por su público (de no especialistas) y en general desarrollan para él. R es gratis. Y en esta época de austeridad, cualquier ahorro es bueno. Que sea gratis no significa que no esté respaldado: existen versiones de R respaldadas por grandes compañías como Microsoft Todo lo que se hace en R es público. R no tiene métodos secretos ni es una caja negra. Todo lo que hace cada una de las funciones de R, cualquiera lo puede revisar, por completo. En R puedes hacer libros o notas ¡como este! donde guardes todo tu trabajo, reportes automatizados e incluso documentos interactivos para facilitar el análisis de datos. R puede hacer gráficas bonitas: Por supuesto, no todo es miel sobre hojuelas con R. Particularmente, algunos de los problemas con el lenguaje: Figure A.2: La curva de aprendizaje de R es más empinada pero después de un rato vale la pena La curva de aprendizaje es mucho más empinada que para otros programas estadísticos (como Stata, SAS o SPSS) ¡particularmente si es tu primera vez programando! La mayor parte de las personas que trabajan en R no son programadores de verdad. Gran parte del código que te puedes encontrar en el mundo real está escrito con prisa para salir del aprieto sin mucha planeación, con pocos comentarios, falta de control de versiones y pocas herramientas de revisión. ¡Internet está lleno de creaturas espantosas escritas en R! Figure A.3: R puede ser muy lento pero eso te da oportunidad de hacer otras cosas ;) . R de ninguna manera es veloz por lo que algunos programas (lo veremos en simulación) pueden ser extremada (y dolorosamente) lentos. A.2 Bienvenidx a R, Taking Off Again (sí, así se llama esta versión) R es un lenguaje de cómputo y un programa estadístico libre, gratuito, de programación funcional (¿qué es eso?), orientado a objetos (what??) que mutó a partir de otros dos lenguajes conocidos como Scheme y S9. El primero de estos fue desarrollado en el MIT por Sussman y Steele mientras que el segundo surgió en los laboratorios Bell10 creado por Becker, Wilks y Chambers. R nació en junio de 1995 a partir del trabajo de Ross Ihaka y Robert Gentleman11. Desde su creación, la mayor parte del desarrollo de R ha sido trabajo completamente voluntario de la Fundación R, del equipo de R Core y de miles de usuarios que han creado funciones específicas para R conocidas como paquetes (packages). Actualmente el repositorio más importante de R, CRAN, contiene más de 16000 paquetes con distintas funciones para hacer ¡lo que quieras! Como todo el trabajo en R es voluntario hace falta: Una homologación en los métodos. Puedes encontrar varias funciones que supuestamente hacen exactamente lo mismo (como es el caso de emojifont, fontemoji y emoGG para graficar usando emojis). Estandarizar la notación. Algunos paquetes como aquellos del tidyverse (veremos más adeltna) utilizan pipes (%&gt;%); estos sólo funcionan en el tidyverse pero no fuera del mismo. Sin embargo, también es una gran ventaja que sean los usuarios de R quienes guían su desarrollo. El lenguaje va mutando según peticiones de las personas que lo usan. Si hay algo que te gustaría R tuviera y aún no existe ¡lo puedes proponer! A.3 Instalando cosas A.3.1 Instalación de R Figure A.4: Oficialmente, la página de R es de las páginas más feas del mundo. ¡No te dejes llevar por las apariencias! A lo largo de estas notas estaré trabajando con: R version 4.0.2 (2020-06-22) Taking Off Again. La más reciente versión de R la puedes encontrar en CRAN. Para ello ve al sitio y selecciona tu plataforma. Nota usuarios de Mac En algunas Mac, al abir R, aparece el siguiente mensaje de advertencia: During startup - Warning messages: 1: Setting LC_CTYPE failed [...] para solucionarlo ve a Aplicaciones y abre Terminal. Copia y pega en ella el siguiente texto: defaults write org.R-project.R force.LANG en_US.UTF-8 Da enter, cierra la Terminal y reinicia R. En el caso de Windows da clic en Download R for Windows y luego en install R for the first time. Finalmente, ejecuta el instalable que aparece al dar click en Download R 4.0.2 for Windows . Para este curso pudiera ser que requirieras las herramientas de desarrollador Rtools. En el caso de Mac selecciona Download R for (Mac) OS X y luego elige R-4.0.2.pkg. En Mac puede que necesites instalar adicionalmente XQuartz (según tu versión de Mac). Si tu Mac es una versión suficientemente antigua, sigue las instrucciones específicas de CRAN. En el caso de Linux al elegir Download R for Linux tendrás la opción de buscar tu distribución específica. Al elegirla, aparecerán instrucciones para tu terminal de comandos; síguelas. En el caso de Linux, según los paquetes de R que elijamos instalar en la computadora requerirás instalar paquetería adicional para tu distribución de Linux. R te informará de la paquetería necesaria conforme la requiera. Si tienes problemas para instalar puedes usar RStudio Cloud. A.4 Instalación de RStudio Figure A.5: RStudio es una empresa que se dedica a hacer cosas para R. RStudio es una interfaz gráfica (IDE) para R. Puedes pensar a R como el Bloc de Notas y a RStudio como Word. El Bloc tiene todas las capacidades que necesitas para poder escribir; empero, es muchísimo mejor trabajar tus papers en Word. De la misma manera, R tiene todas las capacidades para hacer estadística pero un formato horrible y RStudio se ha convertido en la más popular forma de usar R. Por supuesto que no es la única; algunas alternativas son Atom con ide-r, Eclipse con StatET y RKWard. En general es posible seguir estas notas sin que tengas RStudio pero, si es tu primera vez programando, no lo recomiendo. Si ya tienes experiencia con lenguajes como Python, Javascript, Java ó alguno de los mil C que existen, no tendrás ningún problema usando el editor de tu preferencia. Para descargar RStudio ve a su página y da clic en Download RStudio. Baja tu pantalla hasta donde dice Installers for Supported Platforms y elige tu plataforma: Windows, Mac OS X ó tu sabor de Linux preferido. Una vez descargado el archivo, ábrelo y sigue las instrucciones que aparecen en pantalla. A.5 Primeros pasos en R usando RStudio Una vez hayas instalado R y RStudio, abre RStudio12. Te enfrentarás a una pantalla similar a esta: Figure A.6: La primera vez que abres RStudio Si tu RStudio tiene sólo 3 páneles, como en mi caso, ve a la esquina superior izquierda (signo de hoja+) y elige un nuevo R Script Figure A.7: Elige hoja+ para crear un nuevo archivo Tendrás, entonces, 4 páneles como se ve a continuación: Figure A.8: RStudio &lt;3 El primer panel (esquina inferior izquierda) es la Consola. Aquí es donde se ejecutan las acciones. Prueba escribir 2 + 3 en él y presiona enter. Aparece el resultado de la suma. Definitivamente, R es la calculadora que más trabajo cuesta instalar. Figure A.9: La consola de R es la calculadora más difícil de instalar que existe. El segundo panel (esquina superior izquierda) es el panel con el Script. Aquí se escribe el programa pero no se ejecuta. Prueba escribir 10 + 9. ¿Ves que no pasa nada? Lo que acabas de hacer es crear un programa que, cuando se ejecute, hará la suma de 10 + 9. ¡Qué programa más aburrido! Sin embargo, no todo está perdido: presiona CTRL+Enter (Cmd+Enter en Mac) al final de la línea o bien da clic en Run y verás que, en la consola, aparece la instrucción y el resultado de la misma. El Script es una excelente fuente para tener un historial de lo que estás haciendo. Figure A.10: El Script sirve para salvar las instrucciones en el orden en que las vas a ejecutar. El tercer panel contiene el ambiente. Aquí aparecerán las variables que vayamos creando. Por ahora, para poner un ejemplo, importaremos el archivo Example1.csv (con valores simulados) disponible en Github dando clic en Import Dataset y From Text (base). Selecciona el archivo y elige las opciones en la ventana de previsualización que hagan que se vea bien. Nota que una vez realizada la importación aparece en el panel derecho Example1. Al dar clic podrás ver la base de datos. Las bases de datos y variables que utilices durante tus análisis aparecerán en esa sección. Figure A.11: El Ambiente muestra las variables (incluyendo bases de datos) que estás utilizando en este momento. A diferencia de otros programas estadísticos (o sea Stata) en R es posible tener múltiples bases de datos abiertas a la vez. Para entender mejor lo que ocurre en el último de los páneles, lo mejor es trabajar con nuestra base. Escribe en la consola plot(Example1) . En el cuarto pánel aparecerá una gráfica. El cuarto de los páneles para nosotros tendrá esa utilidad: mostrará las gráficas que hagamos así como la ayuda. Para ver la ayuda para las instrucciones de R puedes escribir ?. Prueba teclear ?plot en la consola. El signo de interrogación es un help() que muestra las instrucciones para usar una función. Figure A.12: La gráfica que aparece de hacer un plot de la base de datos de ejemplo. Figure A.13: El cuarto panel muestra respectivamente las gráficas y la ayuda. Mi sugerencia personal es que escribas todo lo que haces en el Script y que sólo utilices la consola para verificar valores. De esta manera podrás almacenar todas las instrucciones ejecutadas y volver a ellas cuando se requieran. Por último te sugiero utilizar # gatos para comentar tu código. Así, el código anterior lo podrías ver en la consola como: Comenta. Comenta. Comenta, por favor. Tu ser del futuro que regrese a sus archivos de R un mes después de haberlos hecho te lo agradecerá (y tu profe también). Finalmente y como aclaración para estas notas, el código de R aparece como: Mientras que los resultados de evaluar en R se ven con #: [1] 5 Así, la evaluación con su resultado se ve de la siguiente forma: [1] 5 A.6 Cálculos numéricos R sirve como calculadora para las operaciones usuales. En él puedes hacer sumas, [1] 43 Figure A.14: Ada Lovelace (1815-1852), la primera en diseñar un algoritmo computacional ¡y sin tener computadoras! restas, [1] -1 multiplicaciones, [1] 56 divisiones, [1] 2 sacar logaritmos naturales \\(\\ln\\), [1] 4.60517 o bien logaritmos en cualquier base,13 [1] 2 también puedes elevar a una potencia (por ejemplo hacer \\(6^3\\)), [1] 216 calcular la exponencial \\(e\\), [1] 2.718282 o bien exponenciar cualquier variable \\(e^{-3}\\), [1] 0.04978707 también puedes usar el número \\(\\pi\\). [1] 3.141593 No olvides que R usa el orden de las operaciones de matemáticas. Siempre es de izquierda a derecha con las siguientes excepciones: Primero se evalúa lo que está entre paréntesis. En segundo lugar se calculan potencias. Lo tercero en evaluarse son multiplicaciones y divisiones. Finalmente, se realizan sumas y restas. Por ejemplo, en la siguiente ecuación \\[ 2 - 2 \\cdot \\frac{(3^4 - 9)}{(5 + 4)} \\] se resuelven primero los paréntesis \\((3^4 - 9) = 81 - 9 = 72\\) y \\((5 + 4) = 9\\); luego se resuelve la división: \\(\\frac{72}{9}=8\\), se multiplica por el \\(2\\): \\(2 \\cdot 8 = 16\\) y finalmente se hace la resta: \\(2-16 = -14\\). A.6.1 Ejercicio Determina, sin evaluar, los resultados de los siguientes segmentos de código: Evalúa para comprobar tu respuesta. A.6.2 Ejercicio Calcula el área y el perímetro de un círculo de radio 5. Recuerda que la fórmula del área es \\(\\pi \\cdot r^2\\) donde \\(r\\) es el radio; mientras que la del perímetro es: \\(\\pi \\cdot d\\) donde \\(d\\) es el díametro (= dos veces el radio). A.6.3 Respuestas Área = 78.5398163397448 Perímetro = 31.4159265358979 A.7 Variables R es un programa orientado a objetos; esto quiere decir que R almacena la información en un conjunto de variables que pueden tener diferentes clases y opera con ellos según su clase. Por ejemplo, un conjunto de caracteres, entre comillas, es un Character (R lo piensa como texto) [1] “Hola” Un número (por ejemplo 2 tiene clase numeric)14. Hay que tener mucho cuidado con combinar floats con Strings: [1] 6 Figure A.15: El algoritmo diseñado por Ada Lovelace. ## Error in 2 + &quot;4&quot;: non-numeric argument to binary operator Si lo piensas, este último error ¡tiene todo el sentido! no puedes sumar un número a un texto. ¿O qué significaría 'Felices' * 4 ? La magia de R comienza con que puedes almacenar valores en variables. Por ejemplo, podemos asignar un valor a una variable: Hay dos formas de asignar valores, una es con la flecha de asignación \\(\\leftarrow\\) y otra con el signo de igual: Nota que, cuando realizamos operaciones, la asignación es la última que se realiza: Los valores que fueron asignados en las variables, R los recuerda y es posible calcular con ellos: [1] 16 [1] 8 Podemos preguntarnos por el valor de las variables numéricas mediante los operadores == (sí, son dos iguales), != (que es un \\(\\neq\\)) &gt;, &gt;=, &lt;= y &lt;: [1] FALSE El operador de asignación también se puede utilizar al revés \\(2 \\rightarrow x\\) pero no lo hagas, por favor. Nota que no estamos asignando el valor de x: [1] 10 Podemos preguntarnos por diferencia: [1] TRUE Así como por mayores, menores incluyendo posibles igualdades (i.e. los casos \\(\\geq\\) y \\(\\leq\\)) [1] TRUE [1] TRUE [1] FALSE [1] TRUE En todos los casos los resultados han sido TRUE ó FALSE. La clase de variables que toma valores TRUE ó FALSE se conoce como booleana. Hay que tener mucho cuidado con ellas porque, puedes acabar con resultados muy extraños: [1] 101 [1] 0 Aquí puedes encontrar una lista de malas prácticas en computación a evitar. Finalmente, nota que es posible reescribir una variable y cambiar su valor: [1] 10 [1] 0.5 A.7.1 Ejercicios Determina el valor que imprime R en cada caso, sin que corras los siguientes pedazos de código. Después, verifica tu respuesta con R: A.7.2 NIVEL 3 Determina, sin correr el programa, qué regresa la consola en este caso Comprueba con la consola tus resultados; puede que encuentres respuestas poco intuitivas. A.8 Observaciones sobre la aritmética de punto flotante Si hiciste el penúltimo ejercicio (el cual, obviamente hiciste y comprobaste con la consola) podrás haber notado una trampa. Analicemos qué ocurre; quizá hicimos mal la suma [1] 0.3 [1] 0.3 Aparentemente no hay nada malo ¿qué rayos le pasa a R? La respuesta está en la aritmética de punto flotante. Podemos pedirle a R que nos muestre los primeros 100 dígitos de la suma 0.1 + 0.1 + 0.1: Figure A.16: Réplica de la Z3, la primer computadora con punto flotante (1941). [1] 0.3000000000000000444089 El comando options(digits = 22) especifica que R debe imprimir en la consola 22 dígitos. No más. ¡Ahí está el detalle! R no sabe sumar. En general, ningún programa de computadora sabe hacerlo. Veamos otros ejemplos: [1] 3.999999999999999555911 [1] 0.2999999999999999888978 [1] Inf El problema está en cómo las computadoras representan los números. Ellas escriben los números en binario. Por ejemplo, 230 lo representan como 11100110 mientras que el 7 es: 111. El problema de las computadoras radica en que éstas tienen una memoria finita por lo que números muy grandes como: \\(124765731467098372654176\\) la computadora hace lo mejor por representarlos eligiendo el más cercano: [1] 124765731467098377420800 Un error de punto flotante en la vida real ocasionó en los años noventa, la explosión del cohete Ariane 5. Moraleja: hay que tener cuidado y respeto al punto flotante. No olvides cambiar la cantidad de dígitos que deseas que imprima R en su consola de vuelta: El mismo problema ocurre con números decimales cuya representación binaria es periódica; por ejemplo el \\(\\frac{1}{10}\\) en binario se representa como \\(0.0001100110011\\overline{0011}\\dots\\). Como es el cuento de nunca acabar con dicho número, R lo trunca y almacena sólo los primeros dígitos de ahí que, cada vez que escribes 0.1, R en realidad almacene el 0.1000000000000000055511 que es casi lo mismo pero no es estrictamente igual. Hay que tener mucho cuidado con esta inexactitud de las computadoras (inexactitud estudiada por la rama de Análisis Numérico) pues puede generar varios resultados imprevistos. A.8.1 ¿Cómo checar un if? En general lo que hacen las computadoras para comparar valores es que verifican que, en valor absoluto, el error sea pequeño. Recuerda que el valor absoluto de \\(x\\), \\(|x|\\), regresa siempre el positivo: \\[ |4| = 4 \\qquad \\textrm{y} \\qquad |-8| = 8 \\] Para verificar que algo es más o menos \\(0.3\\) suele usarse el valor absoluto15 de la siguiente manera: [1] TRUE donde 1.e-6 es notación corta para 0.000001 (también escrito como \\(1\\times 10^{-6}\\)). La pregunta que nos estamos haciendo es que si el error entre sumar \\(0.1+0.1+0.1\\) y \\(0.3\\) es muy pequeño \\(&lt; 0.000001\\): \\[ | (0.1 + 0.1 + 0.1) - 0.3 | &lt; 0.000001 \\] A.9 Leer y almacenar variables en R Para terminar esta sección, aprenderemos cómo guardar variables en R. Para eso, el concepto de directorio es uno de los más relevantes. En general, en computación, el directorio se refiere a la dirección en tu computadora donde estás trabajando. Por ejemplo, si estás en una carpeta en tu escritorio de nombre “Ejercicios_R” probablemente tu directorio sea ‘~/Desktop/Ejercicios_R/’ (en Mac) o bien ‘~\\Desktop\\Ejercicios_R\\’ en Windows16. La forma de saber tu directorio (en general) es ir a la carpeta que te interesa y con clic derecho ver propiedades (o escribir ls en la terminal Unix). R tiene un directorio default que quién sabe dónde está (depende de tu instalación, generalmente está donde tu Usuario). Usualmente lo mejor es elegir un directorio para cada uno de los proyectos que hagas. Para ello si estás en RStudio puedes utilizar Shift+Ctrl+H (Shift+Cmd+H en Mac) o bien ir a Session &gt; Set Working Directory &gt; Choose Directory y elegir el directorio donde deseas trabajar tu proyecto. Pensando que elegiste el escritorio (Desktop en mi computadora) notarás que en la consola aparece el comando setwd(\"~/Desktop\") (o bien con ‘\\’ si eres Windows). Mi sugerencia es que copies ese comando en tu Script para que, la próxima vez que lo corras ya tengas preestablecido el directorio. Podemos verificar el directorio elegido con getwd(): En general es buena práctica en R establecer, hasta arriba del Script, el comando de directorio. Esto con el propósito de que, cuando compartas un archivo, la persona a quien le fue compartido el archivo pueda rápidamente elegir su propio directorio en su computadora. Probemos guardar unas variables en un archivo dentro de nuestro directorio. Para ello utilizaremos el comando save. Si vas a tu directorio, notarás que el archivo MisVariables.rda acaba de ser creado. De esta forma R puede almacenar objetos creados en R que sólo R puede leer (más adelante veremos cómo exportar bases de datos y gráficas). Observa que en tu ambiente (si estás en RStudio puedes verlas en el panel 3) deben aparecer las variables que hemos usado hasta ahora: [1] “sub10” “sub11” “sub12” [4] “sub13” “sub14” “sub15” [7] “var.x” “sub16” “sub17” [10] “nsim” “sub18” “sub19” [13] “imagen” “muestra” “sub20” [16] “sub21” “sub22” “sub23” [19] “base.costos” “sub24” “sub25” [22] “sub26” “sub27” “sub28” [25] “sub29” “varianza” “zalpha” [28] “total” “sub30” “sub31” [31] “sub32” “sub33” “sub34” [34] “proba” “sub35” “sub36” [37] “sub37” “sub38” “sub39” [40] “sub40” “confianza.alto” “sub41” [43] “sub42” “sub43” “sub44” [46] “sub45” “sub46” “sub47” [49] “raiz” “sumaAh” “sub48” [52] “sub49” “B” “sub50” [55] “sub51” “sub52” “sub53” [58] “sub54” “imagenes.muestreadas” “img” [61] “sub55” “sub56” “sub57” [64] “sub58” “sub59” “var.total” [67] “N” “sub60” “sub61” [70] “sub62” “sub63” “epsilon” [73] “sub64” “sub65” “sub66” [76] “sub67” “sub68” “sub69” [79] “promedios.muestra” “sub100” “ci” [82] “s2” “sub70” “sub71” [85] “sub72” “f” “sub73” [88] “sub74” “ci_low” “varianza.est” [91] “sub75” “i” “sub76” [94] “j” “sub77” “k” [97] “sub78” “p.val” “sub79” [100] “m” “n” “m.val” [103] “alpha” “sub80” “sub81” [106] “sub82” “sub83” “sub84” [109] “x” “sub85” “y” [112] “sub86” “z” “sub87” [115] “sub88” “sub89” “g.fun” [118] “sub90” “sub91” “sub92” [121] “sub1” “sub93” “sub2” [124] “sub94” “sub3” “sub95” [127] “sub4” “sub96” “sub5” [130] “sub97” “sub6” “sub98” [133] “sub7” “sub99” “sub8” [136] “sub9” “confianza.bajo” “ci_up” [139] “vr.name” “eps.error” “Ntotal” [142] “base.completa” “edad” “Bi” [145] “base.datos” “costo” “total.muestra” [148] “nombres” “Example1” “make.vr” [151] “datos” “intervalos.simulados” “dats” [154] “base.nh” “xbarra” Podemos probar sumar nuestras variables y todo funciona súper: [1] 300 Limpiemos el ambiente. El comando equivalente al clear all en R es un poco más complicado de memorizar: Ahora, si vuelves a ver el ambiente, éste estará vacío: ¡hemos limpiado el historial! Nota que si intentamos operar con las variables, R ya no las recuerda: ## Error in eval(expr, envir, enclos): object &#39;x&#39; not found Así como hay que lavarse las manos antes de comer, es buen hábito limpiar todas las variables del ambiente de R antes de usarlo. Podemos leer la base de datos usando load: [1] 300 Por último, es necesario resaltar la importancia del directorio. Para ello crea una nueva carpeta en tu escritorio de nombre Mi_curso_de_R. Mueve el archivo \"MisVariables.rda\" dentro de la carpeta. Borra todo e intenta leer de nuevo el archivo: ## Warning in readChar(con, 5L, useBytes = TRUE): cannot open compressed file ## &#39;MisVariables.rda&#39;, probable reason &#39;No such file or directory&#39; ## Error in readChar(con, 5L, useBytes = TRUE): cannot open the connection Este error es porque R sigue pensando que nuestro directorio es el escritorio y está buscando el archivo ahí sin hallarlo. Para encontrarlo hay que cambiar el directorio a través de RStudio (ya sea Ctrl+Shift+H o Session &gt;Set Working Directory &gt; Choose Directory) o bien a través de comandos en R: A.9.1 Ejercicio Responde a las siguientes preguntas: ¿Qué es el directorio y por qué es necesario establecerlo? Si R me da el error 'No such file or directory' ¿qué hice mal? En RStudio, ¿qué hace Session &gt; Restart R? ¿cuál es la diferencia con rm(list = ls())? ¿Qué hace el comando cat(\"\\014\")? (Ojo puede que no haga nada). Si funciona, ¿cuál es la diferencia con rm(list = ls()) y con Restart R? A.10 Instalación de paquetes Un paquete de R es un conjunto de funciones adicionales elaboradas por los usuarios, las cuales permiten hacer cosas adicionales en R. Para instalarlos requieres de una conexión a Internet (o bien puedes instalarlos a partir de un archivo, por ejemplo, mediante una USB). El comando de instalación es install.packages seguido del nombre del paquete. Por ejemplo (y por ocio) descarguemos el paquete beepr para hacer reproducir sonidos en la computadora17. Para ello: [...] * DONE (beepr) The downloaded source packages are in ‘/algun/lugar/downloaded_packages’ Esto significa que el paquete ha sido instalado. Nos interesa usar la función beep que emite un sonido (??beep para ver la ayuda). Si la llamamos así tal cual, nos da error: R es incapaz de hallar la función porque aún no le hemos dicho dónde se encuentra. Para ello podemos llamar al paquete mediante la función library y decirle a R que incluya las funciones que se encuentran dentro de beepr: El comando library le dice a R ¡hey, voy a usar unas funciones que creó alguien más y que están dentro del paquete beepr! De esta manera, al correr beep(3), R ya sabe dónde hallar la función y por eso no arroja error. A.10.1 Ejercicios NIVEL 1 Instala los paquetes tidyverse en R. De tidyverse haz lo necesario para que el siguiente bloque de código te arroje una gráfica: NIVEL 3 Instala el paquete devtools (para hacerlo probablemente necesites instalar más cosas en tu computadora; averigua cuáles) Usa devtools para instalar el paquete emoGG desde Github. Verifica que tu instalación fue correcta haciendo la siguiente gráfica: A.11 Comentarios adicionales sobre el formato Así como en el español existen reglas de gramática para ponernos todos de acuerdo y entendernos entre todos, en R también existen sugerencias a seguir para escribir tu código. Las sugerencias que aquí aparecen fueron adaptadas de las que utiliza el equipo de Google. No escribas líneas de más de 80 caracteres (si se salió de tu pantalla, mejor continúa en el siguiente renglón). Coloca espacios entre operadores +,*,/,-,&lt;-,=, &lt;, &lt;=, &gt;, &gt;=, == y usa paréntesis para agrupar: Intenta alinear la asignación de variables para legibilidad: Utiliza nombres que evoquen la variable que representas No utilices un nombre demasiado similar para cosas diferentes. Comenta: Figure A.17: Trad: Un periodista se acerca a un programador a preguntarle ¿qué hace que un código sea malo? -Sin comentarios. Siempre pon las llamadas a los paquetes y el directorio al inicio de tu archivo para que otro usuario sepa qué necesita. Código limpio y legible: es siempre preferible a código escrito con prisas : Figure A.18: Yo, leyendo mi código no comentado y con mala edición 6 meses después de haberlo hecho. Siempre escribe tu código pensando que alguien más (y ese alguien más puedes ser tú) va a leerlo. ¡No olvides comentar! Modelos de simulación más avanzados suelen hacerse en C, C++ o Fortran por su velocidad; empero, es necesario conocer más de programación.↩︎ La práctica hace al maestro↩︎ De ahí que se llame R porque la R es una mejor letra que la S (todos lo sabemos)\n-Atte. Rodrigo, el autor de este documento.↩︎ Mejor conocidos ahora como AT&amp;T, la compañía celular que nunca tiene señal.↩︎ Sus nombres empiezan con la letra R ¿coincidencia?↩︎ Si decidiste no instalar RStudio salta al final de esta sección.↩︎ Recuerda que un logaritmo base \\(a\\) te dice a qué potencia \\(b\\) tuve que elevar \\(a\\) para llegar a \\(b\\). Por ejemplo \\(\\log_{10}(100) = 2\\) te dice que para llegar al \\(100\\) tuviste que hacer \\(10^2\\).↩︎ Puede ser float, int, double pero no nos preocuparemos por eso.↩︎ En R el comando abs toma el valor absoluto.↩︎ Windows usa backslash. Y hay toda una historia detrás de ello↩︎ En los siguientes capítulos descargaremos paquetes más interesantes; pero no desprecies la utilidad de beepr yo lo he usado en múltiples ocasiones para que la computadora me avise que ya terminó de correr un código.↩︎ "],
["repaso-de-proba.html", "B Repaso de Proba B.1 Funciones indicadoras B.2 Conteo B.3 Espacios de probabilidad B.4 Probabilidad condicional B.5 Independencia B.6 Variables aleatorias y función de distribución (acumulada) B.7 Funciones de masa de probabilidad B.8 Funciones de densidad B.9 Teorema de cambio de variable unidimensional B.10 Probabilidad Multivariada B.11 Esperanza, varianza y covarianza B.12 Condicionamiento por otra variable aleatoria B.13 Funciones características B.14 Convergencias B.15 Ley de los grandes números B.16 Teorema del límite central", " B Repaso de Proba B.1 Funciones indicadoras Dado un conjunto \\(A\\) definimos la función indicadora de \\(A\\) como sigue: \\[ \\mathbb{I}_A (x)= \\begin{cases} 1 &amp; \\text{ si } x \\in A \\\\ 0 &amp; \\text{ si } x \\not\\in A \\end{cases} \\] La función indicadora cumple las siguientes propiedades: Sean \\(A,B\\) conjuntos; luego: \\(\\mathbb{I}_{A \\cap B}(x) = \\mathbb{I}_{A}(x) \\cdot \\mathbb{I}_{B}(x)\\) \\(\\mathbb{I}_{A \\cup B}(x) = \\mathbb{I}_{A}(x) + \\mathbb{I}_{B}(x) - \\mathbb{I}_{A}(x) \\cdot \\mathbb{I}_{B}(x)\\) \\(\\mathbb{E}_X[\\mathbb{I}_A(X)] = \\mathbb{P}(X\\in A)\\) Demostración: 1. Si \\(x\\in A \\cap B\\) pasa que \\(\\mathbb{I}_{A \\cap B}(x) = 1\\); además, por hipótesis \\(x\\in A\\) y \\(x \\in B\\) lo que implica que \\(\\mathbb{I}_{A}(x) = 1\\) y \\(\\mathbb{I}_{B}(x) = 1\\); en caso contrario \\(\\mathbb{I}_{A \\cap B}(x) = 1\\) y como no está en el conjunto al menos uno \\(\\mathbb{I}_{A}(x)\\) ó \\(\\mathbb{I}_{B}(x)\\) es cero. Esto concluye la prueba. 2. Demostración es similar 3. Para cualquier variable aleatoria \\(X\\), \\(\\mathbb{I}_{A}(X)\\) sólo toma dos valores: \\(0\\) si \\(X\\not\\in A\\) y \\(1\\) si \\(X\\in A\\). Luego: \\[ \\mathbb{E}_X[\\mathbb{I}_A(X)] = 1 \\cdot \\mathbb{P}(X\\in A) + 0 \\cdot \\mathbb{P}(X\\not\\in A) = \\mathbb{P}(X\\in A) \\] B.2 Conteo Intentemos resumir todas las formas de contar que tenemos con un ejemplo de Casella and Berger (2002). En la lotería de Nueva York se eligen \\(6\\) de \\(44\\) números para un ticket. ¿Cuántos boletos de lotería posibles hay? Veamos algunas formas posibles de solución18: Ordenado y sin reemplazo Si sólo importa el orden y una vez que sale un número no se vuelve a meter a los posibles entonces tenemos: \\[ \\frac{44!}{(44-6)!} \\] Ordenado y con reemplazo En cada uno de los \\(6\\) lugares hay \\(44\\) números posibles: \\[ 44^6 \\] Sin orden y sin reemplazo Esto es una combinación por lo que la forma de extraerlo es: \\[ \\binom{44}{6} \\] Sin orden y con reemplazo Para resolver este caso podemos usar la técnica de las barras y los puntos. Coloquemos barras y los huecos entre ellas representan cada uno de los \\(44\\) números. \\[\\begin{equation}\\nonumber |\\underbrace{\\_}_{1}|\\underbrace{\\_}_{2}|\\underbrace{\\_}_{3}|\\cdots |\\underbrace{\\_}_{n}| \\end{equation}\\] Coloquemos puntos (\\(\\circ\\)) donde estén los números seleccionados. Por ejemplo la siguiente representa la combinación \\(113555\\) \\[\\begin{equation}\\nonumber |\\underbrace{\\circ \\circ}_{1}|\\underbrace{\\_}_{2}|\\underbrace{\\circ}_{3}||\\underbrace{\\_}_{4}|\\underbrace{\\circ \\circ \\circ}_{5}|\\cdots |\\underbrace{\\_}_{n}| \\end{equation}\\] Tenemos entonces que el problema se reduce a colocar \\(n - 1= 43\\) barritas (son un total de \\(45\\) pero la primera y la última no deben cambiar de lugar) y \\(k = 6\\) círculos por tanto colocamos \\(49\\) elementos en total. De estos, nos interesa poner \\(6\\) por lo que tenemos: \\[ \\binom{44 + 6 - 1}{6} \\] formas distintas. Esto nos lleva a la tabla siguiente: Para obtener una muestra de tamaño \\(k\\) a partir de un conjunto de tamaño \\(n &gt; 0\\) éstas son las opciones: \\(\\quad \\text{Con Reemplazo}\\) \\(\\quad \\text{Sin Reemplazo}\\) \\(\\quad \\text{Con Orden}\\) \\(\\quad n^k\\) \\(\\quad (n)_k\\) \\(\\quad \\text{Sin Orden}\\) \\(\\quad \\binom{n+k-1}{k}\\) \\(\\quad \\binom{n}{k}\\) B.3 Espacios de probabilidad Los ingredientes para un modelo probabilístico son \\(3\\): Un conjunto \\(\\Omega\\) conocido como espacio muestral el cual es el conjunto de los resultados de interés. Por ejemplo, en el tiro de un dado \\(\\Omega = \\{1,2,3,4,5,6\\}\\), para el lanzamiento de una moneda \\(\\Omega = \\{\\text{Águila},\\text{Sol}\\}\\) o bien en seleccionar un número uniforme entre \\(0\\) y \\(1\\) tenemos que \\(\\Omega = [0,1]\\). Una colección \\(\\mathcal{F}\\) de subconjuntos de \\(\\Omega\\) conocida como sigma-álgebra o bien como espacio de eventos la cual cumple las siguientes características: \\(\\Omega \\in \\mathcal{F}\\) Si \\(A\\in\\mathcal{F}\\) entonces \\(A^C \\in \\mathcal{F}\\) Si \\(A_1, A_2, \\dots\\) es una colección finita ó numerable de elementos de \\(\\mathcal{F}\\) entonces \\(\\bigcup_{n} A_n \\in \\mathcal{F}\\) Generalmente identificamos a la \\(\\mathcal{F}\\) con el potencia para conjuntos \\(\\Omega\\) finitos; para casos infinitos el teorema de Vitali nos dice que las cosas son más complicadas. Una función \\(\\mathbb{P}:\\mathcal{F} \\to [0,1]\\) que cumple que: \\(\\mathbb{P}(\\Omega) = 1\\). \\(\\mathbb{P}(A) \\geq 0\\) para todo \\(A \\in \\mathcal{F}\\). Si \\(A_1, A_2, \\dots\\) es una colección finita ó numerable de conjuntos disjuntos (\\(A_i\\cap A_j = \\emptyset\\) para \\(i \\neq j\\)) entonces \\(\\mathbb{P}(\\bigcup_{n} A_n) = \\sum\\limits_{n}\\mathbb{P}(A_n)\\). Estos últimos tres puntos se conocen como Axiomas de Kolmogorov. Una vez armados con los axiomas podíamos empezar a probar cosas con ellos; por ejemplo: Sea \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) un espacio de probabilidad. Sea \\(A\\) evento de \\(\\mathcal{F}\\). Luego: \\[ \\mathbb{P}(A^C) = 1 - \\mathbb{P}(A). \\] Para verlo, podemos escribir \\(\\Omega = A\\cup A^C\\) de donde se sigue que: \\[ 1 = \\mathbb{P}(\\Omega) = \\mathbb{P}(A \\cup A^C) = \\mathbb{P}(A) + \\mathbb{P}(A^C); \\] si despejamos obtenemos el resultado deseado. También podemos probar, por ejemplo: \\[ \\mathbb{P}(A\\setminus B) = \\mathbb{P}(A) - \\mathbb{P}(A \\cap B) \\] si escribimos \\(A = (A\\setminus B) \\cup (A \\cap B)\\) de donde se sigue que: \\[ \\mathbb{P}(A) = \\mathbb{P}\\big((A\\setminus B) \\cup (A \\cap B) \\big) = \\mathbb{P}(A\\setminus B) + \\mathbb{P} (A \\cap B) \\] y despejamos para tener el resultado deseado. Una última cosa de importancia es tomar \\(A,B\\) eventos de \\(\\mathcal{F}\\). Luego: \\[ \\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B) \\] Para verlo, escribimos \\(A\\cup B\\) como \\(A\\cup B = (A\\setminus B)\\cup (A \\cap B)\\cup (B\\setminus A)\\) luego: \\[\\begin{equation}\\nonumber \\begin{aligned} \\mathbb{P}(A\\cup B) &amp; = \\mathbb{P}(A\\setminus B) + \\mathbb{P} (A \\cap B) + \\mathbb{P}(B\\setminus A) \\\\ &amp; = \\mathbb{P}(A) - \\mathbb{P}(A \\cap B) + \\mathbb{P} (A \\cap B) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B) \\\\ &amp; = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B) \\end{aligned} \\end{equation}\\] B.4 Probabilidad condicional Muchas veces la probabilidad cambia conforme obtenemos información extra. Por ejemplo, si consideramos los tiros de un dado \\(\\Omega = \\{1,2,3,4,5,6\\}\\) y se sabe que cayó par \\(B = \\{2,4,6 \\}\\), la probabilidad de obtener \\(2\\) ó \\(4\\) (el evento) \\(A = \\{ 2, 4\\}\\) cambia de probabilidad: \\[ \\mathbb{P}(A | B) = \\dfrac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)} \\] En particular hay dos teoremas principales con probabilidad condicional: la ley de probabilidad total que te permite reconstruirlas probabilidades originales a partir de las condicionales y el de Bayes. El teorema de Bayes puede deducirse a partir de un simple despeje pues notamos que: \\[ \\mathbb{P}(A | B) = \\dfrac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)} \\] y por otro lado: \\[ \\mathbb{P}(B | A) = \\dfrac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(A)} \\] Si despejamos del segundo, obtenemos: \\[ \\mathbb{P}(B | A)\\mathbb{P}(A) = \\mathbb{P}(A \\cap B) \\] Podemos sustituir la definición de intersección en \\(\\mathbb{P}(A|B)\\) y así obtener: \\[ \\mathbb{P}(A | B) = \\dfrac{\\mathbb{P}(B | A)\\mathbb{P}(A)}{\\mathbb{P}(B)} \\] Por otro lado, dada una partición \\(B_1, B_2, \\dots\\) finita o numerable de \\(\\Omega\\) podemos definir la probabilidad de \\(A\\) en términos de cada uno de los pedazos: \\[ \\mathbb{P}(A) = \\sum\\limits_{k} \\mathbb{P}(A | B_k) \\cdot \\mathbb{P}(B_k) \\] Esta identidad se sigue de que: \\[ \\mathbb{P}(A | B_k) = \\dfrac{\\mathbb{P}(A \\cap B_k)}{\\mathbb{P}(B_k)} \\] de donde podemos sustituir arriba y obtener: \\[ \\mathbb{P}(A) = \\sum\\limits_{k} \\dfrac{\\mathbb{P}(A \\cap B_k)}{\\mathbb{P}(B_k)} \\cdot \\mathbb{P}(B_k) = \\sum\\limits_{k} \\mathbb{P}(A \\cap B_k) = \\mathbb{P}\\big(A \\cap (\\bigcup_k B_k) \\big) = \\mathbb{P}\\big(A \\cap \\Omega \\big) \\] Tenemos entonces el teorema siguiente: Sean $B_1, B_2, $ eventos que forman una partición de \\(\\Omega\\); sea \\(A\\) un evento cualquiera; luego: \\[ \\mathbb{P}(A) = \\sum\\limits_{k} \\mathbb{P}(A | B_k) \\cdot \\mathbb{P}(B_k) \\] Usando probabilidad condicional podemos resolver problemas como el siguiente: Considera el conjunto \\(C = \\{1,2,\\dots, n\\}\\) para \\(n \\geq 2\\). Se extraen dos números \\(a\\) y \\(b\\) (primero el \\(a\\) y luego el \\(b\\)) con probabilidad uniforme sin reemplazo. Determina la probabilidad de que \\(a &gt; b\\). Podemos utilizar probabilidad condicional para representar el evento: \\[\\begin{equation}\\nonumber \\begin{aligned} \\mathbb{P}(a &gt; b) &amp; = \\sum\\limits_{k = 1}^{n} \\mathbb{P}(a &gt; b \\quad | \\quad a = k) \\mathbb{P}(a = k) \\end{aligned} \\end{equation}\\] Donde \\(\\mathbb{P}(a = k) = \\frac{1}{n}\\) para todos los \\(k\\) pues es uniforme (y es el primero en salir). Luego: \\[\\begin{equation}\\nonumber \\begin{aligned} \\mathbb{P}(a &gt; b) &amp; = \\sum\\limits_{k = 1}^{n} \\mathbb{P}(a &gt; b \\quad | \\quad a = k) \\mathbb{P}(a = k) \\\\ &amp; = \\dfrac{1}{n} \\sum\\limits_{k = 1}^{n} \\mathbb{P}(a &gt; b \\quad | \\quad a = k) \\\\ &amp; = \\dfrac{1}{n} \\sum\\limits_{k = 1}^{n} \\mathbb{P}(k &gt; b \\quad | \\quad a = k) \\\\ \\end{aligned} \\end{equation}\\] Notamos que cuando \\(k = 1\\) no hay forma de que \\(k &gt; b\\); cuando \\(k = 2\\) hay una única forma (que \\(b\\) valga \\(1\\)); cuando \\(k = 3\\) hay dos formas. En general para una \\(k\\) genérica hay \\(k-1\\) formas de seleccionar un \\(b\\) menor a \\(k\\) luego: \\[\\begin{equation}\\nonumber \\begin{aligned} \\mathbb{P}(a &gt; b) &amp; = \\dfrac{1}{n} \\sum\\limits_{k = 1}^{n} \\dfrac{k-1}{n} \\\\ &amp; = \\dfrac{1}{n^2} \\sum\\limits_{k = 1}^{n} k-1 \\\\ &amp; = \\dfrac{1}{n^2} \\sum\\limits_{k = 0}^{n-1} k \\\\ &amp; = \\dfrac{1}{n^2} \\dfrac{n(n-1)}{2} \\\\ &amp; = \\dfrac{n-1}{2n} \\end{aligned} \\end{equation}\\] B.5 Independencia Dos eventos \\(A,B\\) son independientes si: \\[ \\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\mathbb{P}(B) \\] Intuitivamente esto significa que saber \\(A\\) no me dice nada de \\(B\\) pues la independencia implica que: \\[ \\mathbb{P}(A | B) = \\mathbb{P}(A) \\] B.6 Variables aleatorias y función de distribución (acumulada) Para hablar de probabilidad uno de los ingredientes principales eran las variables aleatorias. Éstas son funciones (no son variables ni son aleatorias) de tal manera que su imagen inversa pertenece a la sigma-álgebra \\(\\mathcal{F}\\): Una función \\(X: \\Omega \\to \\mathbb{R}\\) es una variable aleatoria si: \\[ X^{-1}(A) = \\{ \\omega \\in \\Omega \\quad : \\quad X(\\omega) \\in A \\} \\in \\mathcal{F} \\] para todo \\(A\\subseteq\\textrm{Dom}_X\\) En general la pregunta \\(\\mathbb{P}(X \\in A)\\) la traducíamos a una pregunta sobre conjuntos: \\[ \\mathbb{P}(X \\in A) = \\mathbb{P}\\Big( \\{ \\omega \\in \\Omega \\quad : \\quad X(\\omega) \\in A \\} \\Big) \\] y esto nos permitía hablar de probabilidades. En particular, construíamos la función de distribución acumulada como sigue: Definimos la función de distribución acumulada de una variable aleatoria \\(X: \\Omega \\to \\mathbb{R}\\) como: \\[ F_X(x) = \\mathbb{P}(X \\leq x) \\] donde \\(X\\) es la variable aleatoria y \\(x\\in\\mathbb{R}\\) es un real. La función de distribución acumulada cumplía varias propiedades: \\(\\lim_{x \\to \\infty} F_X(x) = 1\\) \\(\\lim_{x \\to -\\infty} F_X(x) = 0\\) \\(F_X\\) es no decreciente. \\(F_X\\) es continua por la derecha. \\(F_X\\) tiene límites por la izquierda. Los puntos 4 y 5 se resumen diciendo que la función es càdlág. Tener la acumulada nos permitía calcular probabilidades de intervalos; por ejemplo: \\[ \\mathbb{P}(a &lt; X \\leq b) = F_X(b) - F_X(a) \\] o bien: \\[ \\mathbb{P}(X &lt; x) = \\lim_{z \\to x^-} F(z) \\] Las funciones de distribución acumulada más comunes se veían como en la imagen: Si una función de distribución acumulada \\(F_X\\) era continua entonces decíamos que la variable aleatoria asociada (\\(X\\)) es continua. En particular, la continuidad implica que: \\[ \\mathbb{P}(X = k) = 0 \\qquad \\forall k \\] B.7 Funciones de masa de probabilidad Si una variable aleatoria \\(X\\) tomaba una cantidad finita o numerable de valores decíamos que \\(X\\) es una variable aleatoria discreta. Dentro de las variables aleatorias discretas teníamos varios modelos. Una cosa importante de las variables aleatorias es la función de masa de probabilidad que se define como: Dada una variable aleatoria discreta \\(X\\) definimos la función de masa de probabilidad de \\(X\\) como la función \\(p:\\mathbb{R} \\to \\mathbb{R}\\) tal que: \\[ p(x) = \\mathbb{P}(X = x) \\] para todo \\(x \\in\\mathbb{R}\\). Algunos modelos importantes son: Sea \\(A = \\{ a_1, a_2, \\dots, a_n \\}\\) un conjunto finito de \\(n\\) elementos. Una variable aleatoria \\(X\\) tiene una distribución uniforme discreta si: \\[ \\mathbb{P}\\big( X = a_k \\big) = \\dfrac{1}{n} \\cdot \\mathbb{I}_{A}(a_k) \\qquad \\forall k \\in \\{ 1, 2, \\dots, n \\} \\] Un modelo particular salía de considerar el siguiente problema: Tenemos una moneda que cae Águila con probabilidad \\(p\\) y Sol con probabilidad \\((1-p)\\) (con \\(0 &lt; p &lt; 1\\)). Nos interesa saber cuál es la probabilidad de tener \\(k\\) Águilas en \\(n\\) tiros. Solución A fin de resolver este problema notamos que necesitamos acomodar las \\(k\\) águilas en los \\(n\\) tiros para ello hay \\(\\binom{n}{k}\\) formas de hacerlo; cada águila cae con probabilidad \\(p\\) y hay \\(k\\); como son independientes esto nos da \\(p^k\\); por otro lado hay \\(n-k\\) soles cada uno cayó con probabilidad \\((1-p)\\). Esta lógica da origen al modelo binomial: Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Binomial}(n,p)\\) si: \\[ \\mathbb{P}\\big(X = k \\big) = \\binom{n}{k} p^k (1-p)^{n-k} \\mathbb{I}_{\\{0,1,2,\\dots,n \\}}(k) \\] Una pregunta distinta que nos pudimos hacer fue: Tenemos una moneda que cae Águila con probabilidad \\(p\\) y Sol con probabilidad \\((1-p)\\) (con \\(0 &lt; p &lt; 1\\)). Arrojamos la moneda hasta obtener \\(r\\) Águilas y en ese momento nos detenemos. Determina la probabilidad de que se aviente la moneda \\(k\\) veces. Para ello notamos que la última Águila está fija por lo que sólo debemos poner las \\(r-1\\) Águilas en los \\(k-1\\) lugares restantes, \\(\\binom{k-1}{r-1}\\). Por otro lado, cada Águila tiene probabilidad \\(p\\) y como son \\(k\\) tiros independientes entonces tenemos \\(p^r\\); para los soles tenemos \\((1-p)^{k-r}\\). Esto nos genera el modelo Binomial Negativo: Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Binomial Negativa}(r,p)\\) si: \\[ \\mathbb{P}\\big(X = k \\big) = \\binom{k-1}{r-1} p^r (1-p)^{k-r} \\mathbb{I}_{\\{r, r+1, r+2, \\dots \\}}(k) \\] Finalmente, otro modelo que pudimos hacer con monedas es un caso específico del Binomial Negativo . Aquí la pregunta es, se tira una moneda que tiene probabilidad \\(p\\) de salir Águila hasta que se obtiene el águila. Contamos cuántos tiros ocurrieron hasta que ocurriera el primer Águila y la pregunta de interés es la probabilidad de haber realizado específicamente \\(k\\) tiros. Para ello necesitamos tener \\((k-1)\\) tiros que fueran sol: \\((1-p)^{k-1}\\) y un tiro que saliera águila \\(p\\). Esto nos genera el modelo geométrico: Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Geométrica}(p)\\) si: \\[ \\mathbb{P}\\big(X = k \\big) = (1-p)^k p \\cdot \\mathbb{I}_{\\mathbb{N}}(k). \\] Otro modelo de interés es el siguiente: Se tiene una población de tamaño \\(M\\) donde \\(N\\) individuos pertenecen al partido político AZUL y \\(M-N\\) pertenecen al VERDE Se toma una submuestra de tamaño \\(m\\). Determina la probabilidad de que haya \\(n\\) individuos del partido político AZUL. Para ello notamos que hay \\(\\binom{M}{m}\\) muestras totales. Por otro lado, necesitamos extraer de los \\(N\\) azules a una submuestra de \\(n\\): \\(\\binom{N}{n}\\); finalmente, de los \\(M\\) verdes necesitamos extraer una submuestra de \\(m\\), hay \\(\\binom{M-N}{m-n}\\) formas de hacerlo. Concluimos entonces con el modelo hipergeométrico: Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Hipergeométrica}(M,N,m)\\) si: \\[ \\mathbb{P}\\big(X = n \\big) = \\dfrac{\\binom{M-N}{m-n} \\binom{N}{n} }{\\binom{M}{m}} \\cdot \\mathbb{I}_{\\{0,1,\\dots, \\text{mín}\\{m, N\\} \\}} (n) \\] El modelo Poisson va a ser bastante útil. Para estudiarlo, consideremos un modelo. Vamos a pensar en un servidor de computación (piensa en una página de Internet) que recibe solicitudes de entrar a la página de manera independiente y aleatoria en un intervalo de tiempo entre \\(t = 0\\) y \\(t = 1\\). Como primera aproximación podemos dividir el intervalo en \\(n\\) pedazos cada uno de longitud \\(1/n\\) y asumir que, a fuerza, sólo una conexión se puede realizar en cada uno de esos pedazos. Finalmente, asumamos que la probabilidad \\(p\\) de que se haga una conexión es proporcional a la longitud del intervalo y sea \\(p = \\lambda / n\\). Con estas hipótesis, la probabilidad de tener \\(k\\) conexiones (\\(k\\) entero entre \\(0\\) y \\(n\\)) está dada por un modelo binomial: \\[\\begin{equation}\\nonumber \\begin{aligned} f_n(k) &amp; = \\binom{n}{k} \\Big( \\frac{\\lambda}{n} \\Big)^k \\Big(1 - \\frac{\\lambda}{n} \\Big)^k \\ &amp; = \\frac{\\lambda^k}{k!} \\Big( 1 - \\frac{\\lambda}{n})^n \\frac{n!}{n^k(n-k)!} \\Big( 1 - \\frac{\\lambda}{n})^{-k} \\end{aligned} \\end{equation}\\] de donde concluimos que si continuamos partiendo el intervalo en pedazos cada vez más pequeños obtenemos: \\[\\begin{equation}\\nonumber \\begin{aligned} \\lim_{n \\to \\infty} f_n(k) &amp; = \\frac{e^{-\\lambda} \\lambda^k}{k!} \\end{aligned} \\end{equation}\\] Esto resulta en el modelo Poisson: Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Poisson}(\\lambda)\\) si: \\[ \\mathbb{P}\\big(X = k \\big) = \\dfrac{\\lambda^k e^{-\\lambda}}{k!} \\mathbb{I}_{\\mathbb{N}\\cup \\{ 0 \\}}(k) \\] B.8 Funciones de densidad Por construcción, las variables aleatorias continuas no tienen una función de masa de probabilidad (recuerda que \\(\\mathbb{P}(X = k) = 0\\) si \\(X\\) es continua para todo \\(k\\)). Sin embargo, es posible definir, si \\(F_X\\) es diferenciable algo similar, la función de densidad. Para una variable aleatoria \\(X\\) con función de distribución acumulada \\(F_X\\) diferenciable, definimos la función de densidad como: \\[ f_X(x) = \\dfrac{d}{dx} F_X(x) \\] Notamos que una función de densidad no es una probabilidad y no necesariamente sigue las mismas reglas; lo único que se requiere es: \\(f_X(x) \\geq 0\\) para toda \\(x\\). \\(\\int\\limits_{-\\infty}^{\\infty} f_X(x) dx = 1\\). La primer función de densidad es la que a un intervalo \\([a,b]\\) (ya sea abierto, cerrado o como sea) asigna a cada subintervalo una probabilidad proporcional a su longitud. Éste es el modelo uniforme: Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Uniforme}(a,b)\\) si: \\[ f_X(x) = \\dfrac{1}{b-a}\\mathbb{I}_{(a,b)}(x) \\] Una generalización del modelo uniforme es el beta (eventualmente veremos de dónde sale): Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Beta}(\\alpha,\\beta)\\) si: \\[ f_X(x) = \\dfrac{x^{\\alpha - 1}(1-x)^{\\beta - 1}}{B(\\alpha, \\beta)}\\mathbb{I}_{(0,1)}(x) \\] donde \\[ B(\\alpha, \\beta) = \\dfrac{\\Gamma (\\alpha) \\Gamma (\\beta)}{\\Gamma (\\alpha + \\beta)} \\] Podemos deducir el modelo exponencial a partir de la descripción del Poisson. Volvamos al mismo problema del \\(\\textrm{Poisson}(\\lambda)\\) donde hay computadoras conectándose a un servidor. Sea \\(W\\) la variable aleatoria que denota el tiempo de espera hasta el primer evento. Analicemos su distribución acumulada; notamos que \\[ F_W(w) = \\mathbb{P}(W \\leq w) = 1 - \\mathbb{P}(W &gt; w) \\] Ahora, para que \\(W &gt; w\\) eso significa que ningún evento tuvo que haber ocurrido en los primeros \\(w\\) minutos (horas, lo que sea la unidad de tiempo). Y ese evento es equivalente a que nuestra variable aleatoria Poisson (tasa \\(\\lambda w\\))19 no tenga ningún arribo: \\[ \\mathbb{P}(X = 0) = \\dfrac{(\\lambda w)^0 e^{-\\lambda w}}{0!} = e^{-\\lambda} \\] De donde se obtiene la función de distribución acumulada: \\[ F_W(w) = 1 - e^{-\\lambda w} \\] De donde, al derivar respecto a \\(w\\), se obtiene el modelo exponencial: Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Exponencial}(\\lambda)\\) si: \\[ f_X(x) = \\lambda e^{-\\lambda x} \\mathbb{I}_{(0,\\infty)}(x) \\] Para deducir la distribución gamma, vamos a preguntarnos por exactamente el mismo proceso pero esta vez, en lugar de preguntarnos por el tiempo para la primer conexión nos preguntaremos por el tiempo para la \\(\\alpha\\)-ésima conexión. Para ello, sea \\(W_{\\alpha}\\) el tiempo hasta la \\(\\alpha\\)-ésima conexión. Usamos el mismo truco del complemento que la vez pasada: \\[ F_{W_{\\alpha}}(w) = \\mathbb{P}(W_{\\alpha} \\leq w) = 1 - \\mathbb{P}(W_{\\alpha} &gt; w) \\] Y notamos que para que \\(W_{\\alpha} &gt; w\\) entonces a lo más debieron haber \\(\\alpha-1\\) conexiones. Podemos reescribir: \\[ F_{W_{\\alpha}}(w) = 1 - \\mathbb{P}(W_{\\alpha} &gt; w) = 1 - \\sum\\limits_{k = 0}^{\\alpha - 1} \\dfrac{(\\lambda w)^k e^{-\\lambda w}}{k!} = 1 - e^{- \\lambda w} - \\sum\\limits_{k = 1}^{\\alpha - 1} \\dfrac{(\\lambda w)^k e^{-\\lambda w}}{k!} \\] Derivamos: \\[\\begin{equation}\\nonumber \\begin{aligned} \\dfrac{d}{dw}F_{W_{\\alpha}}(w) &amp; = -\\lambda e^{- \\lambda w} - \\sum\\limits_{k = 1}^{\\alpha - 1} \\dfrac{k \\lambda (\\lambda w)^{k-1} e^{-\\lambda w} - \\lambda (\\lambda w)^k e^{-\\lambda w}}{k!} \\ &amp; = -\\lambda e^{- \\lambda w} - \\lambda e^{- \\lambda w} \\sum\\limits_{k = 1}^{\\alpha - 1} \\underbrace{\\dfrac{(\\lambda w)^{k-1}}{(k-1)!} - \\dfrac{(\\lambda w)^k }{k!}}_{\\text{Telescópica}} \\ &amp; = -\\lambda e^{- \\lambda w} + \\lambda e^{- \\lambda w} \\Bigg( \\dfrac{(\\lambda w)^{\\alpha - 1} }{(\\alpha - 1)!} - 1 \\Bigg) \\ &amp; = \\lambda e^{- \\lambda w} \\dfrac{(\\lambda w)^{\\alpha - 1} }{(\\alpha - 1)!} \\ &amp; = \\dfrac{\\beta^{\\alpha} }{\\Gamma (\\alpha)} w^{\\alpha - 1} e^{- \\frac{w}{\\beta}} \\ \\end{aligned} \\end{equation}\\] donde tomamos \\(\\beta = \\frac{1}{\\lambda}\\). Esto sugiere el modelo gamma: Una variable aleatoria \\(W\\) tiene una distribución \\(\\text{Gamma}(\\alpha,\\beta)\\) si: \\[ f_W(w) = \\dfrac{\\beta^{\\alpha} }{\\Gamma (\\alpha)} w^{\\alpha - 1} e^{- \\frac{w}{\\beta}} \\mathbb{I}_{(0,\\infty)} \\] para \\(\\alpha,\\beta &gt; 0\\). Para deducir el modelo normal consideremos lo siguiente. Pensemos que estamos midiendo la posición de las estrellas en el cielo. Para ello hay dos formas. Bajo coordenadas cartesianas \\((x,y)\\) pensemos que el error de medición es independiente; es decir, si \\(f(x,y)\\) es la densidad de los errores entonces: \\[ \\rho (x,y) = f(x) f(y) \\] Por otro lado, asumamos que existe también una representación en coordenadas polares de la posición de la estrella: \\[ g (r, \\theta) = g(r) \\] donde el error de medición depende sólo del radio (no del ángulo). Notamos entonces que: \\[ f(x) f(y) = g\\Big( \\sqrt{x^2 + y^2} \\Big) \\] Si tomamos \\(y = 0\\) tenemos que \\(f(x) f(0) = g(x)\\) (asumo \\(x &gt; 0\\); los otros casos son similares). Podemos entonces sustituir: \\[ \\dfrac{f(x) f(y)}{f(0)^2} = \\dfrac{f\\Big( \\sqrt{x^2 + y^2} \\Big) }{f(0)} \\] Tomamos logaritmo: \\[ \\ln \\dfrac{f(x)}{f(0)} + \\ln \\dfrac{f(y)}{f(0)} = \\ln \\dfrac{f\\Big( \\sqrt{x^2 + y^2} \\Big) }{f(0)} \\] Notamos que una solución es que: \\[ \\ln \\dfrac{f(x)}{f(0)} = \\alpha x^2 \\] de donde despejamos y obtenemos: \\[ f(x) = \\frac{1}{f(0)} e^{\\alpha x^2} \\] Finalmente sabemos que debe integrar a \\(1\\) y por tanto esto fuerza a \\(\\alpha\\) a ser negativo. En particular tomaremos \\(\\alpha = -\\frac{1}{2}\\) \\[ f(x) = \\frac{1}{f(0)} e^{-\\frac{1}{2} x^2} \\] Y para que integre a \\(1\\):s \\[ f(x) = \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{1}{2} x^2} \\] Por último, notamos que si \\(Z\\sim \\textrm{Normal}(0,1)\\) entonces \\(X = \\sigma Z + \\mu\\) tiene la densidad dada por20: \\[ f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{1}{2\\sigma^2} (x - \\mu)^2} \\] Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Normal}(\\mu,\\sigma)\\) si: \\[ f_X(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}} \\] B.9 Teorema de cambio de variable unidimensional Supongamos que tenemos una variable aleatoria \\(X\\) y nos interesa ver cómo se ve la \\(X\\) después de aplicarle una función \\(\\phi\\). Por ejemplo, si \\(X\\sim\\textrm{Normal}(0,1)\\) la función de densidad de \\(e^X\\) está dada por: \\[ f_X(x) = \\dfrac{1}{x \\sqrt{2 \\pi \\sigma^2}}e^{-(\\ln(x) - \\mu)^2/2\\sigma^2} \\mathbb{I}_{(0,\\infty)}(x). \\] Lo cual cambia mucho la forma de la distribución: La pregunta es, cómo obtener la función de densidad de \\(X\\) si se conoce la función \\(\\phi\\); el teorema de cambio de variable nos da una respuesta cuando \\(\\phi\\) es monótona estrictamente creciente o bien estrictamente decreciente y diferenciable. Sea \\(X\\) una variable aleatoria continua y \\(\\phi\\) una función estrictamente creciente ó estrictamente decreciente y diferenciable. Entonces: \\[ f_{\\phi(X)}(t) = f_X( \\phi^{-1}(t) ) \\cdot \\left| \\dfrac{d}{dt} \\phi^{-1}(t) \\right| \\] DEM: Caso estrictamente decreciente Como \\(\\phi\\) es estrictamente decreciente es invertible y por tanto: \\[\\begin{equation}\\nonumber \\begin{aligned} F_{\\phi(X)}(t) &amp; = \\mathbb{P}(\\phi(X) \\leq t) \\\\ &amp; = \\mathbb{P}(X \\geq \\phi^{-1}(t) ) \\\\ &amp; = 1 - \\mathbb{P}(X \\leq \\phi^{-1}(t) ) \\\\ &amp; = 1 - F_X( \\phi^{-1}(t) ) \\end{aligned} \\end{equation}\\] luego derivamos respecto a \\(t\\): \\[\\begin{equation}\\nonumber \\begin{aligned} f_{\\phi(X)}(t) &amp; = \\dfrac{d}{dt} F_{\\phi(X)}(t) \\\\ &amp; = - \\dfrac{d}{dt} F_X( \\phi^{-1}(t) ) \\\\ &amp; = - f_X( \\phi^{-1}(t) ) \\cdot \\dfrac{d}{dt} \\phi^{-1}(t) \\\\ &amp; = f_X( \\phi^{-1}(t) ) \\cdot \\left| \\dfrac{d}{dt} \\phi^{-1}(t) \\right| \\end{aligned} \\end{equation}\\] Donde el valor absoluto sale de que \\(\\phi^{-1}(t) &lt; 0\\) por ser estrictamente decreciente la \\(\\phi\\). B.10 Probabilidad Multivariada De la misma manera que hablamos de una sola variable aleatoria podemos hablar de muchas como múltiples funciones de \\(\\Omega \\in \\mathbb{R}\\). Para una colección finita \\(\\{ X_i \\}_{i = 1}^n\\) de variables aleatorias podemos hablar de su función de distribución acumulada conjunta como: \\[ F_{\\vec{X}}(x_1, x_2, \\dots, x_n) = \\mathbb{P}\\big( X_1 \\leq x_1, X_2 \\leq x_2, \\dots, X_n \\leq x_n) \\] donde suponemos que \\(\\vec{X} = (X_1, X_2, \\dots, X_n)^T\\) es un vector aleatorio cuyas entradas son las variables de la colección anterior. En el caso de que las \\(n\\) variables sean discretas la función de masa conjunta está dada por: \\[ p_{\\vec{X}}(x_1, x_2, \\dots, x_n) = \\mathbb{P}(X_1 = x_1, X_2 = x_2, \\dots, X_n = x_n) \\] En el caso de que sean continuas (\\(F_{\\vec{X}}\\) diferenciable en sus \\(n\\) entradas) entonces la densidad está dada por: \\[ f_{\\vec{X}}(x_1, x_2, \\dots, x_n) = \\dfrac{\\partial^n}{\\partial x_1 \\partial x_2 \\dots \\partial x_n}F_{\\vec{X}}\\Bigg|_{(x_1, x_2, \\dots, x_n)} \\] En general la función de probabilidad conjunta siempre va a esta dada por: \\[ \\mathbb{P}(X_1 \\in A_1, X_2 \\in A_2, \\dots, X_n \\in A_n) = \\mathbb{P}\\Big(\\{ \\omega \\in \\Omega : X_1(\\omega) \\in A_1 \\text{ y } X_2(\\omega) \\in A_2 \\text{ y } \\dots \\text{ y } X_n(\\omega) \\in A_n \\}\\Big) \\] para \\(A_1, A_2, \\dots, A_n\\) medibles (bajo \\(X_1, X_2, \\dots, X_n\\) respectivamente). Dos variables aleatorias \\(X_i\\) y \\(X_j\\) (\\(i \\neq j\\)) son independientes si: \\[ \\mathbb{P}(X_i \\in A, X_i \\in B) = \\mathbb{P}(X_i \\in A) \\cdot \\mathbb{P}(X_j \\in B) \\] para \\(A,B\\) medibles. Una colección \\(\\{ X_i \\}_{i}\\) de variables aleatorias es completamente independiente si para cualquier subcolección finita \\(\\{ X_{i_k} \\}_{i_k}\\) se tiene que: \\[ \\mathbb{P}(X_{i_1} \\in A_{i_1}, X_{i_2} \\in A_{i_2}, \\dots, X_{i_n} \\in A_{i_n} ) = \\prod_{k = 1}^n \\mathbb{P}(X_{i_k} \\in A) \\] en el contexto de estas notas, a menos que se indique lo contrario, las variables aleatorias que utilicemos serán completamente independientes. Un aspecto interesante de la independencia es que permite partir las funciones de masa, densidad y distribución acumulada en dos funciones independientes. Así, si \\(X,Y\\) son independientes con masa conjunta \\(p\\): \\[ p_{X,Y}(x,y) = \\mathbb{P}(X = x, Y = y) = \\mathbb{P}(X = x)\\cdot\\mathbb{P}(Y = y) = p_X(x)\\cdot p_Y(y) \\] El resultado se mantiene para distribuciones: \\[ F_{X,Y}(x,y) = \\mathbb{P}(X \\leq x, Y \\leq y) = \\mathbb{P}(X \\leq x)\\cdot\\mathbb{P}(Y \\leq y) = F_X(x)\\cdot F_Y(y) \\] y si derivamos (en caso de \\(F\\) diferenciable), se mantiene para densidades: \\[ f_{X,Y}(x,y) = \\dfrac{\\partial^2}{\\partial x\\partial y} F_{X,Y}\\Big|_{(x,y)} = \\dfrac{\\partial^2}{\\partial x\\partial y} F_X(x)\\cdot F_Y(y)\\Big|_{(x,y)} = f_X(x) f_Y(y) \\] B.11 Esperanza, varianza y covarianza Para una función medible \\(g\\) de una variable aleatoria \\(X\\) definimos su valor esperado (si existe) como: \\[ \\mathbb{E}\\big[g(X)\\big] = \\begin{cases} \\sum\\limits_{x \\in \\text{Supp}(X)} g(x) \\cdot \\mathbb{P}(X = x) &amp; \\text{ si } X \\text{ discreta.} \\\\ \\int\\limits_{-\\infty}^{\\infty} g(x) \\cdot f_X(x) dx&amp; \\text{ si } X \\text{ continua} \\end{cases} \\] donde \\(f_X\\) es la densidad de \\(X\\) en el caso continuo y \\(\\text{Supp}(X)\\) es el conjunto imagen de \\(X\\) (el soporte): \\[ \\text{Supp}(X) = \\{ x : X(\\omega) = x \\text{ para } \\omega \\in \\Omega \\} \\] En el caso de conjuntos finitos de variables aleatorias la definción es similar: Para una función \\(g:\\mathbb{R}^n \\to \\mathbb{R}\\) multivariada de \\(n\\) variables aleatorias (sobre los reales) \\(X_1, X_2, \\dots, X_n\\) definimos su valor esperado (si existe y sin pérdida de generalidad suponiendo las primeras \\(j\\) son discretas y las últimas \\(n - (j + 1)\\) continuas) como: \\[\\begin{equation}\\nonumber \\begin{aligned} \\mathbb{E}\\big[ g(X_1, X_2, \\dots, X_n) \\big] = \\\\ &amp; \\int_{-\\infty}^{\\infty} \\dots \\int_{-\\infty}^{\\infty} \\sum_{x_j \\in \\text{Supp}(X_j)} \\dots \\sum_{x_1 \\in \\text{Supp}(X_1)} g(x_1, x_2, \\dots, x_n) p(x_1) \\dots p(x_j) f_{X_{j+1}}(x_{j+1}) \\dots f_{X_{n}}(x_{n}) dx_{j+1} \\dots dx_{n} \\end{aligned} \\end{equation}\\] donde \\(p(x_j)\\) es la masa de \\(X_j\\) (es decir \\(p(x_j) = \\mathbb{P}(X_j = x_j)\\). En el caso particular de dos variables aleatorias \\(X_1\\) y \\(X_2\\) podemos escribir la expresión de manera más sencilla: \\[\\begin{equation}\\nonumber \\begin{aligned} \\mathbb{E}\\big[ &amp; g(X_1, X_2) \\big] = \\begin{cases} \\int\\limits_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(x_1, x_2) f_{X_1}(x_1) f_{X_2}(x_2) dx_1 dx_2 &amp; \\text{ ambas continuas,} \\\\ \\\\ \\sum\\limits_{x \\in \\text{Supp}(X_1)} \\sum\\limits_{x \\in \\text{Supp}(X_2)} g(x_1, x_2) p(x_1) p(x_2) &amp; \\text{ ambas discretas,} \\\\ \\\\ \\int_{-\\infty}^{\\infty} \\sum\\limits_{x \\in \\text{Supp}(X_1)} g(x_1, x_2) p(x_1) f(x_2) dx_2 &amp; X_1 \\text{ discreta, } X_2 \\text{ continua.} \\\\ \\end{cases} \\end{aligned} \\end{equation}\\] En particular, en el espacio de las variables aleatorias definimos un producto interno, la covarianza la cual está dada por: \\[ \\textrm{Cov}(X_1, X_2) = \\mathbb{E}\\Big[ \\big(X_1 - \\mathbb{E}[X_1]\\big) \\cdot \\big(X_2 - \\mathbb{E}[X_2]\\big) \\Big] \\] La varianza es un caso particular de la covarianza: cuando \\(X_1 = X_2\\): \\[ \\textrm{Cov}(X_1, X_1) = \\mathbb{E}\\Big[ \\big(X_1 - \\mathbb{E}[X_1]\\big)^2 \\Big] \\] B.11.1 Propiedades de valor esperado, varianza y covarianza El valor esperado al ser representable mediante sumas ó integrales cumple todas las propiedades de las sumas (resp integrales) en particular la linealidad: \\[ \\mathbb{E}\\Big[ a X + Y\\Big] = a \\mathbb{E}[X] + \\mathbb{E}[Y] \\] La demostración se hace exactamente igual en el caso de variables discretas, continuas (ó mezcla de una y una). Aquí muestro la de continuas con densidades \\(f_X\\) y \\(f_Y\\): \\[\\begin{equation} \\begin{aligned} \\mathbb{E}\\Big[ a X + Y\\Big] &amp; = \\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\infty} (a x + y) f_{X,Y}(x,y) dx dy \\\\ &amp; = a \\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\infty} x f_{X,Y}(x,y) dx dy + \\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\infty} y f_{X,Y}(x,y) dx dy \\\\ &amp; = a \\Big[ \\int\\limits_{-\\infty}^{\\infty} x f_X(x) dx \\Big] + \\int\\limits_{-\\infty}^{\\infty} y f_Y(y) dy \\\\ &amp; = a \\mathbb{E}[X] + \\mathbb{E}[Y] \\end{aligned} \\end{equation}\\] Otro resultado importante es que si dos variables aleatorias \\(X,Y\\) son independientes entonces el valor esperado del producto se parte: \\[ \\mathbb{E}[XY] = \\mathbb{E}[X] \\cdot \\mathbb{E}[Y] \\] La demostración se hace de manera idéntica en todos los casos. Aquí mostramos el caso de \\(X,Y\\) discretas: \\[\\begin{equation} \\begin{aligned} \\mathbb{E}\\Big[XY\\Big] &amp; = \\sum\\limits_{y \\in \\text{Sup}(Y)} \\sum\\limits_{x \\in \\text{Sup}(X)} xy \\mathbb{P}(X = x, Y = y) \\\\ &amp; = \\sum\\limits_{y \\in \\text{Sup}(Y)} \\sum\\limits_{x \\in \\text{Sup}(X)} xy \\mathbb{P}(X = x) \\mathbb{P}(Y = y) \\\\ &amp; = \\Big[\\sum\\limits_{y \\in \\text{Sup}(Y)} y \\mathbb{P}(Y = y)\\Big] \\Big[\\sum\\limits_{x \\in \\text{Sup}(X)} x \\mathbb{P}(X = x)\\Big] \\\\ &amp; = \\mathbb{E}[X] \\cdot \\mathbb{E}[Y] \\end{aligned} \\end{equation}\\] La linealidad nos permite reescribir la covarianza: \\[\\begin{equation} \\begin{aligned} \\textrm{Cov}(X_1, X_2) &amp; = \\mathbb{E}\\Big[ \\big(X_1 - \\mathbb{E}[X_1]\\big) \\cdot \\big(X_2 - \\mathbb{E}[X_2]\\big) \\Big] \\\\ &amp; = \\mathbb{E}\\Big[ X_1 X_2 \\Big] - \\mathbb{E}\\Big[X_1\\Big]\\mathbb{E}\\Big[X_2\\Big] - \\mathbb{E}\\Big[X_1\\Big]\\mathbb{E}\\Big[X_2\\Big] + \\mathbb{E}\\Big[X_1\\Big]\\mathbb{E}\\Big[X_2\\Big] \\\\ &amp; = \\mathbb{E}\\Big[ X_1 X_2 \\Big] - \\mathbb{E}\\Big[X_1\\Big]\\mathbb{E}\\Big[X_2\\Big] \\end{aligned} \\end{equation}\\] de tal forma que es claro que si \\(X_1\\) y \\(X_2\\) son independientes entonces \\(\\textrm{Cov}(X_1, X_2) = 0\\) por la propiedad anterior del valor esperado. OJO De manera general covarianza \\(0\\) no implica que las variables sean independientes como puede verse con las variables aleatorias siguientes: \\[ f_{X,Y}(x,y) = \\begin{cases} 1/8 &amp; \\text{ si } (x,y) \\in \\{ (-1,-1), (-1,1), (1, -1), (1,1)\\} \\\\ 1/2 &amp; \\text{ si } (x,y) = (0,0), \\\\ 0 &amp; \\text{ en otro caso} \\end{cases} \\] las cuales no son independientes pues \\(\\mathbb{P}(X = 0, Y = 0) = 1/2\\neq 1/4 = \\mathbb{P}(X = 0)\\cdot \\mathbb{P}(Y = 0)\\); sin embargo (ejercicio sugerido) la covarianza es \\(0\\). Una segunda propiedad de interés de la covarianza es que actúa como el producto interno (de hecho es uno): \\[ \\text{Cov}(a X + bY, cW + dV) = ac \\text{Cov}(X,W) + ad \\text{Cov}(X,V) + bc \\text{Cov}(Y,W) + bd \\text{Cov}(Y,V) \\] la cual se demuestra igual mediante la linealidad: \\[\\begin{equation} \\begin{aligned} \\textrm{Cov}(a &amp; X + bY, cW + dV) = \\mathbb{E}\\Big[ (a X + bY) (cW + dV) \\Big] - \\mathbb{E}\\Big[a X + bY\\Big]\\mathbb{E}\\Big[cW + dV\\Big] \\\\ &amp; = \\mathbb{E}\\Big[ ac XW + bc YW + ad XV + bd YV\\Big] - \\bigg( a \\mathbb{E}\\Big[ X \\Big] + b\\mathbb{E}\\Big[ Y\\Big]\\bigg)\\bigg( c\\mathbb{E}\\Big[W\\Big] + d\\mathbb{E}\\Big[V\\Big] \\bigg) \\\\ &amp; = ac \\text{Cov}(X,W) + ad \\text{Cov}(X,V) + bc \\text{Cov}(Y,W) + bd \\text{Cov}(Y,V) \\end{aligned} \\end{equation}\\] donde la última igualdad se sigue de agrupar los términos idénticos tras sus constantes. B.12 Condicionamiento por otra variable aleatoria A rellenarse pronto B.13 Funciones características A rellenarse pronto B.14 Convergencias A rellenarse pronto B.14.1 Teorema de continuidad de Lévy A rellenarse pronto B.15 Ley de los grandes números A rellenarse pronto B.16 Teorema del límite central A rellenarse pronto B.16.1 Programación en R del teorema del límite central con variables aleatorias independientes idénticamente distribuidas Lo que programaremos (por facilidad) en esta sección corresponde a ejemplos del teorema de proba 2: dadas variables aleatorias independientes idénticamente \\(\\{X_i\\}\\) distribuidas con media \\(\\mu\\) y varianza finita \\(\\sigma^2\\) tenemos que: \\[ Z =\\lim_{n \\to \\infty} \\sqrt{\\dfrac{n}{\\sigma^2}} \\cdot \\Big( \\frac{1}{n}\\sum_{i = 1}^n X_i - \\mu\\Big) \\sim \\textrm{Normal}(0,1) \\] donde el símbolo \\(\\sim\\) se lee “se distribuye”. En este caso la interpretación va a ser que para \\(n\\) muy grande tendremos que \\[ \\sqrt{\\dfrac{n}{\\sigma^2}} \\cdot \\Big( \\frac{1}{n}\\sum_{i = 1}^n X_i - \\mu\\Big) \\mathrel{\\dot\\sim} \\textrm{Normal}(0,1) \\] donde \\(\\mathrel{\\dot\\sim}\\) se lee como “se distribuye aproximadamente”. Programaremos una función en R que para \\(n\\) grande muestre eso: donde podemos ver la aproximación normal si tomamos, por ejemplo, las \\(X_i\\) siguen una distribución Gamma: La binomial se ve así: Poisson: E inclusive uniformes: Experimenta con otras distribuciones ¿puedes encontrar alguna para la que no funcione? B.16.2 Ejercicio Repite la programación del teorema del límite central pero ahora tomando las \\(X_k\\) con distintas distribuciones siempre y cuando \\(X_k\\) tenga media \\(\\mu_k\\) finita y las variables aleatorias satisfagan la condición de Lindberg (una forma de hacerlo es teniendo varianzas finitas que no incrementan con la \\(k\\)). "]
]
