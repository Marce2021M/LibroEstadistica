[
["index.html", "Estadística I: Análisis exploratorio de datos y muestreo Capítulo 1 Historia y conceptos", " Estadística I: Análisis exploratorio de datos y muestreo Rodrigo Zepeda-Tello 2020-09-03 Capítulo 1 Historia y conceptos El libro está en construcción. Por favor si encuentras cualquier error levanta un issue en Github "],
["análisis-exploratorio-de-datos.html", "Capítulo 2 Análisis Exploratorio de Datos 2.1 Inicio 2.2 Librerías 2.3 Base a analizar 2.4 Definiciones y notación 2.5 Estadísticos univariados 2.6 Ejercicio 2.7 Ejercicios 2.8 Gráficas univariadas 2.9 Gráficas bivariadas 2.10 Estadísticos bivariados 2.11 Ejercicio 2.12 Ajuste funcional 2.13 Ejercicios del capítulo", " Capítulo 2 Análisis Exploratorio de Datos 2.1 Inicio Siempre que inicies un nuevo trabajo en R ¡no olvides borrar el historial! rm(list = ls()) #Clear all 2.2 Librerías Para este análisis vamos a tener que llamar a las siguientes librerías previamente instaladas (por única vez) con install.packages: library(tidyverse) library(dplyr) library(moments) library(lubridate) library(ggcorrplot) library(ks) Si no tienes una librería puedes instalarla escribiendo en la consola el install junto con su nombre: install.packages(&quot;lubridate&quot;) 2.3 Base a analizar Como ejemplo analizaremos la base de Carpetas de Investigación de la Fiscalía General de Justicia de la CDMX para el año 2018 y mes de Diciembre misma que se encuentra en este link Si el link anterior no abre ve al sitio https://datos.cdmx.gob.mx/explore/dataset/carpetas-de-investigacion-pgj-cdmx/table/?refine.ao_hechos=2018 y elige la opción de año 2018, mes diciembre y descargar como csv. La forma más fácil en RStudio es yéndonos a Import Dataset en el panel derecho seguido de From Text y seleccionamos el archivo. En este caso hay dos opciones cualquiera de las dos opciones funciona: si en tu ordenador no sirve una, ¡prueba la otra! En mi caso el archivo está en una carpeta que se llama datasets y se lee de la siguiente manera: datos &lt;- read.csv(&quot;datasets/carpetas-de-investigacion-pgj-cdmx.csv&quot;) 2.4 Definiciones y notación Siguiendo la definición de Gelman et al. (2013) , denotamos el conjunto de datos observados como la matriz (base de datos) de \\(\\ell \\times n\\) \\[ Z = \\begin{pmatrix} z_1 \\Big| z_2 \\Big| \\dots \\Big| z_{\\ell} \\end{pmatrix} \\] donde \\(\\ell \\in \\mathbb{N}\\) con \\(\\ell &gt; 0\\) y las \\(z_i\\) sin pérdida de generalidad, son vectores columna de longitud \\(n\\) (\\(z_i = (z_{i,1}, z_{i,2}, \\dots, z_{i,n})^T\\)). Una columna \\(z_{k}\\) con \\(0 \\leq k \\leq \\ell\\) se le conoce como: Numérica si \\(z_{k} \\in \\mathbb{R}^{n}\\). En particular es entera si \\(z_{j} \\in \\mathbb{Z}^{n}\\). Categórica si cada entrada de \\(z_{k}\\) es una indicadora de pertenencia a algún conjunto (por ejemplo Hombre / Mujer ó Ingresos Altos / Ingresos Medios / Ingresos Bajos). Usualmente \\(z_{k}\\) se representa con un caracter o con un entero. Una variable cateórica puede ser lógica si \\(z_{k}\\) es un indicador que toma alguno de los dos valores: TRUE ó FALSE. Ordinal Una variable ordinal es aquél \\(z_{k} \\in \\mathcal{C}\\) donde sobre \\(\\mathcal{C}\\) existe un orden total; es decir si \\(x,y,w\\in z_{k}\\) se tiene que: Ocurre al menos una de las siguientes: \\(x \\leq y\\) ó \\(x \\geq y\\). Si \\(x \\leq y\\) y \\(y \\geq w\\) entonces \\(x \\leq w\\) Si \\(x \\leq y\\) y \\(x \\geq y\\) entonces \\(x = y\\). Variables numéricas univariadas son ordinales por el orden natural de \\(\\mathbb{R}\\). Caracter si \\(z_{k}\\) es un caracter o una cadena de caracteres donde los caracteres son el objeto de análisis en sí (no como pertenencia). Por ejemplo si cada entrada \\(z_{k,m}\\) representa un Tweet. OJO Los datos \\(z_{k,m}\\) son variables fijas ya dadas y NO SON ALEATORIAS. En el caso de nuestra base de datos podemos resumir la información contenida en la misma mediante glimpse: datos %&gt;% glimpse() ## Rows: 19,861 ## Columns: 18 ## $ año_hechos &lt;int&gt; 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, … ## $ mes_hechos &lt;chr&gt; &quot;Diciembre&quot;, &quot;Diciembre&quot;, &quot;Diciembre&quot;, &quot;Diciembr… ## $ fecha_hechos &lt;chr&gt; &quot;2018-12-13 12:00:00&quot;, &quot;2018-12-22 19:00:00&quot;, &quot;2… ## $ delito &lt;chr&gt; &quot;USURPACIÓN DE IDENTIDAD&quot;, &quot;SUSTRACCION DE MENOR… ## $ categoria_delito &lt;chr&gt; &quot;DELITO DE BAJO IMPACTO&quot;, &quot;DELITO DE BAJO IMPACT… ## $ fiscalía &lt;chr&gt; &quot;INVESTIGACIÓN EN MIGUEL HIDALGO&quot;, &quot;INVESTIGACIÓ… ## $ agencia &lt;chr&gt; &quot;MH-2&quot;, &quot;59&quot;, &quot;BJ-1&quot;, &quot;IZP-9&quot;, &quot;75TER&quot;, &quot;FDS-5&quot;,… ## $ unidad_investigacion &lt;chr&gt; &quot;UI-1SD&quot;, &quot;UI-1CD&quot;, &quot;UI-1SD&quot;, &quot;UI-2SD&quot;, &quot;3 S/D&quot;,… ## $ colonia_hechos &lt;chr&gt; &quot;LOMAS DE SOTELO&quot;, NA, &quot;DEL VALLE CENTRO&quot;, &quot;AMPL… ## $ alcaldia_hechos &lt;chr&gt; &quot;MIGUEL HIDALGO&quot;, &quot;CUAUTLA&quot;, &quot;BENITO JUAREZ&quot;, &quot;I… ## $ fecha_inicio &lt;chr&gt; &quot;2019-06-16 12:14:09&quot;, &quot;2019-06-06 16:26:15&quot;, &quot;2… ## $ mes_inicio &lt;chr&gt; &quot;Junio&quot;, &quot;Junio&quot;, &quot;Febrero&quot;, &quot;Febrero&quot;, &quot;Abril&quot;,… ## $ ao_inicio &lt;int&gt; 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, … ## $ calle_hechos &lt;chr&gt; &quot;AV. CONSCRIPTO&quot;, &quot;AVENIDFA DIEZ DE MARZO&quot;, &quot;FEL… ## $ calle_hechos2 &lt;chr&gt; &quot;.&quot;, &quot;HECHOS EN CUAUTLA MORELOS&quot;, &quot;ESQUINA COYOA… ## $ longitud &lt;dbl&gt; -99.22535, NA, -99.17088, -99.03016, -99.13423, … ## $ latitud &lt;dbl&gt; 19.44028, NA, 19.37207, 19.34797, 19.54788, 19.3… ## $ Geopoint &lt;chr&gt; &quot;19.4402832543,-99.2253527208&quot;, &quot;&quot;, &quot;19.37206828… Notamos que el vector columna año_hechos es una variable numérica mientras que mes_hechos es categórica. No hay variables lógicas en esta base. Una variable caracter es el vector columna calle_hechos que no denota un conjunto sino una cadena de caracteres (véanse las faltas de ortografía, por ejemplo). Al ser la tabla de datos una matriz podemos acceder a la entrada en la fila \\(j\\) y columna \\(k\\) haciendo: \\[ \\textrm{base}[j,k] \\] por ejemplo: datos[4,6] ## [1] &quot;INVESTIGACIÓN EN IZTAPALAPA&quot; NOTACIÓN Para facilitar la notación en lo que sigue de estas notas y hasta nuevo aviso, si \\(z_k\\) es una columna categórica de \\(Z\\) denotaremos a los elementos de dicha columna como \\(C = (c_1, c_2, \\dots, c_n) = z_k^T\\). Si \\(z_k\\) es numérica denotamos a los elementos de dicha columna como \\(\\vec{x} = (x_1, x_2, \\dots, x_n) = z_k^T\\). 2.5 Estadísticos univariados 2.5.1 Definición [Estadístico] Un estadístico es una función cuyo dominio es la matriz de datos observados \\(Z\\) o una columna de la misma. Es decir, un estadístico es cualquier función de los datos (ver Wolfe and Schneider (2017)). A continuación veremos algunos ejemplos de estadísticos así como su interpretación. 1. Media poblacional Dado un vector de datos numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) definimos la media poblacional como: \\[ \\bar{x} = \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} x_i, \\qquad x_i \\in \\mathbb{R} \\] En el caso de nuestros datos podemos calcular el promedio de delitos por día como sigue. Primero necesitamos especificar a R que la fecha_hechos es una fecha. Esto lo hacemos mediante la función ymd_hms (year-month-day_hour-minute-second) del paquete de lubridate y la función mutate (que cambia una columna de la base de datos). El siguiente código le indica a R que cambie la columna fecha_hechos volviéndola a leer como fecha: datos &lt;- datos %&gt;% mutate(fecha_hechos = ymd_hms(fecha_hechos)) Para mantener sólo la fecha y eliminar la hora de fecha_hechos podemos generar una nueva columna como sigue: datos &lt;- datos %&gt;% mutate(fecha = date(fecha_hechos)) Finalmente podemos contar (tally) observaciones agrupadas (group_by) por día mediante la combinación de ambas funciones: conteo_delitos &lt;- datos %&gt;% group_by(fecha) %&gt;% tally() ## # A tibble: 6 x 2 ## fecha n ## &lt;date&gt; &lt;int&gt; ## 1 2018-12-01 674 ## 2 2018-12-02 584 ## 3 2018-12-03 790 ## 4 2018-12-04 640 ## 5 2018-12-05 724 ## 6 2018-12-06 718 Hay distintas formas de calcular la media. La primera es tomando la columna directo, para acceder a una columna utilizamos el signo de pesos \\(\\$\\) como sigue: \\[ \\texttt{base} \\texttt{\\$} \\texttt{columna} \\] En nuestro caso: mean(conteo_delitos$n) ## [1] 640.6774 O bien podemos usar la función summarise integrada en dplyr: conteo_delitos %&gt;% summarise(mean(n)) ## # A tibble: 1 x 1 ## `mean(n)` ## &lt;dbl&gt; ## 1 641. NOTA Una media como está descrita arriba no aplica para datos circulares. Por ejemplo, si queremos determinar el mes promedio en el que ocurren las lluvias dentro de los años se sabe que después del mes 12 continúa el mes 1 del próximo año. Una media tradicional no considera datos que pueden ser descritos mediante aritmética modular (como los meses). Para ello se utiliza la media circular: 1.1 Media circular Consideremos el problema de determinar el día promedio de la semana en que más ocurren delitos (de Lunes a Domingo). Podemos resumir los eventos usando la función weekdays: datos &lt;- datos %&gt;% mutate(`Día de la Semana` = weekdays(fecha)) conteo.dia &lt;- datos %&gt;% group_by(`Día de la Semana`) %&gt;% count() de donde se tiene el conteo: Día de la Semana n Monday 3251 Saturday 3197 Friday 2833 Sunday 2722 Wednesday 2701 Thursday 2679 Tuesday 2478 Para obtener el día promedio representamos cada uno de los días en el círculo usando coordenadas polares. Nota que el radio es irrelevante en este caso: sólo el ángulo importa; de ahí que tomemos \\(r = 1\\): Para un conjunto de mediciones con ángulos \\((\\theta_1, \\theta_2, \\dots, \\theta_n)^T\\) el centro de masa asociado a dichas mediciones es el punto \\((\\bar{c}, \\bar{s})\\) donde \\[ \\bar{c} = \\frac{1}{n}\\sum\\limits_{i = 1}^n \\cos (\\theta_i) \\qquad \\text{y} \\qquad \\bar{s} = \\frac{1}{n}\\sum\\limits_{i = 1}^n \\sin (\\theta_i) \\] La dirección media se define como la solución \\(\\bar{\\theta}\\) (si \\(\\bar{r} &gt; 0\\)) a: \\[ \\bar{c} = \\bar{r}\\cos\\bar{\\theta} \\qquad \\text{ y } \\qquad \\bar{s} = \\bar{r}\\sin\\bar{\\theta} \\] donde \\(\\bar{r}\\) se conoce como la longitud resultante promedio . Si \\(\\bar{r} = 0\\) no existe dirección media. De manera explícita, por geometría tenemos que: \\[ \\bar{r} = (\\bar{c}^2 + \\bar{s}^2)^{1/2} \\] y que: \\[ \\bar{\\theta} = \\text{atan2}(\\bar{s}/\\bar{c}) = \\begin{cases} \\text{atan}(\\bar{s}/\\bar{c}) &amp; \\text{ si } \\bar{c} \\geq 0\\\\ \\text{atan}(\\bar{s}/\\bar{c}) + \\pi &amp; \\text{ si } \\bar{c} &lt; 0\\\\ \\end{cases} \\] donde el caso \\(\\bar{c} = 0\\) se interpreta como el límite por la derecha (respectivamente por la izquierda) de la arcotangente de acuerdo con el signo de \\(\\bar{s}\\). Para más información sobre estadística circular puedes consultar Pewsey, Neuhäuser, and Ruxton (2013) 2.6 Ejercicio Utiliza la función atan2 de R junto con cos y sin para seno y coseno para estimar el día promedio en el que ocurren más delitos según la base conteo.dia. 2. Total poblacional (ver Särndal, Swensson, and Wretman (2003)) Dado un vector de datos numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) definimos el total poblacional como: \\[ t_{\\vec{x}} = \\sum\\limits_{i=1}^{n} x_i, \\qquad x_i \\in \\mathbb{R} \\] En este caso de las carpetas de investigación el total nos daría todas las carpetas abiertas durante diciembre. Para ello calculamos el total sumando todos los elementos: sum(conteo_delitos$n) ## [1] 19861 O bien (y esto es una de las cosas interesantes de tidyverse) agregándolo a los cálculos previos: conteo_delitos %&gt;% summarise(mean(n), sum(n)) ## # A tibble: 1 x 2 ## `mean(n)` `sum(n)` ## &lt;dbl&gt; &lt;int&gt; ## 1 641. 19861 3. Varianza poblacional (no ajustada) Dado un vector de datos numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) definimos la varianza poblacional como1: \\[ \\sigma^2_{\\vec{x}} = \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} (x_i - \\bar{x})^2, \\qquad x_i \\in \\mathbb{R} \\] Misma que podemos calcular con el comando var ya sea directamente en la columna: var(conteo_delitos$n) ## [1] 10046.23 O bien a través del summarise integrando con el anterior: conteo_delitos %&gt;% summarise(mean(n), sum(n), var(n)) ## # A tibble: 1 x 3 ## `mean(n)` `sum(n)` `var(n)` ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 641. 19861 10046. La raíz cuadrada de la varianza se conoce como desviación estándar y se denota como sigue: \\[ \\sigma_{\\vec{x}} = \\sqrt{\\sigma^2_{\\vec{x}}} \\] Recuerda que la varianza se interpreta como la distancia cuadrática promedio a la que están los datos. En particular la varianza casi no considera valores que están a menos de \\(1\\) de distancia de \\(\\bar{x}\\) (pues \\((x_i - \\bar{x})^2 &lt; 1\\) en ese caso) pero le da mayor peso a valores que están muy lejanos (donde \\((x_i - \\bar{x})^2 \\gg 1\\) si \\(x_i\\) está muy lejos de \\(\\bar{x}\\)). Gráficamente: Si nos interesara que todos los valores (tanto los cercanos a \\(\\bar{x}\\) como los lejanos) pesaran de manera idéntica entonces usaríamos el MAD: 3.1 Varianza angular (circular) En el caso de datos circulares, Pewsey, Neuhäuser, and Ruxton (2013) define la varianza circular como: \\[ \\textrm{Var} = 1 - \\bar{r} \\] donde \\(\\bar{r} = (\\bar{c}^2 + \\bar{s}^2)^{1/2}\\) es el resolvente explicado anteriormente. 4. Desviación Media Absoluta (MAD) Dado un vector de datos numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) definimos la desviación media absoluta, MAD, como (Panaretos (2016)): \\[ \\text{MAD}_{\\vec{x}} = \\frac{1}{n} \\sum\\limits_{i=1}^{n} | x_i - \\bar{x} | \\] Misma que se puede calcular en R como: mad(conteo_delitos$n) ## [1] 115.6428 o bien dentro del summarise: conteo_delitos %&gt;% summarise(mean(n), sum(n), var(n), mad(n)) ## # A tibble: 1 x 4 ## `mean(n)` `sum(n)` `var(n)` `mad(n)` ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 641. 19861 10046. 116. La MAD también es una forma de medir distancia pero en este caso se tiene que todos aportan por igual los muy alejados y los que no: Para pensarle: En el caso de una variable que se supone que es uniforme y no interesa penalizar valores lejanos de la media ¿cuál sería una mejor manera de cuantificar la dispersión MAD ó varianza? ¿en qué casos importaría la otra? Las siguientes dos definiciones son con base en conceptos de proba. ¿Los recuerdas? 5. Coeficiente de asimetría Dado un vector de datos numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) definimos el coeficiente de asimetría de Fisher (skewness) como: \\[ \\text{Skewness}_{\\vec{x}} = \\frac{1}{n \\sigma^3_{\\vec{x}} } \\sum\\limits_{i = 1}^{n} (x_i - \\bar{x})^3 \\] Para más referencias ver Panaretos (2016). A fin de interpretar el coeficiente de asimetría podemos dividir esa suma en dos pedazos (olvidándonos de la constante): \\[ \\sum\\limits_{i = 1}^{n} (x_i - \\bar{x})^3 = \\underbrace{\\sum\\limits_{\\substack{i = 1 \\\\ \\\\ x_i &gt; \\bar{x}}}^{n} (x_i - \\bar{x})^3}_{\\text{A}} + \\underbrace{\\sum\\limits_{\\substack{i = 1 \\\\ \\\\ x_i &lt; \\bar{x}}}^{n} (x_i - \\bar{x})^3}_{\\text{B}} \\] Notamos que si \\(|A| &gt; |B|\\) la mayor parte de las \\(x_i\\) (o las que se alejan más de la media) son mayores a \\(\\bar{x}\\) y por tanto los datos van a estar sesgados a la derecha:. Por otro lado si \\(|B| &gt; |A|\\) significa que hay más \\(x_i\\) (o con mayor peso) del lado izquierdo de la media que del lado derecho de la misma y por tanto los datos están sesgados a la izquierda. Datos insesgados son aquellos donde \\(\\text{Skewness}_{\\vec{x}} = 0\\). En el caso de las carpetas podemos calcular la asimetría que no se encuentra preprogramada en R como sigue: #Estimación de la desviación estándar desv.est &lt;- sd(conteo_delitos$n) #Estimación del x barra x.barra &lt;- mean(conteo_delitos$n) #Obtención de la n (longitud del vector) n.longitud &lt;- length(conteo_delitos$n) #Cálculo de la asimetría (1/desv.est^3)*mean((conteo_delitos$n - x.barra)^3) ## [1] -0.4528209 ¿Qué implica el resultado anterior? 6. Curtosis Dado el mismo vector \\(\\vec{x}\\) que en el enunciado anterior el coeficiente de curtosis se define como \\[ \\text{Curtosis}_{\\vec{x}} = \\frac{1}{n \\sigma^4_{\\vec{x}} } \\sum\\limits_{i = 1}^{n} (x_i - \\bar{x})^4 \\] La interpretación de la curtosis es similar a la que hizimos de la varianza en el sentido que el elevar a la cuarta va a magnificar los efectos de aquellos valores que estén a más de \\(\\sigma\\) de distancia de la media pues podemos reescribir la suma como: \\[ \\frac{1}{n \\sigma^4_{\\vec{x}} } \\sum\\limits_{i = 1}^{n} (x_i - \\bar{x})^4 = \\frac{1}{n \\sigma^4_{\\vec{x}} } \\underbrace{\\sum\\limits_{\\substack{i = 1 \\\\ \\\\ | x_i - \\bar{x}| &lt; \\sigma}}^{n} (x_i - \\bar{x})^4}_{\\text{A}} + \\frac{1}{n \\sigma^4_{\\vec{x}} } \\underbrace{\\sum\\limits_{\\substack{i = 1 \\\\ \\\\ | x_i - \\bar{x}| &gt; \\sigma}}^{n} (x_i - \\bar{x})^3}_{\\text{B}} \\] Notamos que la única parte importante que apota a la curtosis es la dada por B que es la que capta las colas de la distribución (pues ese lado es \\(\\gg 1\\)) . De ahí que podamos decir que, entre dos vectores de datos, uno tiene colas más pesadas que el otro si su curtosis es mayor. En este caso podemos analizar la latitud y longitud de los datos a través de la curtosis: datos %&gt;% summarise(kurtosis(latitud, na.rm = T), kurtosis(longitud, na.rm = T)) ## kurtosis(latitud, na.rm = T) kurtosis(longitud, na.rm = T) ## 1 2.857934 3.045037 donde se agregó el comando na.rm = T para eliminar los valores de no respuesta (missing) marcados como NA. Del análisis notamos que la longitud tiene colas más pesadas que la latitud. NOTACIÓN Dado un vector \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) de valores numéricos denotamos el \\(j\\)-ésimo valor muestral (\\(1 \\leq j \\leq n\\)) como \\(x_{(j)}\\) tal que \\(x_{(1)} = \\min \\{ x_1, x_2, \\dots, x_n \\}\\) y \\[ x_{(j)} = \\min \\{ x_1, x_2, \\dots, x_n \\} \\setminus \\{ x_{(1)}, x_{(2)}, \\dots, x_{(j-1)} \\} \\] Es decir \\(x_{(j)}\\) es el valor en orden \\(j\\) al momento de ordenar la muestra. Como nota adicional se define \\(x_{(0)} = 0\\) y \\(x_{(n+1)} = 0\\). Nota La curtosis a veces se define con un denominador distinto (en términos de las \\(n\\)) como en Myatt and Johnson (2007). 7. Mediana Dado un vector de valores numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) definimos la mediana como (Panaretos (2016)): \\[ \\text{Mediana}_{\\vec{x}} = \\dfrac{x_{(\\lfloor \\frac{n+1}{2} \\rfloor)} + x_{(\\lceil \\frac{n+1}{2} \\rceil)}}{2} \\] La mediana puede calcularse fácilmente haciendo: median(conteo_delitos$n) ## [1] 646 8. Cuantil Dado un vector de valores numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) el \\(\\alpha\\)-ésimo cuantil está dado por: \\[ \\text{Cuantil}_{\\vec{x}}(\\alpha) = \\dfrac{x_{x_{(\\lfloor \\alpha\\cdot (n+1) \\rfloor)}} + x_{(\\lceil \\alpha\\cdot (n+1)\\rceil)}}{2} \\] donde \\(x_{(0)} = x_{(n+1)} = 0\\). R no calcula los cuantiles de manera exacta sino que por velocidad los aproxima mediante la función quantile. Por ejemplo en el cálculo de los cuantiles \\(\\alpha = 0.1\\) y \\(\\alpha = 0.66\\): conteo_delitos %&gt;% summarise(quantile(n, c(0.1, 0.66))) ## # A tibble: 2 x 1 ## `quantile(n, c(0.1, 0.66))` ## &lt;dbl&gt; ## 1 501 ## 2 707 La función summary también es bastante útil resumiendo múltiples observaciones de la base: summary(conteo_delitos$n) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 397.0 568.0 646.0 640.7 721.0 790.0 Ésta incluye los cuartiles los cuales corresponden a los cuantiles asociados a \\(\\alpha =0.25, 0.5, 0.75\\) y \\(1\\). Nota Hay múltiples definiciones de cuantil (ver Hyndman and Fan (1996) para un intento de homologación). En particular R utiliza una distinta y tus cómputos no van a coincidir si lo haces con esta definición y con la de R. Si quieres saber más de R consulta ?quantile 9. Rango intercuartílico Definimos el rango intercuartílico (Panaretos (2016)) para valores numèricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) como la distancia entre el cuantil \\(0.75\\) y el \\(0.25\\) (primer y tercer cuartil): \\[ \\text{IQR}_{\\vec{x}} = \\text{Cuantil}_{\\vec{x}}(0.75) - \\text{Cuantil}_{\\vec{x}}(0.25) \\] IQR(conteo_delitos$n) ## [1] 153 10. Valores atípicos (outliers) Dado un vector de datos numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) seguimos a Panaretos (2016) para definir los valores atípicos outliers como aquellas observaciones: \\[ \\textrm{Outliers}_{\\vec{x}} = \\Big\\{ x_i \\in \\vec{x} \\big| x_i \\not\\in \\big[ \\text{Cuantil}_{\\vec{x}}(0.25) - \\frac{3}{2} \\text{IQR}_{\\vec{x}}, \\text{Cuantil}_{\\vec{x}}(0.75) + \\frac{3}{2} \\text{IQR}_{\\vec{x}}\\big] \\Big\\} \\] Los outliers en esta definción son valores que serían verdaderamente improbables bajo una distribución normal. Particularmente en el caso de la normal los outliers son valores que tienen una probabilidad de salir aproximadamente de 0.0069766 (por eso son atípicos porque no se esperaría que aparecieran nunca). Para identificar los outliers calculamos el IQR primero y los cuartiles: iqr &lt;- IQR(conteo_delitos$n) cuartil1 &lt;- quantile(conteo_delitos$n, 0.25) cuartil3 &lt;- quantile(conteo_delitos$n, 0.75) después identificamos el límite inferior y superior del conjunto lim.inf &lt;- cuartil1 - 3/2*iqr lim.sup &lt;- cuartil3 + 3/2*iqr finalmente preguntamos por cuáles están antes o después: outliers &lt;- conteo_delitos %&gt;% filter(n &lt; lim.inf | n &gt; lim.sup) En este caso no tenemos outliers. NOTA Según la aplicación que tenemos la definición de outlier cambia. La actual es la que se utiliza para datos que pudieran ser descritos mediante una Normal; empero, no siempre esta definición de outlier es un buen modelo (por ejemplo en datos como ingreso que son cantidades positivas, con mucha asimetría y cola pesada). Un buen tratamiento sobre los outliers puedes encontrarlo en SURI, Murty, and Athithan (2019). 11. Rango El rango (Peck, Olsen, and Devore (2015)) se define como la diferencia entre el mínimo y el máximo de los valores de un vector numérico \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\): \\[ \\textrm{Rango}_{\\vec{x}} = \\max \\{x_1, x_2, \\dots, x_n\\} - \\min \\{x_1, x_2, \\dots, x_n\\} \\] En R puede calcularse con la resta: #Obtenemos máximo y mínimo maximo &lt;- max(conteo_delitos$n) minimo &lt;- min(conteo_delitos$n) #Rango maximo - minimo ## [1] 393 Nota En algunos casos el rango se refiere al intervalo \\([a,b]\\) de valores donde \\(a = \\min \\{x_1, x_2, \\dots, x_n\\}\\) y \\(b = \\max \\{x_1, x_2, \\dots, x_n\\}\\). Éste es el caso de la función range en R: range(conteo_delitos$n) ## [1] 397 790 12. Conteo asociado a un conjunto Sea \\(\\vec{y} = (y_1, y_2, \\dots, y_n)^T\\) un vector de datos de cualquier tipo (numéricos, categóricos, lógicos, caracteres, etc). Para un conjunto \\(A\\) definimos el conteo asociado al conjunto \\(A\\) como: \\[ \\text{Conteo}_{\\vec{y}}(A) = \\sum\\limits_{i = 1}^{n} \\mathbb{I}_A (y_i) \\] donde \\[ \\mathbb{I}_A (y) = \\begin{cases} 1 &amp; \\text{ si } y \\in A, \\\\ 0 &amp; \\text{ en otro caso }, \\end{cases} \\] es una variable indicadora. Una forma rápida de obtener dicho conteo en R es mediante table: table(datos$delito) ## ## ABANDONO DE PERSONA ABORTO ABUSO DE AUTORIDAD ABUSO DE CONFIANZA ## 53 15 102 276 ## ABUSO SEXUAL ACOSO SEXUAL ## 252 30 O bien si se desean contar en la base de datos por ejemplo los delitos de ABANDONO DE PERSONA pueden hacerse mediante un filtro. datos %&gt;% filter(delito == &quot;ABANDONO DE PERSONA&quot;) %&gt;% tally() ## n ## 1 53 Al filtro pueden agregárseles grupos por si se desea obtener por fecha: datos %&gt;% filter(delito == &quot;ABANDONO DE PERSONA&quot;) %&gt;% group_by(fecha) %&gt;% tally() ## # A tibble: 21 x 2 ## fecha n ## &lt;date&gt; &lt;int&gt; ## 1 2018-12-01 3 ## 2 2018-12-02 3 ## 3 2018-12-04 2 ## 4 2018-12-05 8 ## 5 2018-12-06 1 ## 6 2018-12-07 1 ## 7 2018-12-10 1 ## 8 2018-12-12 2 ## 9 2018-12-13 3 ## 10 2018-12-14 2 ## # … with 11 more rows El filtro funciona igual que un if pudiéndose usar (&amp;) u (|): datos %&gt;% filter(delito == &quot;ABANDONO DE PERSONA&quot; | delito == &quot;ABORTO&quot;) %&gt;% group_by(fecha) %&gt;% tally() ## # A tibble: 25 x 2 ## fecha n ## &lt;date&gt; &lt;int&gt; ## 1 2018-12-01 3 ## 2 2018-12-02 3 ## 3 2018-12-04 5 ## 4 2018-12-05 8 ## 5 2018-12-06 1 ## 6 2018-12-07 2 ## 7 2018-12-10 1 ## 8 2018-12-12 2 ## 9 2018-12-13 4 ## 10 2018-12-14 2 ## # … with 15 more rows datos %&gt;% filter(delito == &quot;ABANDONO DE PERSONA&quot; &amp; fiscalía == &quot;INVESTIGACIÓN EN IZTAPALAPA&quot;) %&gt;% group_by(fecha) %&gt;% tally() ## # A tibble: 3 x 2 ## fecha n ## &lt;date&gt; &lt;int&gt; ## 1 2018-12-02 1 ## 2 2018-12-13 1 ## 3 2018-12-20 1 13. Moda En términos simples, la moda es el conjunto de los valores que más se repiten. Matemáticamente (ver Peck, Olsen, and Devore (2015)) la moda es el conjunto \\(\\textrm{Moda}_{\\vec{y}} = \\{ m_1, m_2, \\dots, m_k \\}\\) tal que \\(m \\in \\textrm{Moda}\\) sí y sólo si \\[ \\sum_{i = 1}^{n} \\mathbb{I}_{\\{m\\}}(y_i) \\geq \\sum_{i = 1}^{n} \\mathbb{I}_{\\{ \\ell\\}}(y_i) \\qquad \\forall \\ell \\neq m \\textrm{ donde } y_i \\in \\vec{y}. \\] Para calcularla en R no existe una función predefinida para calcular la moda. Nosotros podemos crearla con el comando function. El término function nos sirve para construir funciones; por ejemplo, una función que eleva al cuadrado: elevar.cuadrado &lt;- function(x){ return(x^2) } Observa la estructura que siempre será de esta forma: nombre de la función &lt;- function(parámetro, otro parámetro){ #Lo que sea que haga return(lo que devuelve) } Podemos llamar a la función con un número: elevar.cuadrado(8) ## [1] 64 o bien con un vector: elevar.cuadrado(12) ## [1] 144 En nuestro caso vamos a crear una función que se llame moda para estimar la moda: #Función para estimar la moda de un vector x moda &lt;- function(x){ #Contar cuántas veces aparecen las observaciones conteo &lt;- table(x) #Obtengo el máximo que aparece max_aparece &lt;- max(conteo) #Busco cuáles aparecen más y obtengo los nombres moda &lt;- names(conteo)[which(conteo == max_aparece)] #Finalmente checo que si los datos eran numéricos moda debe #ser numérico if (is.numeric(x)){ moda &lt;- as.numeric(moda) } return(moda) } Podemos probar nuestra función con datos que ya sepamos su resultado nada más para asegurarnos que funciona: #Creamos un vector numérico con dos modas vector.ejemplo.1 &lt;- c(1,6,6,1,2,7,8,10) moda(vector.ejemplo.1) ## [1] 1 6 Podemos probarlo también con caracteres: #Creamos un vector numérico con dos modas vector.ejemplo.2 &lt;- c(&quot;manzana&quot;,&quot;pera&quot;,&quot;guayaba&quot;,&quot;perejil&quot;,&quot;manzana&quot;) moda(vector.ejemplo.2) ## [1] &quot;manzana&quot; Una vez sabemos funciona podemos buscar el delito que ocurrió más: moda(datos$delito) ## [1] &quot;VIOLENCIA FAMILIAR&quot; 2.7 Ejercicios Construye una función que tome de input dos variables: \\(x\\) un vector y \\(k\\) un entero de tal manera que calcule el \\(k\\)-ésimo momento central de los datos: \\[ \\text{Momento}_{\\vec{x}}(k) = \\frac{1}{n} \\sum\\limits_{i=1}^n (x_i - \\bar{x})^k \\] La función debe tener la siguiente estructura: kesimo.momento &lt;- function(x, k){ #Rellena aquí } Sin usar la opción de trim ni trimmed.mean crea una función que calcule la media de los datos que están entre el cuantil \\(\\alpha/2\\) y el cuantil \\(1 - \\alpha/2\\) (\\(0 \\leq \\alpha \\leq 1\\). A esta media se le conoce como media truncada al nivel \\(\\alpha \\times 100\\%\\). Matemáticamente se define como: \\[ \\textrm{Media Truncada}_{\\vec{x}}(\\alpha) = \\frac{1}{n_\\alpha} \\sum\\limits_{i = 1}^{n} x_i \\cdot \\mathbb{I}_{[q_{\\alpha/2}, q_{1-\\alpha/2}]}(x_i) \\] donde \\(n_{\\alpha} = \\sum_{i=1}^n \\mathbb{I}_{[q_{\\alpha/2}, q_{1-\\alpha/2}]}(x_i)\\) es la cantidad de \\(x_i\\) que están en el intervalo \\([q_{\\alpha/2}, q_{1-\\alpha/2}]\\) donde \\(q_{\\alpha/2} = \\text{Cuantil}_{\\vec{x}}(\\alpha/2)\\) y \\(q_{1 - \\alpha/2} = \\text{Cuantil}_{\\vec{x}}(1 - \\alpha/2)\\). Una función llamada jesimo.dato de dos argumentos que dado un vector de datos \\(\\vec{x}\\) me devuelva el \\(j\\)-ésimo dato ordenado (es decir el \\(x_{(j)}\\)). NOTA No confundir con devolver el \\(x_j\\) que es la \\(j\\)-ésima entrada. Como sugerencia usar arrange, order ó sort. Un ejemplo de lo que debe hacer la función es: x &lt;- c(12,8,9,7,14, 21) jesimo.dato(x, 4) ## [1] 12 2.8 Gráficas univariadas 1. Gráfica de caja (boxplot) Una gráfica de caja pretende resumir los cuartiles, la mediana e identificar los outliers todo en una sola imagen (Panaretos (2016)). Para ello considera un vector numérico \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) tal que: \\(q_1\\) sea el primer cuartil (\\(\\textrm{Cuantil}_{\\vec{x}}(0.25)\\)), \\(q_2\\) sea la mediana (que es lo mismo que el segundo cuartil o bien \\(\\textrm{Cuantil}_{\\vec{x}}(0.5)\\)) y \\(q_3\\) corresponda al tercer cuartil (\\(\\textrm{Cuantil}_{\\vec{x}}(0.75)\\)). \\(w_1 = \\min \\{x_j \\in \\vec{x} | x_j \\geq q_1 - \\frac{3}{2} IQR \\}\\) es el valor más pequeño de \\(\\vec{x}\\) que no es outlier y \\(w_2 = \\max \\{x_j \\in \\vec{x} | x_j \\leq q_3 + \\frac{3}{2} IQR \\}\\) es el valor más grande de \\(\\vec{x}\\) que no es outlier. Sea \\(\\textrm{Outliers}_{\\vec{x}}\\) el conjunto de outliers como lo definimos anteriormente: \\[ \\textrm{Outliers}_{\\vec{x}} = \\Big\\{ x_i \\in \\vec{x} \\big| x_i \\not\\in \\big[ q_1 - \\frac{3}{2} \\text{IQR}_{\\vec{x}}, q_3 + \\frac{3}{2} \\text{IQR}_{\\vec{x}}\\big] \\Big\\} \\] donde \\(\\textrm{Outliers}_{\\vec{x}} = \\{ o_1, o_2, \\dots, o_d \\}\\). Una gráfica de caja corresponde al siguiente diagrama: La imagen anota la mediana, los cuartiles así como el rango de valores donde se sabe que no hay outliers. Finalmente la gráfica identifica los outliers si es que hay. Para armar una gráfica de boxplot usamos la librería de ggplot2 especificando dentro de la función ggplot la base de datos de donde sale nuestra información: ggplot(conteo_delitos) + geom_boxplot(aes(x = n)) la cual pone la mediana en 646 como habíamos calculado, los cuartiles en 568 y 721 respectivamente. Finalmente no presenta outliers pues nuestro análisis previo nos mostraba que no había outliers. Podemos personalizar nuestra gráfica agregando títulos con la función lab: ggplot(conteo_delitos) + geom_boxplot(aes(x = n)) + labs( x = &quot;Cantidad de carpetas de investigación abiertas por día&quot;, y = &quot;&quot;, title = &quot;Gráfica de cajas de los delitos en CDMX&quot;, subtitle = &quot;Fuente: Carpetas de investigación FGJ de la Ciudad de México&quot;, caption = &quot;Datos de Diciembre 2018&quot; ) Finalmente, podemos personalizar los colores de la gráfica editando directamente en el geom_boxplot: ggplot(conteo_delitos) + geom_boxplot(aes(x = n), color = &quot;red&quot;, fill = &quot;deepskyblue4&quot;) + labs( x = &quot;Cantidad de carpetas de investigación abiertas por día&quot;, y = &quot;&quot;, title = &quot;Gráfica de cajas de los delitos en CDMX&quot;, subtitle = &quot;Fuente: Carpetas de investigación FGJ de la Ciudad de México&quot;, caption = &quot;Datos de Diciembre 2018&quot; ) 2. Gráfica de barras Sea \\(\\vec{c} = (c_1, c_2, \\dots, c_n)^T\\) un vector de datos categóricos. Sea \\(C = \\{ a_i | a_i \\in \\vec{c} \\}\\) el conjunto de \\(\\ell\\) valores únicos que se tienen registrados en el vector \\(\\vec{c}\\). Denotamos la cantidad de veces que aparece \\(a_i\\) en \\(\\vec{c}\\) como \\(n_i\\); es decir: \\[ n_i = \\sum\\limits_{k = 1}^n \\mathbb{I}_{\\{a_i\\}}(c_k) \\] Una gráfica de barras consiste en una representación gráfica del conjunto: \\[ \\text{Barras} = \\{ (a_i, n_i) | a_i \\in C \\} \\] Gráficamente: Podemos crear una gráfica de barras con el comando geom_col para ello creemos unas barras correspondientes al tipo de delito (sólo en delitos que categoria_delito dice ROBO) haciendo una nueva base que cuente por delito: conteo_tipo &lt;- datos %&gt;% filter(str_detect(categoria_delito,&quot;ROBO&quot;)) %&gt;% group_by(delito) %&gt;% tally() Y hagamos la gráfica: ggplot(conteo_tipo) + geom_col(aes(x = delito, y = n), color = &quot;white&quot;) + theme_bw() Para evitar que se encime todo el texto podemos establecer un ángulo del mismo al usar theme: ggplot(conteo_tipo) + geom_col(aes(x = delito, y = n), color = &quot;white&quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 90, size = 3)) NOTA Una mala praxis es usar gráficas de pay pues es muy complicado contar una historia a partir de ellas. ¡No lo hagas! 2.9 Gráficas bivariadas 1. Gráfica de puntos (scatterplot) Dada una matriz de datos \\(Z\\) consideramos dos columnas numéricas \\(z_i\\) y \\(z_j\\) (\\(i \\neq j\\)) de dicha matriz. Sea \\(\\mathbb{X} = \\{ (z_{i,1}, z_{j,1}), (z_{i,2}, z_{j,2}), \\dots, (z_{i,n}, z_{j,n}) \\}\\) el conjunto de parejas ordenadas correspondientes a dichas columnas. Una gráfica de puntos consiste en la proyección de dichos puntos sobre \\(\\mathbb{R}^2\\). Para generarla en R podemos usar ggplot: ggplot(datos) + geom_point(aes(x = longitud, y = latitud), size = 1, color = &quot;purple&quot;, alpha = 0.2) donde los parámetros size establecen el tamaño del punto, color su color y alpha su nivel de transparencia (\\(0 \\leq \\alpha \\leq 1\\)). 2. Gráfica de líneas (lineplot) Dada una matriz de datos \\(Z\\) consideramos dos columnas numéricas \\(z_i\\) y \\(z_j\\) (\\(i \\neq j\\)) de dicha matriz. Sea \\(\\mathbb{X} = \\{ (z_{i,1}, z_{j,1}), (z_{i,2}, z_{j,2}), \\dots, (z_{i,n}, z_{j,n}) \\}\\) el conjunto de parejas ordenadas correspondientes a dichas columnas. Para evitar confusión de subíndices escribiré a las \\(z_i\\) como \\(x\\) y a las \\(z_j\\) como \\(y\\) de tal forma que \\(\\mathbb{X} = \\{ (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n) \\}\\) Supongamos, sin pérdida de generalidad que los datos están ordenados según las \\(x\\): \\(x_1 \\leq x_2 \\leq \\dots \\leq x_n\\). Sea \\(f\\) la función de interpolación lineal dada por: \\[ f(x) = \\begin{cases} y_1 + \\frac{y_2 - y_1}{x_2 - x_1} (x -x_1) &amp; \\text{ si } x_1 \\leq x \\leq x_2 \\\\ \\vdots \\\\ y_{k-1} + \\frac{y_k - y_{k-1}}{x_k - x_{k-1}} (x -x_{k-1}) &amp; \\text{ si } x_{k-1} \\leq x \\leq x_k \\\\ \\vdots \\\\ y_{n-1} + \\frac{y_{n} - y_{n-1}}{x_n - x_{n-1}} (x -x_{n-1}) &amp; \\text{ si } x_{n-1} \\leq x \\leq x_n \\\\ \\end{cases} \\] Una gráfica de líneas corresponde a la representación gráfica del conjunto \\[ \\textrm{Gr}_f = \\Big\\{ \\big(x, f(x)\\big) | x_1 \\leq x \\leq x_n \\Big\\} \\] De manera un poco más intuitiva notamos que si tenemos, por ejemplo, \\(\\mathbb{X} = \\{(x_1, y_1),(x_2, y_2), (x_3, y_3), (x_4, y_4)\\}\\) una gráfica de líneas se construye interpolando una línea entre \\((x_1, y_1)\\) y \\((x_2, y_2)\\), otra línea entre \\((x_2, y_2)\\) y \\((x_3, y_3)\\) y, finalmente, otra recta entre \\((x_3, y_3)\\) y \\((x_4, y_4)\\). Usando la ecuación de la línea \\[ y = \\frac{y_2 - y_1}{x_2 - x_1} (x - x_1) + y_1 \\] interpolamos cada uno de los puntos como en la gráfica siguiente: Para realizar una gráfica de líneas podemos usar de nuevo ggplot2 con la opción de geom_line: ggplot(conteo_delitos) + geom_line(aes(x = fecha, y = n)) Podemos cambiar el tema y agregar puntos de otro color para que nuestra gráfica se vea más bonita: ggplot(conteo_delitos) + geom_line(aes(x = fecha, y = n)) + geom_point(aes(x = fecha, y = n), color = &quot;red&quot;, size = 3) + theme_classic() + labs( x = &quot;Fecha de apertura de la carpeta de investigación&quot;, y = &quot;Cantidad de carpetas de investigación en FGJ&quot; ) Finalmente con geom_label podemos agregar anotaciones a nuestra gráfica: ggplot(conteo_delitos) + geom_line(aes(x = fecha, y = n)) + geom_point(aes(x = fecha, y = n), color = &quot;red&quot;, size = 3) + theme_classic() + labs( x = &quot;Fecha de apertura de la carpeta de investigación&quot;, y = &quot;Cantidad de carpetas de investigación en FGJ&quot; ) + geom_label(aes(x = dmy(&quot;25/12/2018&quot;), y = 425), label = &quot;Efecto de Navidad&quot;) 2.9.1 Ejercicio Utiliza las siguiente bases de datos para replicar exactamente el formato de las gráficas que se muestran abajo de las bases. No todo viene en estas notas, la idea es que investigues y para ello te sugiero consultar este libro Gráfica de barras datos.barras &lt;- data.frame(Pais = c(&quot;EEUU&quot;,&quot;Canadá&quot;,&quot;México&quot;), PIB = c(20.54, 17.13, 1.21)) Los colores usados son firebrick, deepskyblue3 y forestgreen: Línea x &lt;- seq(-2*pi, 2*pi, length.out = 100) datos.linea &lt;- data.frame(x = x, y = sin(x)) Boxplot x &lt;- c(1,10, 100, -2, 3, 5, 6, 12, -8, 31, 2, pi, 3) datos.linea &lt;- data.frame(Dientes = x) Puntos datos.arbol &lt;- data.frame(altura = c(1.7, 1.4, 1.8, 1.9, 1.5, 1.7, 1.6, 1.8, 1.7, 1.8), ancho = c(1.2, 1.4, 1.2, 1, 1.5, 1.7, 1.6, 1.2, 1.2, 1), tipo = c(&quot;Pino&quot;,&quot;Sauce&quot;,&quot;Sauce&quot;,&quot;Sauce&quot;,&quot;Pino&quot;, &quot;Pino&quot;,&quot;Pino&quot;,&quot;Sauce&quot;,&quot;Sauce&quot;,&quot;Sauce&quot;)) 2.10 Estadísticos bivariados NOTACIÓN Para esta sección vamos a considerar dos (vectores) columnas de la matriz de datos \\(Z\\) y los denominaremos \\(\\vec{x}\\) y \\(\\vec{y}\\) (en lugar de \\(z_i\\) y \\(z_j\\)). En particular, denotaremos \\(\\mathcal{X} = \\{ a_{i, x } | a_{i,x} \\in \\vec{x} \\}\\) el conjunto de valores únicos del vector \\(\\vec{x}\\) y \\(\\mathcal{Y} = \\{ a_{y ,j} | a_{y ,j} \\in \\vec{y} \\}\\) el conjunto de valores únicos de \\(\\vec{y}\\). La cardinalidad de dichos conjuntos es \\(\\ell_{x}\\) y \\(\\ell_{y}\\) respectivamente. Finalmente, definimos el conteo de cuántas veces aparece el valor \\(a_{i,x }\\) (respectivamente el \\(a_{y ,j}\\)) en los vectores \\(\\vec{x}\\) (respectivamente \\(\\vec{y}\\)) como: \\[\\begin{equation} \\begin{aligned} n_{i,x } &amp; = \\sum\\limits_{k=1}^{n} \\mathbb{I}_{\\{ a_{i,x } \\}}(x_k) \\\\ n_{y ,j} &amp; = \\sum\\limits_{k=1}^{n} \\mathbb{I}_{\\{ a_{y ,j} \\}}(y_k) \\end{aligned} \\end{equation}\\] para \\(1 \\leq i \\leq \\ell_{x}\\) y \\(1 \\leq j \\leq \\ell_{y}\\). Por poner un ejemplo, considera el siguiente conjunto de datos: x y Rojo Coche Azul Taza Verde Árbol Rojo Taza Verde Libro En este sentido el vector es \\(\\vec{x} = (\\text{Rojo},\\text{Azul},\\text{Verde},\\text{Rojo},\\text{Verde})^T\\) mientras que el conjunto de valores únicos asociados está dado por \\(\\mathcal{X} = \\{ \\text{Rojo},\\text{Azul},\\text{Verde} \\}\\). En este sentido (siguiendo el conjunto) se tiene que \\(a_{1,x} = \\text{Rojo}\\), \\(a_{2,x} = \\text{Azul}\\) y \\(a_{3,x } = \\text{Verde}\\) mientras que (siguiendo el vector) se observa \\(x_1 = \\text{Rojo}\\), \\(x_2 = \\text{Azul}\\), \\(x_3 = \\text{Verde}\\), \\(x_4 = \\text{Rojo}\\), \\(x_5 = \\text{Verde}\\). Finalmente notamos que el conteo de veces que aparece cada cosa es: \\(n_{1,x} = 2\\) (aparece el \\(a_{1, x}\\) que es rojo tres veces), \\(n_{2,x} = 1\\) y \\(n_{3,x } = 2\\) (el azul y verde dados por \\(a_{2,x}\\) y \\(a_{3,x}\\) respectivamente aparecen una vez para azul y dos veces para verde). Por otro lado, \\(\\vec{y} = (\\text{Coche},\\text{Taza},\\text{Árbol},\\text{Taza},\\text{Libro})^T\\) con su conjunto de valores únicos \\(\\mathcal{Y} = \\{ \\text{Coche},\\text{Taza},\\text{Árbol}, \\text{Libro} \\}\\). Para el caso de \\(\\vec{y}\\) se tiene que \\(y_1 = \\text{Coche}\\), \\(y_2 = \\text{Taza}\\), \\(y_3 = \\text{Árbol}\\), \\(y_4 = \\text{Taza}\\), \\(y_5 = \\text{Libro}\\) mientras que en el caso de valores únicos \\(a_{y, 1} = \\text{Coche}\\), \\(a_{y, 2} = \\text{Taza}\\), \\(a_{y, 3} = \\text{Árbol}\\), \\(a_{\\cdot, 4} = \\text{Libro}\\). Los conteos asociados son: \\(n_{y,1} = n_{y,3} = n_{y,4} = 1\\) (aparecen el coche, el árbol y el libro una vez) mientras que \\(n_{y,2} = 2\\) representa que la taza está dos veces. Por otro lado denotamos a la submatriz de \\(Z\\) compuesta solamente por las columnas \\(\\vec{x}\\) y \\(\\vec{y}\\) como: \\[ Z_{(x,y)} = \\begin{pmatrix} x_1 &amp; y_1 \\\\ x_2 &amp; y_2 \\\\ \\vdots &amp; \\vdots \\\\ x_n &amp; y_n \\\\ \\end{pmatrix} \\] Sea \\(\\mathcal{X}\\times\\mathcal{Y} = \\{ a_{i,j} = (x_i,y_j) | x_i \\in \\mathcal{X} \\quad \\&amp; \\quad y_j \\in \\mathcal{Y}\\}\\) el conjunto de observaciones únicas posibles de las parejas \\((x,y)\\) (todas las permutaciones). Finalmente, el conteo de cuántas veces aparece el vector bivariado \\(a_{i,j}\\) en los datos está dado por: \\[ n_{i,j} = \\sum\\limits_{k = 1}^{n} \\mathbb{I}_{\\{ a_{i,j} \\}}\\big( (x_k, y_k) \\big) \\] Lo anterior puede representarse en la tabla : En el ejemplo anterior, la tabla se vería como la . Una excelente referencia para esta sección es el capítulo 4 de Peck, Olsen, and Devore (2015). 1. Tabla de contingencia Para \\(\\vec{x}\\), \\(\\vec{y}\\) definidas como al inicio de la sección (y siguiendo la notación anterior), definimos una tabla de contingencia como la matriz \\(N_{x,y}\\) dada por: \\[ N_{x,y} = \\begin{pmatrix} n_{1,1} &amp; n_{1,2} &amp; \\dots &amp; n_{1, \\ell_y} \\\\ n_{2,1} &amp; n_{2,2} &amp; \\dots &amp; n_{2, \\ell_y} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ n_{\\ell_x,1} &amp; n_{\\ell_x,2} &amp; \\dots &amp; n_{\\ell_x, \\ell_y} \\\\ \\end{pmatrix} \\] Al vector \\(n_x = (n_{1,x}, n_{2,x}, \\dots, n_{\\ell_x,x})^T\\) se le conoce como distribución frecuencial (observada) marginal de \\(\\vec{x}\\) mientras que \\(n_y = (n_{y,1}, n_{y,2}, \\dots, n_{y,\\ell_y})^T\\) es la distribución frecuencial (observada) marginal de \\(\\vec{y}\\). Una tabla de contingencia representa el conteo de observaciones de una variable ajustado por la otra. Para crear una tabla de contingencia en R podemos usar el mismo comando table que ya usamos antes pero esta vez introduciendo dos vectores como en el siguiente ejemplo donde notamos alcaldía contra año del registo: table(datos$alcaldia_hechos, datos$ao_inicio) ## ## 2018 2019 ## VERACRUZ 0 1 ## VILLAGRAN 1 0 ## XALATLACO 2 0 ## XOCHIMILCO 465 115 ## XOCHITEPEC 1 0 ## ZACATECAS 0 2 Para agregar las distribuciones frecuenciales marginales a la tabla podemos usar el comando addmargins: addmargins(table(datos$alcaldia_hechos, datos$ao_inicio)) ## ## 2018 2019 Sum ## VILLAGRAN 1 0 1 ## XALATLACO 2 0 2 ## XOCHIMILCO 465 115 580 ## XOCHITEPEC 1 0 1 ## ZACATECAS 0 2 2 ## Sum 15952 3896 19848 2. Tabla de frecuencias Una tabla de frecuencia es la matriz \\(\\text{Freq}_{x,y}\\) dada por: \\[ \\text{Freq}_{x,y} = \\begin{pmatrix} f_{1,1} &amp; f_{1,2} &amp; \\dots &amp; f_{1, \\ell_y} \\\\ f_{2,1} &amp; f_{2,2} &amp; \\dots &amp; f_{2, \\ell_y} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ f_{\\ell_x,1} &amp; f_{\\ell_x,2} &amp; \\dots &amp; f_{\\ell_x, \\ell_y} \\\\ \\end{pmatrix} \\] donde \\(f_{i,j} = \\frac{n_{i,j}}{n}\\) representa la frecuencia relativa de la observación de \\((a_{i,x}, a_{y,j})\\) i.e. cuánto representa del total. Al vector \\(f_{x} = (f_{1,x}, f_{2,x}, \\dots, f_{\\ell_x,x})^T\\) se le conoce como la distribución frecuencial marginal relativa de \\(\\vec{x}\\). Análogamente para \\(y\\) se tiene la distribución frecuencial marginal relativa de \\(\\vec{y}\\) dada por: \\(f_{y} = (f_{y,1}, f_{y,2}, \\dots, f_{y,\\ell_y})^T\\). Las entradas de dichos vectores son de la forma \\(f_{i,x} = n_{i,x}/n\\) y \\(f_{y,j} = n_{y,j}/n\\). En R podemos obtener las frecuencias mediante prop.table: prop.table(table(datos$alcaldia_hechos, datos$ao_inicio)) ## ## 2018 2019 ## VERACRUZ 0.000000e+00 5.038291e-05 ## VILLAGRAN 5.038291e-05 0.000000e+00 ## XALATLACO 1.007658e-04 0.000000e+00 ## XOCHIMILCO 2.342805e-02 5.794035e-03 ## XOCHITEPEC 5.038291e-05 0.000000e+00 ## ZACATECAS 0.000000e+00 1.007658e-04 Así mismo, podemos agregar las marginales: addmargins(prop.table(table(datos$alcaldia_hechos, datos$ao_inicio))) ## ## 2018 2019 Sum ## VILLAGRAN 5.038291e-05 0.000000e+00 5.038291e-05 ## XALATLACO 1.007658e-04 0.000000e+00 1.007658e-04 ## XOCHIMILCO 2.342805e-02 5.794035e-03 2.922209e-02 ## XOCHITEPEC 5.038291e-05 0.000000e+00 5.038291e-05 ## ZACATECAS 0.000000e+00 1.007658e-04 1.007658e-04 ## Sum 8.037082e-01 1.962918e-01 1.000000e+00 3. Riesgo Relativo (discreto) Para definir Riesgo Relativo empezaremos por un ejemplo. Tomamos la tabla donde se guardó un registro de personas según si fumaban o no así como si dichas personas desarrollaron o no enfisema pulmonar. Si quisiéramos analizar la hipótesis de que FUMAR está asociado con ENFISEMA tendríamos que ver, dentro de la población de fumadores (FUMAR = SÍ) cuántos hay (proporcionalmente) con ENFISEMA. La hipótesis es que si no hubiera relación, saldría que las proporciones de fumadores con y sin enfisema serían \\(50\\%\\) cada una. La proporción de fumadores con enfisema está dada por \\(100/130\\) mientras que la de no fumadores con enfisema es \\(40/90\\). El riesgo relativo (intuitivamente). se define como la división entre ambas proporciones: \\[ \\text{Riesgo Relativo de Enfisema} = \\dfrac{\\frac{\\text{Expuestos enfermos}}{\\text{Total de expuestos}}}{\\frac{\\text{No Expuestos enfermos}}{\\text{Total de no expuestos}}} = \\dfrac{100/130}{40/90} \\approx 1.73 \\] Lo que se interpreta como que los fumadores tienen \\(1.73\\) veces más riesgo de desarrollar enfisema que los no fumadores ya que si despejamos de la fórmula anterior: \\[ \\frac{\\text{Expuestos enfermos}}{\\text{Total de expuestos}} \\approx 1.73 \\times \\frac{\\text{No Expuestos enfermos}}{\\text{Total de no expuestos}} \\] De manera general, dadas dos vectores lógicos \\(\\vec{x}\\) (interpretada como el resultado) y \\(\\vec{y}\\) (interpretada como la exposición) con una tabla de contingencia y frecuencias marginales dadas por la tabla , definimos el riesgo relativo de \\(\\vec{x}\\) dado \\(\\vec{y}\\) como: \\[ RR(\\vec{x}|\\vec{y}) = \\dfrac{\\frac{a}{a + c}}{\\frac{b}{b + d}} \\] Mientras que el riesgo relativo de no \\(\\vec{x}\\) dado \\(\\vec{y}\\) está dado por: \\[ RR(\\neg \\vec{x}|\\vec{y}) = \\dfrac{\\frac{c}{a + c}}{\\frac{d}{b + d}} \\] La base de datos de los delitos no contiene información suficiente para poder calcular un riesgo relativo pero podemos crear la base de datos correspondiente a la tabla como sigue: fumadores &lt;- data.frame(SI_FUMA = c(100, 30), NO_FUMA =c(40, 50)) Podemos agregar nombres a las filas para tener la base de datos mejor: rownames(fumadores) &lt;- c(&quot;ENFISEMA&quot;,&quot;NO_ENFISEMA&quot;) La tabla se ve así: fumadores ## SI_FUMA NO_FUMA ## ENFISEMA 100 40 ## NO_ENFISEMA 30 50 Luego el riesgo relativo de ENFISEMA está dado por: numerador &lt;- fumadores[&quot;ENFISEMA&quot;,&quot;SI_FUMA&quot;]/sum(fumadores$SI_FUMA) denominador &lt;- fumadores[&quot;ENFISEMA&quot;,&quot;NO_FUMA&quot;]/sum(fumadores$NO_FUMA) rr &lt;- numerador/denominador #El riesgo relativo rr ## [1] 1.730769 Por otro lado, el riesgo relativo de no enfisema es: numerador &lt;- fumadores[&quot;NO_ENFISEMA&quot;,&quot;SI_FUMA&quot;]/sum(fumadores$SI_FUMA) denominador &lt;- fumadores[&quot;NO_ENFISEMA&quot;,&quot;NO_FUMA&quot;]/sum(fumadores$NO_FUMA) rr_neg &lt;- numerador/denominador #El riesgo relativo rr_neg ## [1] 0.4153846 Éste último se interpreta como si la proporción de individuos sin enfisema es \\(0.41\\) veces más pequeña entre fumadores que no fumadores. 4. Razón de momios (discreto) Para dos vectores lógicos \\(\\vec{x}\\) y \\(\\vec{y}\\) definimos la razón de momios como: \\[ \\textrm{OR}(\\vec{x}|\\vec{y}) =\\dfrac{RR(\\vec{x}|\\vec{y})}{RR(\\neg\\vec{x}|\\vec{y})} \\] Podemos calcular en R la razón de momios a partir de los datos: razon.momios &lt;- rr/rr_neg donde la razón de momios de 4.17 se interpreta como “si un individuo tiene enfisema, la factibilidad de que dicho individuo sea fumador es 4.17 veces más alta”. Esta interpretación se obtiene a partir de un despeje y sustitución: \\[\\begin{equation}\\nonumber \\begin{aligned} RR(\\vec{x}|\\vec{y}) &amp; = 4.16 \\cdot RR(\\neg\\vec{x}|\\vec{y}) \\\\ \\\\ \\Leftrightarrow \\dfrac{\\frac{\\text{Expuestos enfermos}}{\\text{Total de expuestos}}}{\\frac{\\text{No Expuestos enfermos}}{\\text{Total de no expuestos}}} &amp; = 4.16 \\cdot \\dfrac{\\frac{\\text{Expuestos no enfermos}}{\\text{Total de expuestos}}}{\\frac{\\text{No Expuestos no enfermos}}{\\text{Total de no expuestos}}} \\\\ \\\\ \\Leftrightarrow \\frac{\\text{Expuestos enfermos}}{\\text{No Expuestos enfermos}} &amp; = 4.16\\cdot \\frac{\\text{Expuestos no enfermos}}{\\text{No Expuestos no enfermos}} \\\\ \\\\ \\Leftrightarrow \\frac{\\text{Expuestos enfermos}}{\\text{Expuestos no enfermos}} &amp; = 4.16\\cdot \\frac{\\text{No Expuestos enfermos}}{\\text{No Expuestos no enfermos}} \\end{aligned} \\end{equation}\\] 5. Correlación (Bravais-Pearson) Sean \\(\\vec{x}\\) y \\(\\vec{y}\\) dos vectores columa numéricos de nuestra matriz de datos \\(Z\\). Tomemos \\(\\tilde{x} = (x_1 - \\bar{x}, x_2 - \\bar{x}, \\dots, x_n - \\bar{x})\\) la versión centrada de \\(\\vec{x}\\) y \\(\\tilde{y} = (y_1 - \\bar{y}, y_2 - \\bar{y}, \\dots, y_n - \\bar{y})\\) la versión centrada de \\(\\vec{y}\\). Al coseno entre dichos vectores (bajo el producto punto) se le conoce como correlación de Bravais-Pearson y se le denota \\(\\rho_{\\vec{x},\\vec{y}}\\). Es decir: \\[ \\rho_{\\vec{x},\\vec{y}} = \\cos(\\tilde{x},\\tilde{y}) = \\dfrac{\\tilde{x} \\cdot \\tilde{y}}{\\|\\tilde{x}\\| \\cdot \\|\\tilde{y}\\|} \\] donde \\(\\tilde{x}\\cdot\\tilde{y} = \\sum_{i=1}^{n} (x_i - \\bar{x}) \\cdot (y_i - \\bar{y})\\) representa el producto de los vectores \\(\\tilde{x}\\) y \\(\\tilde{y}\\) y se conoce como covarianza entre \\(\\vec{x}\\) y \\(\\vec{y}\\). Por otro lado, \\[ \\|\\tilde{x}\\| = \\sqrt{\\sum\\limits_{i=1}^{n} (x_i - \\bar{x})^2} = \\sigma_{\\vec{x}} \\] Por tanto la correlación también puede medirse como: \\[ \\rho_{\\vec{x},\\vec{y}} = \\cos(\\tilde{x},\\tilde{y}) = \\frac{1}{\\sigma_{\\vec{y}} \\sigma_{\\vec{x}}}\\sum\\limits_{i=1}^{n} (x_i - \\bar{x}) \\cdot (y_i - \\bar{y}) \\] Para matriz de datos \\(Z\\) con \\(\\ell\\) columnas, definimos la matriz de correlaciones \\(\\mathcal{C}\\) como la matriz dada por: \\[ \\mathcal{C} = \\begin{pmatrix} \\rho(z_1,z_1) &amp; \\rho(z_1,z_2) &amp; \\dots &amp; \\rho(z_1,z_{\\ell}) \\\\ \\rho(z_2,z_1) &amp; \\rho(z_2,z_2) &amp; \\dots &amp; \\rho(z_2,z_{\\ell}) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\rho(z_{\\ell},z_1) &amp; \\rho(z_{\\ell},z_2) &amp; \\dots &amp; \\rho(z_{\\ell},z_{\\ell}) \\end{pmatrix} \\] Donde notamos (demuestra) que \\(\\rho(z_i, z_i) = 1\\). Podemos usar la base mtcars precargada en R para analizar las correlaciones: data(mtcars) datos.coches &lt;- mtcars La base está explicada en la ayuda de R: ?mtcars Podemos obtener la correlación entre el número de millas por galón mpg y el peso del automóvil wt haciendo: cor(datos.coches$mpg, datos.coches$wt, method = &quot;pearson&quot;) ## [1] -0.8676594 Esta correlación se interpreta como que por cada aumento en el peso corresponde una disminución en las millas por galón. Podemos ver gráficamente que esto es así: ggplot(datos.coches) + geom_point(aes(x = wt, y = mpg), color = &quot;purple&quot;) + theme_bw() Para obtener toda la matriz de correlaciones de la base podemos tomar cor aplicado a toda la base de datos: cor(datos.coches, method = &quot;pearson&quot;) ## mpg cyl disp hp drat wt ## mpg 1.0000000 -0.8521620 -0.8475514 -0.7761684 0.68117191 -0.8676594 ## cyl -0.8521620 1.0000000 0.9020329 0.8324475 -0.69993811 0.7824958 ## disp -0.8475514 0.9020329 1.0000000 0.7909486 -0.71021393 0.8879799 ## hp -0.7761684 0.8324475 0.7909486 1.0000000 -0.44875912 0.6587479 ## drat 0.6811719 -0.6999381 -0.7102139 -0.4487591 1.00000000 -0.7124406 ## wt -0.8676594 0.7824958 0.8879799 0.6587479 -0.71244065 1.0000000 ## qsec 0.4186840 -0.5912421 -0.4336979 -0.7082234 0.09120476 -0.1747159 ## vs 0.6640389 -0.8108118 -0.7104159 -0.7230967 0.44027846 -0.5549157 ## am 0.5998324 -0.5226070 -0.5912270 -0.2432043 0.71271113 -0.6924953 ## gear 0.4802848 -0.4926866 -0.5555692 -0.1257043 0.69961013 -0.5832870 ## carb -0.5509251 0.5269883 0.3949769 0.7498125 -0.09078980 0.4276059 ## qsec vs am gear carb ## mpg 0.41868403 0.6640389 0.59983243 0.4802848 -0.55092507 ## cyl -0.59124207 -0.8108118 -0.52260705 -0.4926866 0.52698829 ## disp -0.43369788 -0.7104159 -0.59122704 -0.5555692 0.39497686 ## hp -0.70822339 -0.7230967 -0.24320426 -0.1257043 0.74981247 ## drat 0.09120476 0.4402785 0.71271113 0.6996101 -0.09078980 ## wt -0.17471588 -0.5549157 -0.69249526 -0.5832870 0.42760594 ## qsec 1.00000000 0.7445354 -0.22986086 -0.2126822 -0.65624923 ## vs 0.74453544 1.0000000 0.16834512 0.2060233 -0.56960714 ## am -0.22986086 0.1683451 1.00000000 0.7940588 0.05753435 ## gear -0.21268223 0.2060233 0.79405876 1.0000000 0.27407284 ## carb -0.65624923 -0.5696071 0.05753435 0.2740728 1.00000000 Finalmente, el paquete ggcorrplot puede ayudarnos a visualizar gráficamente dicha matriz: ggcorrplot(cor(datos.coches, method = &quot;pearson&quot;), lab = TRUE, type = &quot;upper&quot;) + labs(title = &quot;Matriz de Correlaciones&quot;) Una correlación de Pearson igual a \\(1\\) ó \\(-1\\) se interpreta como que hay una relación lineal perfecta mientras que una correlación igual a \\(0\\) se interpreta como que no hay relación lineal (aunque puede existir de otro tipo) #Ejemplo de correlación lineal perfecta x &lt;- seq(0,4, length.out = 9) y &lt;- 2*x + 1 Gráficamente: El valor en este caso de la correlación es: cor(x,y, method = &quot;pearson&quot;) ## [1] 1 Mientras que por otro lado podemos tener variables relacionadas pero sin correlación : #Ejemplo sin correlación lineal pero con variables relacionadas x &lt;- seq(-4, 4,length.out = 9) y &lt;- x^2 cor(x,y, method = &quot;pearson&quot;) ## [1] 0 6. Correlación de rango de Spearman Para hablar de la correlación de rango de Spearman es necesario definir una variable como ordinal. Un vector \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) de variables numéricas o categóricas es ordinal si existe una relación \\(\\leq\\) de orden total sobre los elementos del vector tal que: Es antisimétrica: si \\(x_i \\leq x_j\\) y \\(x_j \\leq x_i\\) entonces \\(x_i = x_j\\). Es transitiva: si \\(x_i \\leq x_j\\) y \\(x_j \\leq x_k\\) entonces \\(x_i \\leq x_k\\). Es conexa: \\(x_i \\leq x_j\\) ó \\(x_j \\leq x_i\\). De manera intuitiva un vector es ordinal si hay un orden para sus entradas. Por ejemplo, cuando calificas un servicio como Malo \\(\\leq\\) Regular \\(\\leq\\) Bueno o bien cuando se compara nivel educativo (en términos de años) Primaria \\(\\leq\\) Secundaria \\(\\leq\\) Preparatoria \\(\\leq\\) Educación superior. Toda variable numérica es ordinal. Para un vector ordinal definimos su ordenamiento como \\(x_{(1)} = \\min \\{ x_1, x_2, \\dots, x_n \\}\\) y \\(x_{(j)} = \\min \\{ x_1, x_2, \\dots, x_n \\} \\setminus \\{ x_{(1)}, x_{(2)}, \\dots, x_{(j-1)} \\}\\) de tal forma que \\(x_{(1)} \\leq x_{(2)} \\leq \\dots \\leq x_{(n)}\\). El rango de \\(x_{(j)}\\) denotado como \\(R(x_{(j)})\\) es \\(j\\) (su posición en el ordenamiento). Es decir: \\[ R(x_i) = j \\Leftrightarrow x_i = x_{(j)} \\] Dado un vector \\(\\vec{x}\\) definimos su vector de rango como: \\[ R(\\vec{x}) = \\big( R(x_1), R(x_2), \\dots, R(x_n) )^T \\] Para dos variables ordinales, \\(\\vec{x}\\) y \\(\\vec{y}\\) se define la correlación de rango de Spearman como la correlación de Pearson entre sus vectores de rangos: \\[ \\rho_{\\text{Spearman}} =\\rho\\big( R(\\vec{x}), R(\\vec{y})) \\] Mientras que la correlación de Pearson mide linealidad; la de Spearman mide monotonicidad (que si una aumenta la otra también; que si una disminuye la otra también). #Comparativo de correlaciones: la de Pearson no encuentra mucha línea x &lt;- seq(0.1, 1, length.out = 25) y &lt;- exp(1/x^2) En este caso la correlación de Pearson es muy mala: cor(x,y, method = &quot;pearson&quot;) ## [1] -0.3396831 Mientras que la de Spearman sí muestra la relación: cor(x,y, method = &quot;spearman&quot;) ## [1] -1 7. \\(\\tau\\) de Kendall Consideremos \\(\\vec{x}\\) y \\(\\vec{y}\\) dos vectores columna ordinales de una matriz de datos \\(Z\\). Para cualquier par de observaciones \\((x_i, y_i)\\) y \\((x_j, y_j)\\) con \\(i &lt; j\\) decimos que dos observaciones son concordantes (\\(c\\)) si los rangos de ambas \\(x\\) y \\(y\\) coinciden; es decir si se cumple una de las siguientes: \\(R(x_i) &lt; R(x_j)\\) y \\(R(y_i) &lt; R(y_j)\\) o bien, \\(R(x_i) &gt; R(x_j)\\) y \\(R(y_i) &gt; R(y_j)\\). Observaciones discordantes (\\(d\\)) ocurren cuando los rangos de las \\(x\\) y las \\(y\\) son inversos el uno del otro; es decir, se cumple una de las siguientes: \\(R(x_i) &gt; R(x_j)\\) y \\(R(y_i) &lt; R(y_j)\\) o bien, \\(R(x_i) &lt; R(x_j)\\) y \\(R(y_i) &gt; R(y_j)\\). En el caso que cualquiera de las dos, \\(x\\) ó \\(y\\) sean igualdades (\\(x_i = x_j\\) ó \\(y_i = y_j\\)) no son discordantes ni concordantes. Observa que existen \\(\\binom{n}{2}\\) distintos pares de \\((x_i,y_i)\\) y \\((x_j,y_j)\\) para comparar. Sea \\(c_{\\vec{x},\\vec{y}}\\) la cantidad de pares concordantes y \\(d_{\\vec{x},\\vec{y}}\\) la cantidad de pares discordantes. Luego la probabilidad de que dos pares seleccionados de manera uniforme sean concordantes es: \\[ \\dfrac{c_{\\vec{x},\\vec{y}}}{\\binom{n}{2}} \\] mientras que la probabilidad de que dos pares seleccionados uniformemente sean discordantes es: \\[ \\dfrac{d_{\\vec{x},\\vec{y}}}{\\binom{n}{2}}. \\] Definimos entonces la \\(\\tau\\) de Kendall como la diferencia entre ambas probabilidades empíricas: \\[ \\tau_{\\vec{x},\\vec{y}} = \\dfrac{c_{\\vec{x},\\vec{y}} - d_{\\vec{x},\\vec{y}}}{\\binom{n}{2}} \\] La tau de Kendall cumple que: \\[ -1 \\leq \\tau_{\\vec{x},\\vec{y}} \\leq 1 \\] donde el \\(-1\\) se alcanza sólo si son completamente discordantes (el rango de \\(x\\) es el inverso del rango de las \\(y\\)) y el \\(1\\) si son completamente concordantes (el rango de \\(x\\) y de \\(y\\) tienen el mismo orden). Una \\(\\tau\\) cercana a cero se interpreta como ausencia de relación en los rangos. Podemos aplicar la tau de Kendall a la siguiente base de datos que contiene la calificación de dos servicios de un restaurante: calidad_alimentos calidad_servicio Malo 1 estrella Bueno 4 estrellas Bueno 5 estrellas Regular 2 estrellas Bueno 5 estrellas Bueno 4 estrellas Para ello codificamos las variables como factor diciéndole que son variables ordinales order = TRUE e indicando el orden de los niveles: calidad_alimentos &lt;- factor(c(&quot;Malo&quot;,&quot;Bueno&quot;,&quot;Bueno&quot;,&quot;Regular&quot;,&quot;Bueno&quot;,&quot;Bueno&quot;), order = TRUE, levels = c(&quot;Malo&quot;,&quot;Regular&quot;,&quot;Bueno&quot;)) calidad_servicio &lt;- factor(c(&quot;1 estrella&quot;, &quot;4 estrellas&quot;, &quot;5 estrellas&quot;, &quot;2 estrellas&quot;, &quot;5 estrellas&quot;,&quot;4 estrellas&quot;), order = TRUE, levels = c(&quot;1 estrella&quot;,&quot;2 estrellas&quot;,&quot;3 estrellas&quot;, &quot;4 estrellas&quot;,&quot;5 estrellas&quot;)) Esto de las variables ordinales permite hacer comparaciones ordinales, por ejemplo: calidad_alimentos[2] &gt; calidad_alimentos[4] ## [1] TRUE Los datos se ven así: Finalmente, calculamos la \\(\\tau\\) de Kendall, para ello es necesario obtener el rango de nuestras variables ordinales: rango_alimentos &lt;- as.numeric(calidad_alimentos) rango_servicio &lt;- as.numeric(calidad_servicio) cor(rango_alimentos, rango_servicio, method = &quot;kendall&quot;) ## [1] 0.8320503 Lo cual indica que hay una relación entre la calificación de calidad de alimentos y la del servicio. 8. Ajuste de modelo lineal Sean \\(\\vec{x}\\) y \\(\\vec{y}\\) dos vectores columna de una matriz de datos \\(Z\\). Supongamos, además, se tiene la hipótesis de que existe una relación afín entre los vectores; es decir que: \\[ \\vec{y} \\approx \\beta_1 \\vec{x} + \\beta_0 \\vec{1} \\] donde \\(\\vec{1} = (1, 1, \\dots, 1)^T\\) es un vector con todas las entradas idénticas a \\(1\\) y \\(\\beta_0, \\beta_1 \\in \\mathbb{R}\\). Algunas razónes para tener esta hipótesis podría ser una correlación de Pearson cercana a \\(\\pm 1\\) o por inspección gráfica. Esta hipótesis implica que: \\[ y \\approx \\underbrace{\\beta_1 x + \\beta_0}_{\\hat{y}} \\] Podemos entonces trazar la línea \\(y = \\beta_0 + \\beta_1 x\\) y graficar contra los puntos \\(\\{(x_i,y_i)\\}_{i=1}^{n}\\), Si la línea no ajusta perfecto tendremos errores \\(e_i = (y_i - \\hat{y}_i)^2\\) de predicción las cuales representan la diferencia entre la \\(y\\) observada (\\(y_i\\)) y la \\(y\\) predicha por la línea \\(\\hat{y}_i = \\beta_1 x_i + \\beta_0\\). La suma de estos errores es: \\[ \\textrm{SSR}(\\beta_0, \\beta_1) = \\sum\\limits_{i=1}^{n} e_i = \\sum\\limits_{i=1}^{n} \\big( y_i - \\hat{y}_i \\big)^2 = \\sum\\limits_{i=1}^{n} \\big( y_i - (\\beta_1 x_i + \\beta_0) \\big)^2 \\] El nombre de SSR es por ( Sum of Squared Residuals ) dado que en estadística se define un residual como \\(r_i = (y_i - \\hat{y}_i)\\) Gráficamente: Lo que se busca entonces es minimizar el error respecto a las constantes a determinar: \\(\\beta_0\\) y \\(\\beta_1\\). Para ello buscamos un punto de inflexión derivando: \\[ \\dfrac{\\partial\\textrm{SSR}}{\\partial \\beta_0} = \\sum\\limits_{i=1}^{n} 2\\big(y_i - (\\beta_1 x_i + \\beta_0 ) \\big) = 0 \\] De donde se sigue que: \\[ \\sum\\limits_{i=1}^n y_i - \\beta_1 \\sum\\limits_{i=1}^{n} x_i - n \\beta_0 = 0 \\Rightarrow \\beta_0 = \\dfrac{1}{n} \\sum\\limits_{i=1}^n y_i - \\beta_1 \\dfrac{1}{n} \\sum\\limits_{i=1}^n x_i \\Rightarrow \\bar{y} - \\beta_1 \\bar{x}, \\] de donde concluimos que de cumplirse la relación lineal se tiene que: \\[ \\beta_0 = \\bar{y} - \\beta_1 \\bar{x}. \\] Por otro lado, la derivada respecto a \\(\\beta_1\\) es: \\[ \\dfrac{\\partial\\textrm{SSR}}{\\partial \\beta_1} = - \\sum\\limits_{i=1}^{n} 2\\big(y_i - (\\beta_1 x_i + \\beta_0 ) \\big) \\cdot x_i = 0 \\] De donde se sigue (si suponemos que existe al menos un \\(x_i \\neq 0\\)): \\[\\begin{equation}\\nonumber \\begin{aligned} 0 &amp; = - \\sum\\limits_{i=1}^n \\Big( x_i y_i - \\beta_1 x_i^2 - \\underbrace{\\beta_0}_{\\bar{y} - \\beta_1 \\bar{x}} x_i \\Big) \\\\ &amp; = \\sum\\limits_{i=1}^n \\Big( x_i y_i - \\beta_1 x_i^2 - \\bar{y}x_i - \\beta_1 \\bar{x} x_i \\Big) \\\\ &amp; = \\sum\\limits_{i=1}^n \\Big( y_i + \\beta_1 x_i - \\bar{y} - \\beta_1 \\bar{x} \\Big) x_i \\\\ &amp; = \\sum\\limits_{i=1}^n \\Big( y_i - \\bar{y} \\Big) x_i - \\beta_1 \\sum\\limits_{i=1}^n\\Big( x_i - \\bar{x} \\Big) x_i \\end{aligned} \\end{equation}\\] de donde se sigue (suponiendo que existen \\(i,j\\) tales que \\(x_i \\neq x_j\\) que: \\[ \\beta_1 = \\dfrac{\\sum_{i=1}^n \\Big( y_i - \\bar{y} \\Big)x_i}{\\sum_{i=1}^n\\Big( x_i - \\bar{x} \\Big)x_i} = \\dfrac{\\sum_{i=1}^n \\Big( y_i - \\bar{y} \\Big)\\Big( x_i - \\bar{x} \\Big)}{\\sum_{i=1}^n\\Big( x_i - \\bar{x} \\Big)^2} = \\dfrac{\\sigma_{\\vec{x}} \\cdot \\sigma_{\\vec{y}} \\cdot \\rho_{\\vec{x},\\vec{y}}}{n \\sigma_{\\vec{x}}^2} \\] por lo cual: \\[ \\beta_1 = \\dfrac{\\sigma_{\\vec{y}}}{\\sigma_{\\vec{x}}} \\cdot \\dfrac{\\rho_{\\vec{x},\\vec{y}}}{n} \\] De donde se tienen las fórmulas para el \\(\\beta_0\\) y \\(\\beta_1\\). 2.10.1 Ejercicio Demuestra la igualdad que usamos anteriormente: \\[ \\dfrac{\\sum_{i=1}^n \\Big( y_i - \\bar{y} \\Big) x_i}{\\sum_{i=1}^n\\Big( x_i - \\bar{x} \\Big)x_i} = \\dfrac{\\sum_{i=1}^n \\Big( y_i - \\bar{y} \\Big)\\Big( x_i - \\bar{x} \\Big)}{\\sum_{i=1}^n\\Big( x_i - \\bar{x} \\Big)^2} \\] En R podemos ajustar un modelo lineal para dos variables de una base de datos con lm: modelo.lineal &lt;- lm(mpg ~ wt, data = datos.coches) coef(modelo.lineal) ## (Intercept) wt ## 37.285126 -5.344472 Gráficamente podemos ver el modelo: ggplot(datos.coches) + geom_point(aes(y = mpg, x = wt)) + geom_smooth(aes(y = mpg, x = wt), method = &quot;lm&quot;, formula = y ~ x, se = FALSE) + theme_minimal() Para predecir, dada una nueva observación, cuál debe haber sido el valor de \\(\\hat{y}\\) para una nueva observación \\(x_*\\) (o varias nuevas observaciones) puede usarse la función predict datos_a_predecir &lt;- data.frame(wt = c(5.5, 6, 6.5)) predict(modelo.lineal, datos_a_predecir) ## 1 2 3 ## 7.890533 5.218297 2.546061 Hay que tener mucho cuidado con la generalización de un modelo lineal como los siguientes valores muestran: datos_a_predecir &lt;- data.frame(wt = c(7,8,9)) predict(modelo.lineal, datos_a_predecir) ## 1 2 3 ## -0.1261748 -5.4706464 -10.8151180 O bien el siguiente comic de xkcd: Para hacer la extrapolación gráfica podemos agregar un fullrange = TRUE combinado con un xlim ggplot(datos.coches) + geom_point(aes(y = mpg, x = wt)) + geom_smooth(aes(y = mpg, x = wt), method = &quot;lm&quot;, formula = y ~ x, se = FALSE, fullrange=TRUE) + theme_minimal() + xlim(c(1,6.5)) 2.11 Ejercicio Generaliza el proceso de estimación para cuando se tiene un polinomio \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\) Utiliza los datos confirmados de COVID-19 a nivel nacional (sólo los confirmados) disponibles en este link. Ajusta un modelo cuadrático (en el lm la fórmula ahora es del estilo de y ~ poly(x,2) ) y predice cuántos casos confirmados habrá el 29 de junio. Grafica tu ajuste así como tu predicción en la misma gráfica. 9. Ajuste general Podemos generalizar el ajuste de mínimos cuadrados planteando el modelo \\(y = f(x, \\vec{\\beta})\\) donde \\(x\\) puede ser una matriz y \\(\\vec{\\beta}\\) es un vector de parámetros. Supondremos que \\(f\\) es diferenciable en \\(\\vec{\\beta}\\). Como ejemplo, en el caso del ajuste lineal: \\[ y = f(x, \\vec{\\beta}) = \\beta_0 + \\beta_1 x \\qquad \\text{con} \\qquad \\vec{\\beta}= (\\beta_0,\\beta_1)^T. \\] o bien podríamos pensar en un ajuste polinomial: \\[ y = f(x, \\vec{\\beta}) = \\sum\\limits_{i = 0}^n \\beta_i x^i \\qquad \\text{con} \\qquad \\vec{\\beta}= (\\beta_0,\\beta_1, \\dots, \\beta_n)^T. \\] No tiene que ser un polinomio, \\(f\\) puede ser lo que ella quiera ser siempre y cuando sea diferenciable en los parámetros: \\[ y = f(x, \\vec{\\beta}) = \\Bigg[\\cos(\\beta_0 + x) + \\int\\limits_{0}^{\\beta_1 x} e^{-t^2} dx \\Bigg] \\cdot \\beta_2 \\ln(x) \\qquad \\text{con} \\qquad \\vec{\\beta}= (\\beta_0,\\beta_1, \\beta_3)^T. \\] 2.12 Ajuste funcional Hacemos una apuesta por teléfono. Yo voy a tirar una moneda \\(10\\) veces y si salen más Soles que Águilas yo gano 50 pesos. Si salen más Águilas que Soles tú ganas la misma cantidad. Al realizar el ejercicio yo te comunico que salieron en total \\(10\\) Soles y por tanto me debes el dinero. ¿Sospecharías algo de mí? Si no hablamos de probabilidad no hay forma en la que se pueda justificar que aparentemente hay algo raro con la moneda. Claro, siempre puede ser un caso improbable (hay gente que lo ha hecho) pero es raro que me hayan salido tantos Soles. Para cuantificar qué tan raro es el evento podemos suponer que las monedas siguen un modelo Binomial con parámetro \\(p = 1/2\\) y en este caso \\(n = 10\\) (fueron 10 tiros). La probabilidad de que haya obtenido \\(10\\) soles bajo este modelo es de: dbinom(10,10, 1/2) ## [1] 0.0009765625 ¡Rarísimo! Este resultado te haría sospechar que quizá mi moneda no es justa y no se obtienen la misma cantidad de Águilas que Soles cuando la tiro. Esto porque, aparentemente, en mi moneda la probabilidad de Sol debería de ser \\(p = 1\\) (por tu triste experiencia). Si por ejemplo en el onceavo tiro saliera un Águila, concluirías que, en mi moneda, aparentemente, la probabilidad de Sol es \\(p = \\frac{10}{11}\\). Por supuesto, entre más tiros y más información obtienes, mejor podrás caracterizar la moneda y con mayor sustento tendrás sospechas (o no) de que mi moneda es tramposa. Formalmente, en el ejemplo anterior, lo que se hace es suponer que existe una variable aleatoria \\(X \\in\\{ \\text{Águila}, \\text{Sol}\\}\\) (el resultado de la moneda) de la cual observamos \\(n = 11\\) realizaciones codificadas en el siguiente vector: \\[ \\vec{x} = \\big( \\text{Sol}, \\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Águila}\\big)^T \\] Aproximamos entonces la probabilidad \\(\\mathbb{P}(X = \\text{Sol})\\) mediante: \\[ \\mathbb{P}(X = \\text{Sol}) \\approx \\dfrac{1}{n}\\sum\\limits_{i=1}^n \\mathbb{I}_{\\{\\text{Sol}\\}}(x_i) = \\dfrac{10}{11} \\] Mientras que la de Águila se aproxima mediante: \\[ \\mathbb{P}(X = \\text{Águila}) \\approx \\dfrac{1}{n}\\sum\\limits_{i=1}^n \\mathbb{I}_{\\{\\text{Águila}\\}}(x_i) = \\dfrac{1}{11} \\] Para ver que éstas son buenas aproximaciones, podemos considerar un vector aleatorio de los posibles datos observados: \\[ \\vec{X} = (X_1, X_2, \\dots, X_{11})^T \\] Donde \\(X_1\\) es una variable aleatoria que representa lo que pudo haber salido en el primer tiro, \\(X_2\\) es una v.a. que representa lo que pudo haber salido en el segundo tiro y en general \\(X_k\\) es una v.a. que representa lo que pudo haber salido en el \\(k\\)-ésimo tiro. Suponiendo que la moneda tiene una probabilidad \\(p\\) de arrojar Sol y \\(1-p\\) de arrojar Águila, notamos que las variables indicadoras evaluadas en las \\(X_i\\) (aleatorias) son variables aleatorias \\[ \\mathbb{I}_{\\{\\text{Sol}\\}}(X_i) \\sim \\text{Beroulli}(p) \\] y que por tanto \\[ \\hat{p} = \\dfrac{1}{n}\\sum\\limits_{i=1}^n \\mathbb{I}_{\\{\\text{Sol}\\}}(X_i) \\] es una variable aleatoria (al ser suma de variables aleatorias). Podemos entonces calcular su valor esperado: \\[ \\mathbb{E}\\big[\\hat{p}\\big] = \\mathbb{E}\\bigg[\\dfrac{1}{n}\\sum\\limits_{i=1}^n \\mathbb{I}_{\\{\\text{Sol}\\}}(X_i)\\bigg] = \\dfrac{1}{n}\\sum\\limits_{i=1}^n \\mathbb{E}\\big[ \\mathbb{I}_{\\{\\text{Sol}\\}}(X_i)\\big] = \\dfrac{1}{n}\\sum\\limits_{i=1}^n p = \\dfrac{1}{n}\\cdot np = p \\] Es decir, que en promedio el estimador \\(\\hat{p}\\) va a atinarle al verdadero valor \\(p\\). Esto lo podemos ver si hacemos nsim\\(= 1000\\) simulaciones de \\(100\\) tiros de una moneda con probabilidad p\\(= 8/10\\) de sol. nsim &lt;- 1000 tiros &lt;- 100 p.val &lt;- 8/10 #Creamos un vector para guardar los valores de p gorro p.gorro &lt;- rep(NA, nsim) #Loop recorriendo cada una de las nsim simulaciones for (i in 1:nsim){ experimento &lt;- sample(c(&quot;Sol&quot;,&quot;Águila&quot;), tiros, replace = TRUE, prob = c(p.val, 1 - p.val)) soles &lt;- table(experimento)[&quot;Sol&quot;] p.gorro[i] &lt;- soles/tiros } Podemos ver que en promedio le atinamos al valor verdadero: #Vemos que en promedio le atina: mean(p.gorro) ## [1] 0.80131 Lo mismo podemos verlo gráficamente: #Graficamos ggplot() + geom_point(aes(x = 1:nsim, y = p.gorro, color = as.character(p.gorro)), size = 2, alpha = 0.2) + geom_hline(aes(yintercept = p.val), size = 1.5, linetype = &quot;solid&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;) + labs( x = &quot;Simulaciones&quot;, y = &quot;Estimación de p&quot;, title = &quot;Simulación de proceso de estimación\\nde que una moneda caiga Sol&quot; ) + geom_label(aes(x = nsim/2, y = p.val), label = &quot;Verdadero valor de p&quot;) ¿Qué significa esto? El que en promedio \\(\\hat{p}\\) sea \\(p\\) (formalmente, que \\(\\mathbb{E}\\big[\\hat{p}\\big] = p\\)) significa que, si yo hago muchísimos experimentos (o procesos de muestreo) de la misma cosa, mi \\(\\hat{p}\\) es un buen estimador porque en promedio le va a atinar. Empero, esto no dice nada de qué tan bueno es mi estimador \\(\\hat{p}\\) para mi caso (mi muestra o mi experimento) específico. Puedes pensarlo con los exámenes: que alguien tenga un promedio de 8 dice que en general le ha ido bien en los exámenes, pero no dice nada respecto al primer examen de cálculo que hizo (donde pudo tener \\(10\\) ó \\(5\\) para llegar a ese promedio de \\(8\\) pero no podemos saber de manera específica cuánto fue ). Esto es igual: en promedio el estimador \\(\\hat{p}\\) será \\(p\\) pero para un análisis específico no sabemos. OJO Los datos observados no son variables aleatorias: esos ya son fijos, ya los viste. Los posibles datos observados sí son variables aleatorias ya que ellos, consisten en las variables que se pudieron haber observado y te permiten calcular las probabilidades de tus datos observados bajo algún modelo. En el caso de la moneda, los datos observados son \\(\\vec{x} = \\big( \\text{Sol}, \\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Sol},\\text{Águila}\\big)^T\\) pero los que se pudieron haber observado son todas las \\(\\binom{n}{2}\\) formas en las que la moneda pudo haber salido. 1. Estimación de una función de masa de probabilidad Formalmente, para una variable aleatoria discreta \\(X\\) que puede tomar los valores \\(\\{ a_1, a_2, \\dots, a_{\\ell} \\}\\) de la cual se observaron \\(n\\) realizaciones descritas mediante \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) (observados, fijos, constantes). Definimos la función de masa de probabilidad empírica como: \\[ \\hat{p}(x) = \\begin{cases} \\\\ \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}_{\\{a_1\\}}(x_i) &amp; \\text{ si } x = a_1 \\\\ \\\\ \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}_{\\{a_2\\}}(x_i) &amp; \\text{ si } x = a_2 \\\\ \\\\ \\vdots \\\\ \\\\ \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}_{\\{a_{\\ell}\\}}(x_i) &amp; \\text{ si } x = a_{\\ell} \\\\ 0 &amp; \\text{ en otro caso} \\end{cases} \\] donde se supone que \\(\\mathbb{P}(X = x) \\approx \\hat{p}(x)\\). Notamos que lo anterior puede resumirse en: \\[ \\hat{p}(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}_{\\{ x\\}}(x_i) \\] Análogamente, nota que para un conjunto (medible) \\(A\\), la aproximación para \\(\\mathbb{P}(X \\in A)\\) está dada por: \\[ \\hat{p}(A) = \\dfrac{1}{n}\\sum\\limits_{i=1}^{n} \\mathbb{I}_A (x_i). \\] Podemos graficar para la base de datos conteo_delitos la probabilidad de que, dado que se cometió un delito, éste haya ocurrido en el dia \\(d_i\\) de diciembre. Para ello usamos un geom_col: ggplot(conteo_delitos) + geom_col(aes(x = fecha, y = n/sum(n), fill = n)) + scale_fill_gradient(&quot;Delito&quot;, low = &quot;orange&quot;, high = &quot;red&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) + labs( y = &quot;p(x)&quot;, x = &quot;x&quot;, title = &quot;Aproximación a p(x)&quot; ) Una propiedad interesante de la función de masa de probabilidad es que, en promedio, le atina al verdadero valor (lo que comentábamos antes de que \\(\\hat{p} = p\\)). Es decir, suponiendo que \\(X\\) tiene una función de masa dada por: \\[ p(x) = \\begin{cases} p_1 &amp; \\text{ si } x = a_1 \\\\ p_2 &amp; \\text{ si } x = a_2 \\\\ \\vdots \\\\ p_{\\ell} &amp; \\text{ si } x = a_{\\ell} \\\\ \\end{cases} \\] y suponiendo un vector de muestras posibles \\(\\vec{X} = (X_1, X_2, \\dots, X_n)^T\\) notamos que \\[ \\mathbb{I}_{\\{ a_j \\}}(X_i) \\sim \\text{Bernoulli} (p_j) \\] Luego para cualquier \\(x\\) se tiene que: \\[ \\mathbb{E}\\big[ \\hat{p}(x)\\big] = \\mathbb{E}\\bigg[ \\dfrac{1}{n}\\sum\\limits_{i=1}^{n} \\mathbb{I}_{\\{ x \\}}(X_i) \\bigg] = \\dfrac{1}{n}\\sum\\limits_{i=1}^{n}\\mathbb{E}\\big[ \\mathbb{I}_{\\{ x \\}}(X_i) \\big] = \\frac{1}{n} n \\cdot p_j = p_j. \\] 2. Función de distribución empírica Recuerda que para cualquier variable aleatoria \\(X:\\mathbb{R}\\to\\mathbb{R}\\) existe su función de distribución \\(F_X\\) dada por: \\[ F_X(x) = \\mathbb{P}(X \\leq x) \\] La idea de la función de distribución empírica es reconstruir (a partir de los datos observados) a \\(F_X\\). Para ello, notamos que queremos estimar \\[ \\mathbb{P}(X \\leq x) \\qquad \\forall x\\in\\mathbb{R} \\] esto es equivalente a estimar: \\[ \\mathbb{P}\\big(X \\in (-\\infty, x] \\big) \\] y podemos aplicar la aproximación que usamos arriba para un conjunto \\(A\\): \\[ \\mathbb{P}\\big(X \\in (-\\infty, x] \\big) \\approx \\sum\\limits_{i=1}^n \\mathbb{I}_{(-\\infty, x]}(x_i) \\] La función de distribución empírica está definida para un vector numérico \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) por: \\[ \\hat{F} (x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\mathbb{I}_{(-\\infty, x]}(x_i) \\] La función de distribución empírica es una función de distribución pues cumple las siguientes propiedades (demuéstralo): \\(\\lim_{x \\to -\\infty} \\hat{F}(x) = 0\\) \\(\\lim_{x \\to \\infty} \\hat{F}(x) = 1\\) Si \\(x &lt; y\\) entonces \\(\\hat{F}(x) \\leq \\hat{F}(y)\\) (no decreciente) \\(\\hat{F}\\) es continua por la derecha con límites por la izquierda (càdlàg). Para demostrar 4. basta con demostrar que para \\(x_i\\) fija, la función \\(i(x) = \\mathbb{I}_{(-\\infty, x]}(x_i)\\) es continua por la derecha con límites por la izquierda pues \\(\\hat{F}(x)\\) es una suma de dichas funciones. En particular, podemos notar que la función de distribución empírica \\(\\hat{F} (x)\\) le atina a la función de distribución; es decir: \\[ \\mathbb{E}\\big[\\hat{F} (x) \\big] = F(x) \\] Para ello consideramos un vector de valores posibles \\(\\vec{X} = (X_1, X_2, \\dots, X_n)^T\\) donde las \\(X_i\\) tienen la misma distribución que \\(X\\). Y notamos que: \\[ \\mathbb{I}_{(-\\infty, x]}(X_i) \\sim \\textrm{Bernoulli}\\big(F(x)\\big) \\] pues \\(\\mathbb{I}_{(-\\infty, x]}(X_i) = 1\\) si \\(X_i \\leq x\\) y \\(\\mathbb{I}_{(-\\infty, x]}(X_i) = 0\\) si \\(X_i &gt; x\\). Luego: \\[ \\mathbb{P}\\Big( \\mathbb{I}_{(-\\infty, x]}(X_i) = 1 \\Big) = \\mathbb{P}(X_i \\leq x) = \\mathbb{P}(X\\leq x) = F(x) \\] donde la igualdad del medio se sigue de que \\(X_i\\) y \\(X\\) tienen la misma distribución. Entonces: \\[ \\mathbb{E}\\big[ \\hat{F}(x) \\big] = \\mathbb{E}\\Big[ \\dfrac{1}{n} \\sum\\limits_{i=1}^{n} \\mathbb{I}_{(-\\infty, x]}(X_i) \\Big] = \\dfrac{1}{n}\\sum\\limits_{i=1}^{n} \\mathbb{E}\\big[ \\mathbb{I}_{(-\\infty, x]}(X_i)\\big] = \\dfrac{1}{n} n \\cdot F(x) = F(x) \\] En R podemos calcular la función de distribución empírica con el comando ecdf el cual cuenta la cantidad de observaciones y regresa una función. Así, para la base de datos conteo_delitos podemos calcular la función de distribución empírica ecdf asociada a la cantidad de delitos que se cometen en un día mediante: Fgorro &lt;- ecdf(conteo_delitos$n) De esta forma podemos calcular la probabilidad de que en un día se cometan menos de 500 delitos: Fgorro(500) ## [1] 0.09677419 O bien podemos graficar la función: x &lt;- seq(300, 1000, length.out = 100) y &lt;- Fgorro(x) ggplot() + geom_step(aes(x = x, y = y), color = &quot;red&quot;) + labs( x = &quot;Número de carpetas de investigación (x)&quot;, y = &quot;Probabilidad de que en un día\\nse abran menos de x carpetas&quot;, title = &quot;Distribución acumulada de carpetas de investigación en CDMX&quot; ) + theme_minimal() Mediante simulaciones, podemos observar que \\(\\hat{F}\\) realmente le atina a \\(F\\) como sigue: #Cantidad de simulaciones nsim &lt;- 100 #Tamaño de la muestra en cada simulacion n_muestra &lt;- 100 #Valores a evaluar la función x &lt;- seq(-5, 5, length.out = 200) #Base de datos para guardar resultados de simulaciones F_simulado &lt;- data.frame(matrix(NA, ncol = nsim, nrow = length(x))) for (i in 1:nsim){ valores_simulados &lt;- rnorm(n_muestra) F_empirica &lt;- ecdf(valores_simulados) F_simulado[,i] &lt;- F_empirica(x) } F_simulado$Valor_x &lt;- x #Cambiamos el formato de la base para graficar F_simulado &lt;- F_simulado %&gt;% pivot_longer(cols = -Valor_x) ggplot(F_simulado) + geom_step(aes(x = Valor_x, y = value, color = name), alpha = 0.1) + geom_line(aes(x = Valor_x, y = pnorm(Valor_x)), color = &quot;black&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;) + labs( x = &quot;x&quot;, y = &quot;F(x)&quot;, title = &quot;Simulaciones de funciones de distribuciones acumuladas empíricas&quot;, subtitle = &quot;Para X ~ Normal(0,1)&quot; ) 2. Histograma Para una variable aleatoria continua, la aproximación \\(\\hat{p}\\) que hicimos no funciona (la masa siempre es \\(0\\)). Por lo que es necesario analizar alternativas para estudiar la densidad si suponemos que los datos pueden modelarse mediante algo continuo. Para construir un histograma consideremos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) y una constante \\(h &gt; 0\\) llamada el ancho de banda (binwidth). Sea \\(\\{ I_j \\}\\) una colección de intervalos no vacíos de \\(\\mathbb{R}\\) tal que \\(\\cup_{j=1} I_j = \\mathbb{R}\\) e \\(I_j \\cap I_k = \\emptyset\\) (i.e. los \\(\\{ I_j \\}\\) forman una partición de \\(\\mathbb{R}\\)). Supongamos, además, los \\(I_j\\) son de la forma: \\[ I_j = \\Big[\\kappa + (j-1) h, \\kappa + jh \\Big) \\] para algún \\(\\kappa \\in \\mathbb{R}\\) fijo. Sea \\[ n_j(\\vec{x}) = \\sum\\limits_{i=1}^n \\mathbb{I}_{I_j}(x_i) \\] la cantidad de \\(x_i\\) en el intervalo \\(I_j\\). Un histograma es la gráfica de la función (ver Panaretos (2016)): \\[ \\text{hist}_{\\vec{x}}(x) = \\frac{1}{n \\cdot h} \\sum\\limits_{j} n_j(\\vec{x}) \\cdot \\mathbb{I}_{I_j}(x) \\] Una propiedad interesante de un histograma es que éste aproxima correctamente las probabilidades \\(\\mathbb{P}(X \\in I_j)\\). Para ver esto, consideramos un vector de valores posibles \\(\\vec{X} = (X_1, X_2, \\dots, X_n)^T\\) y que \\(x\\in I_j\\), luego: \\[ \\mathbb{E}\\Big[ \\int_{I_j} \\text{hist}_{\\vec{X}}(x) dx \\Big] = \\mathbb{E}\\bigg[ \\frac{1}{n \\cdot h} \\sum\\limits_{j} n_j(\\vec{X}) \\cdot \\int_{I_j} \\mathbb{I}_{I_j}(x) dx \\bigg] = \\frac{1}{n \\cdot h} \\sum\\limits_{j} \\mathbb{E}\\big[ n_j(\\vec{X}) \\big] \\cdot h = \\dfrac{1}{n}\\sum\\limits_{j} \\mathbb{E}\\big[ n_j(\\vec{X}) \\big] \\] donde las \\(n_j(\\vec{X})\\) son variables aleatorias en este caso y: \\[ \\mathbb{E}\\big[ n_j(\\vec{X})\\big] = \\sum\\limits_{i=1}^n \\mathbb{E}\\big[\\mathbb{I}_{I_j}(X_i)\\big] = \\sum\\limits_{i=1}^n\\mathbb{P}(X_i \\in I_j) = n \\mathbb{P}(X \\in I_j) \\] donde la última igualdad se da pues las \\(X_i\\) tienen la misma distribución que \\(X\\). Luego: \\[ \\mathbb{E}\\Big[ \\int_{I_j} \\text{hist}_{\\vec{X}}(x) dx \\Big] = \\mathbb{P}(X \\in I_j) \\] Es decir, el valor esperado del área bajo un histograma en un intervalo \\(I_j\\) coincide con la probabilidad de que \\(X\\) pertenezca a dicho intervalo. Gráficamente: En R podemos hacer un histograma a través de geom_histogram. En este caso lo haremos de la latitud: #En este caso binwidth = h y kappa = boundary ggplot(datos) + geom_histogram(aes(x = latitud, y = ..density..), binwidth = 0.02, boundary = -99, color = &quot;white&quot;, fill = &quot;purple&quot;) + theme_light() 2.12.1 Ejercicio Considera la siguiente base de datos (obtenida de Cross Validated): mis.datos &lt;- data.frame( A = c(3.15, 5.46, 3.28, 4.20, 1.98, 2.28, 3.12, 4.10, 3.42, 3.91, 2.06, 5.53, 5.19, 2.39, 1.88, 3.43, 5.51, 2.54, 3.64, 4.33, 4.85, 5.56, 1.89, 4.84, 5.74, 3.22, 5.52, 1.84, 4.31, 2.01, 4.01, 5.31, 2.56, 5.11, 2.58, 4.43, 4.96, 1.90, 5.60, 1.92), B = c(2.90, 5.21, 3.03, 3.95, 1.73, 2.03, 2.87, 3.85, 3.17, 3.66, 1.81, 5.28, 4.94, 2.14, 1.63, 3.18, 5.26, 2.29, 3.39, 4.08, 4.60, 5.31, 1.64, 4.59, 5.49, 2.97, 5.27, 1.59, 4.06, 1.76, 3.76, 5.06, 2.31, 4.86, 2.33, 4.18, 4.71, 1.65, 5.35, 1.67), C = c(2.65, 4.96, 2.78, 3.70, 1.48, 1.78, 2.62, 3.60, 2.92, 3.41, 1.56, 5.03, 4.69, 1.89, 1.38, 2.93, 5.01, 2.04, 3.14, 3.83, 4.35, 5.06, 1.39, 4.34, 5.24, 2.72, 5.02, 1.34, 3.81, 1.51, 3.51, 4.81, 2.06, 4.61, 2.08, 3.93, 4.46, 1.4, 5.1, 1.42), D = c(2.40, 4.71, 2.53, 3.45, 1.23, 1.53, 2.37, 3.35, 2.67, 3.16, 1.31, 4.78, 4.44, 1.64, 1.13, 2.68, 4.76, 1.79, 2.89, 3.58, 4.10, 4.81, 1.14, 4.09, 4.99, 2.47, 4.77, 1.09, 3.56, 1.26, 3.26, 4.56, 1.81, 4.36, 1.83, 3.68, 4.21, 1.15, 4.85, 1.17) ) Grafica un histograma de las variables A, B, C y D de dicha base con un ancho de banda (binwidth) igual a 1. ¿Podemos concluir la forma de la distribución a partir del histograma? Es decir ¿hay distribuciones sesgadas a la izquierda, a la derecha, uniformes, centradas o con colas pesadas? Realiza el mismo histograma pero ahora con un ancho de banda de 0.25 ¿por qué hubo cambios? Analiza la base de datos (los valores en función de la columna A) y concluye. 3. Densidad kernel Un histograma tiene muchos bemoles: en particular, es necesario decidir quién es \\(h\\) y quién \\(\\kappa\\) y no hay una regla clara de cómo hacerlo. La densidad kernel es un intento de mejorar esta situación. Para ello recordamos que si \\(X\\) es una variable aleatoria continua con densidad \\(F\\) entonces: \\[ f(x) = F&#39;(x) = \\lim_{h \\to 0} \\dfrac{F(x + h) - F(x - h)}{2h} \\] Por lo que para un \\(h\\) positiva con \\(h \\approx 0\\) tenemos que: \\[ f(x) \\approx \\dfrac{F(x + h) - F(x - h)}{2h} \\] En el caso de un vector de observaciones \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) recordamos que podemos asociar una función de distribución empírica \\(\\hat{F}\\) y por tanto obtener el estimador de Rosenblatt de la densidad \\(f\\) mediante: \\[ \\hat{f}(x) = \\dfrac{\\hat{F}(x + h) -\\hat{F}(x - h)}{2h} \\] Podemos reescribir esto como: \\[ \\hat{f}(x) = \\dfrac{1}{2nh} \\sum\\limits_{i = 1}^n \\mathbb{I}_{(x-h, x+h]}(x_i) = \\dfrac{1}{nh}\\sum\\limits_{i = 1}^n K\\Big( \\frac{x_i - x}{h}\\Big) \\] donde: \\[ K(u) = \\frac{1}{2} \\mathbb{I}_{(-1, 1]}(u) \\] se conoce como el kernel rectangular. Una vez que llegamos hasta este punto notamos que para cualquier \\(K\\) que cumple: \\(\\int\\limits_{-\\infty}^{\\infty} K(u) du = 1\\) \\(K(u) \\geq 0\\) la función \\(\\hat{f}\\) es una función de densidad. La función \\(\\hat{f}\\) se conoce como el estimador de densidad del kernel K. Algunos ejemplos de kernels \\(K\\) son: Rectangular: \\(K(u) = \\frac{1}{2} \\mathbb{I}_{(-1, 1]}(u)\\) Triangular: \\(K(u) = (1 - |u|) \\mathbb{I}_{(-1, 1]}(u)\\) Epanechnikov: \\(K(u) = \\frac{3}{4}(1 - u^2) \\mathbb{I}_{(-1, 1]}(u)\\) Gaussiano: \\(K(u) = \\frac{1}{\\sqrt{2\\pi}}\\text{exp}(-u^2/2)\\) OJO No confundir el Kernel \\(K\\) (que es una función que integra a \\(1\\)) con función de densidad kernel que es una función de los datos que utiliza un kernel y es una densidad por sí misma. En R podemos calcular la densidad kernel en n puntos con relativa facilidad mediante density: densidad_kernel &lt;- density(datos$latitud, kernel = &quot;gaussian&quot;, n = 700, na.rm = TRUE) Nota que R en automático preselecciona los valores de h mediante un criterio preprogramado de optimización. Podemos ver dicha densidad gráficamente (y compararla con un histograma): ggplot(datos) + geom_histogram(aes(x = latitud, y = ..density..), binwidth = 0.01, boundary = 19, fill = &quot;purple&quot;, color = &quot;white&quot;) + geom_density(aes(x = latitud), kernel = &quot;gaussian&quot;, size = 1.5) + theme_bw() Esto no se queda ahí, podemos generalizar el concepto de kernel a dos dimensiones para aproximar una función de densidad \\(f(x,y)\\) de dos variables aleatorias sí tenemos dos vectores \\(\\vec{x}\\) y \\(\\vec{y}\\) y calculamos: \\[ \\hat{f}(x,y) =\\dfrac{1}{nh^2} \\sum\\limits_{i = 1}^n K \\Big( \\frac{x_i - x}{h}\\Big) K \\Big( \\frac{y_i - y}{h}\\Big) \\] En particular esto nos permite generar una densidad en R para saber en qué coordenadas de latitud y longitud ocurren más los delitos: ggplot(datos) + geom_point(aes(x = longitud, y = latitud), alpha = 0.025) + geom_density_2d_filled(aes(x = longitud, y = latitud), alpha = 0.75) + geom_density2d(aes(x = longitud, y = latitud)) + theme_void() 2.12.2 Ejercicio sugerido Este ejercicio es para que tengas la seguridad de que comprendiste los conceptos previos y sabes calcularlos. Es tedioso pero bueno para aclarar dudas. Considera la siguiente base de datos: x y z w 1 -100 Rojo Bueno 2 -2 Azul Malo 3 2 Azul Regular 2 3 Rojo Bueno 1 1 Verde Bueno 3 4 Amarillo Malo Calcula a mano (es decir puedes usar calculadora pero no lo calcules en R) y luego verifica tus cálculos haciéndolo en R: El total de \\(\\vec{x}\\) La media y varianza de \\(\\vec{y}\\) La curtosis y la asimetría de \\(\\vec{x}\\) (su media es 2 y su varianza 0.8). Determina si tiene un sesgo a la derecha, a la izquierda o ninguno. Determina mediante la curtosis si \\(\\vec{x}\\) tiene colas más pesadas que \\(\\vec{y}\\). Calcula el cuantil \\(0.25\\) y el \\(0.75\\) de \\(\\vec{y}\\) así como su rango intercuartílico (IQR). ¿Hay valores atípicos (outliers) en \\(\\vec{y}\\)? En caso afirmativo, determina cuáles son. ¿Cuál es el rango de \\(\\vec{y}\\)? (no confundir con el IQR). Determina la moda de \\(\\vec{z}\\) Determina la mediana de \\(\\vec{x}\\). Determina la MAD de \\(\\vec{x}\\) Realiza el conteo de cuáles \\(\\vec{z}\\) pertenecen al conjunto \\(A = \\{ \\text{Rojo}, \\text{Amarillo} \\}\\) Realiza una tabla de contingencia de \\(\\vec{w}\\) y \\(\\vec{z}\\). Determina la distribución frecuencial (observada) marginal de \\(\\vec{w}\\). Realiza una tabla de frecuencias de \\(\\vec{w}\\) y \\(\\vec{z}\\). Calcula el riesgo relativo de estar en un choque dado que manejas en CDMX a partir de los datos en la tabla : Interpreta tu resultado. De la tabla anterior calcula la razón de momios asociada a chocar dado que manejas en CDMX. Interprétala. Calcula la correlación de Bravais Pearson de \\(\\vec{x}\\) y \\(\\vec{y}\\). Interpreta. Obtén la correlación de Spearman de \\(\\vec{x}\\) y \\(\\vec{y}\\) Para \\(\\vec{w}\\) y \\(\\vec{x}\\) obtén la \\(\\tau\\) de Kendall (son 15 comparaciones para generarla) Descartando el outlier de \\(\\vec{y}\\) (y su \\(\\vec{x}\\) asociada), ajusta un modelo lineal \\(\\hat{y} = \\hat{\\beta}_1 x + \\hat{\\beta}_0\\) y grafícalo para ver qué tan buen modelo es. Realiza una gráfica de caja (boxplot) para \\(\\vec{y}\\) Realiza un scatterplot para la submatriz \\(Z_{(x,y)}\\). Realiza una gráfica de líneas para \\(Z_{(x,y)}\\) identificando la función de interpolación lineal \\(f(x)\\) asociada. Realiza una gráfica de barras de \\(\\vec{w}\\) especificando quiénes son los \\(a_i\\) y los \\(n_j\\). Estima mediante \\(\\hat{p}\\) la función de probabilidad de \\(\\vec{w}\\). Identifica la función de distribución empírica para \\(\\vec{x}\\), \\(\\hat{F}\\) y grafícala. Realiza un histograma con \\(h = 2\\) para \\(x\\). Toma \\(\\kappa = 4\\). Ajusta una densidad kernel a \\(\\vec{x}\\) con \\(h = 1\\) y usando un kernel \\(K\\) triangular. Calcula \\(\\hat{f}(x)\\) para \\(x = 0,1,2,3,4\\). 2.13 Ejercicios del capítulo Dado \\(\\vec{x}\\) vector de variables ordinales, obtén una expresión matemática para los siguientes estadísticos: La media de las diferencias entre las \\(x_i\\) quitando la de \\(x_k\\) consigo misma. El valor numérico o categoría menos común en \\(\\vec{x}\\). Si ordenamos todos los valores, la diferencia más alta entre algún \\(x_{(i)}\\) y su sucesor: \\(x_{(i+1)}\\). Este cálculo de para una S dada como vector numérico: #x es la muestra; x &lt;- c(x1,x2, ..., xn) datos_nuevos &lt;- c() for (i in 1:length(x)){ datos_nuevos &lt;- c(datos_nuevos, x[i]^i) } mean(datos_nuevos) #Este valor es el que me interesa Demuestra que: \\[ \\sigma^2_{\\vec{x}} = \\dfrac{1}{n}\\sum\\limits_{i=1}^n x_i^2 - \\bar{x}^2 \\] Para unos datos observados numéricos \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) se tiene que las \\(x\\) toman el valor \\(a_{x,1}\\) \\(n_{x,1}\\) veces, el valor \\(a_{x,2}\\), \\(n_{x,2}\\) veces y el valor \\(a_{x,\\ell}\\), \\(n_{x,\\ell}\\) veces (\\(n_j \\geq 0\\), \\(0 &lt; \\ell \\leq n\\) y \\(\\sum_{j=1}^{\\ell} n_j = n\\)). Demuestra que: \\[ \\bar{x} = \\dfrac{1}{\\sum_{j = 1}^{\\ell} n_{j}} \\cdot \\sum\\limits_{j=1}^{\\ell} n_j a_j \\] Sea \\(n\\) impar y \\(f\\) una función estrictamente decreciente. Demuestra que si \\(x_*\\) es la mediana de \\(\\vec{x} = ( x_1, x_2,\\dots, x_n)^T\\) entonces \\(f(x_*)\\) es la mediana de \\(\\Tilde{\\vec{x}} = \\big( f(x_1), f(x_2),\\dots, f(x_n)\\big)^T\\). Demuestra que si \\(\\bar{x}\\) es la media observada de \\(\\vec{x} = ( x_1, x_2,\\dots, x_n)^T\\) y \\(f(\\bar{x})\\) es la media observada de \\(\\Tilde{\\vec{x}} = \\big( f(x_1), f(x_2),\\dots, f(x_n)\\big)^T\\) y además \\(f\\) es diferenciable, entonces \\(f(x) = a \\cdot x + b\\) (es decir es una transformación afín). Hint Deriva. Sea \\(\\phi\\) una función convexa. Demuestra que: \\[ \\phi \\Big( \\bar{x} \\Big) \\leq \\dfrac{1}{n}\\sum\\limits_{i=1}^n \\phi(x_i) \\] Hint Deriva. Recuerda que si \\(\\phi\\) es convexa, para \\(0 \\leq \\alpha \\leq 1\\) se tiene que: \\[ \\phi\\big(\\alpha x + (1-\\alpha) y\\big) \\leq \\alpha \\phi(x) + (1-\\alpha)\\phi(y) \\] Sea \\(\\hat{p}_R(x)\\) la densidad kernel asociada a \\(\\vec{x} = (x_1, x_2, \\dots, x_n)\\) con un núcleo (kernel) \\(K(u) \\geq 0\\) y \\(h &gt; 0\\). Demuestra: \\(\\hat{p}_h(x)\\) es una función de densidad de probabilidad ( integra a \\(1\\)). Determina la media de una variable aleatoria \\(X\\) que se distribuye con densidad \\(\\hat{p}_h(x)\\) bajo: Kernel triangular. Determina la varianza de una variable aleatoria \\(X\\) que se distribuye con densidad \\(\\hat{p}_h(x)\\) bajo: Kernel Epanechnikov Sea \\(\\text{hist}_{\\vec{x}}\\) la función de histograma para un vector numérico \\(\\vec{x}\\) con \\(h &gt; 0\\), \\(\\kappa \\in \\mathbb{R}\\) fijos y una partición \\(\\{I_j\\}_{j \\in \\mathbb{Z}}\\). Demuestra que \\(\\text{hist}_{\\vec{x}}\\) es una función de densidad. Demuestra que para \\(\\vec{x} = (x_1, x_2, \\dots, x_n)^T\\) la función de distribución empírica \\(\\hat{F}(x)\\) es una función de distribución acumulada; es decir: \\(\\lim_{x \\to -\\infty} \\hat{F} (x) = 0\\) \\(\\lim_{x \\to \\infty} \\hat{F} (x) = 1\\) \\(\\lim_{x \\to x_0^+} \\hat{F} (x) = \\hat{F} (x_0)\\) (continua por la derecha) \\(\\lim_{x \\to x_0^-} \\hat{F} (x)\\) existe \\(F_n (x)\\) es no decreciente. La tabla muestra datos observados del PIB de un país en billones de dólares: Ajusta una parábola \\(q (x) = a \\cdot x^2 + b\\cdot x + c\\) para obtener la mejor parábola que ajuste esos puntos. ¿Qué valor de PIB se espera para el 2020 bajo este modelo? Demuestra que si \\(\\vec{x} = -\\vec{y}\\) (dos vectores numéricos) la correlación de Spearman entre ambos es \\(-1\\). Para una variable aleatoria \\(T\\) que representa un tiempo, se define una función de supervivencia como la probabilidad de que \\(T\\) dure más que un cierto tiempo \\(t\\); es decir: \\[\\begin{equation}\\nonumber S(t) = \\mathbb{P}(T &gt; t) \\end{equation}\\] Construye \\(\\hat{S}\\) una aproximación empírica a la función de supervivencia \\(S\\) tal que \\(\\mathbb{E}[\\hat{S}(t)] = S(t)\\). Demuestra este último resultado. Demuestra que \\(\\text{Var}\\big[ \\hat{F}(x) \\big] = \\frac{1}{n} F(x) \\big( 1 - F(x) \\big)\\). Recuerda que para dos variables aleatorias \\(X\\) y \\(Y\\) se define la covarianza \\(\\text{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]\\). Calcula entonces la covarianza dada por \\(\\text{Cov}\\big(\\hat{p}(u), \\hat{p}(v)\\big)\\). Demuestra que si \\(X\\) es independiente de \\(Y\\) entonces \\(\\mathbb{I}_A(X)\\) es independiente de \\(\\mathbb{I}_A(Y)\\). Recuerda que dos variables aleatorias \\(X\\), \\(Y\\) son independientes si y sólo si \\(\\mathbb{P}(X \\in A, Y \\in B) = \\mathbb{P}(X \\in A) \\mathbb{P}(Y \\in B)\\) para conjuntos \\(A,B\\) medibles. Sean \\(\\rho\\) la \\(\\rho\\) de Spearman y \\(\\tau\\) la \\(\\tau\\) de Kendall para dos vectores ordinales \\(\\vec{x}\\) y \\(\\vec{y}\\). Demuestra que: \\[ \\frac{1 + \\rho}{2} \\geq \\Big( \\frac{1 + \\tau}{2} \\Big)^2 \\] Da un ejemplo de vector \\(\\vec{x}\\) de al menos dos entradas tal que \\(\\text{MAD}_{\\vec{x}} \\geq \\sigma_{\\vec{x}}\\) Demuestra que \\(\\text{MAD}_{\\vec{x}} = 0 \\Leftrightarrow \\sigma_{\\vec{x}} = 0\\) para el mismo vector \\(\\vec{x}\\). Si un vector \\(\\vec{x}\\) tiene \\(3\\) entradas, media \\(\\bar{x} = 1\\) y varianza \\(\\sigma_{\\vec{x}} = 1\\) y además se sabe que su curtosis es \\(1\\), ¿quién es \\(\\vec{x}\\)? Bajo la correlación de Pearson demuestra que para vectores \\(\\vec{x}, \\vec{y}\\) y \\(\\vec{z}\\) si \\(\\rho_{\\vec{x},\\vec{y}} = 1\\) y \\(\\rho_{\\vec{x},\\vec{z}} = 1\\) entonces \\(\\rho_{\\vec{y},\\vec{z}} = 1\\) Sean \\(\\vec{w}, \\vec{x}, \\vec{y}, \\vec{z} \\in\\mathbb{R}^n\\) y \\(a,b,c,d\\in\\mathbb{R}\\) demuestra que: \\[ \\rho(a \\vec{x} + b \\vec{w}, c \\vec{y} + d \\vec{z}) = K_1\\cdot\\rho(\\vec{x}, \\vec{y}) + K_2\\cdot\\rho(\\vec{w}, \\vec{y}) + K_3\\cdot\\rho(\\vec{x}, \\vec{z}) + K_4\\cdot\\rho(\\vec{w}, \\vec{z}) \\] para algunas constantes \\(K_1, K_2, K_3, K_4\\); donde, además, \\(\\rho\\) es la correlación de Pearson. Sean \\(\\vec{x} = (x_1,x_2, \\dots, x_n)^T\\) el vector de los datos observados (fijo) y \\(\\vec{X} = (X_1,X_2, \\dots, X_n)^T\\) el vector de los datos posibles (aleatorio). Supongamos que las entradas de \\(\\vec{X}\\) son independientes e idénticamente distribuidas con la misma distribución de \\(X\\) con media \\(\\mu\\) y varianza \\(\\sigma^2\\). Demuestra que \\(\\mathbb{E}\\big[ \\bar{X} \\big] = \\mu\\) ¿Qué te dice esto de \\(\\bar{x}\\)? Demuestra que \\(\\mathbb{E}\\big[ \\sigma_{\\vec{X}}^2 \\big] \\neq \\sigma^2\\) ¿Qué te dice esto de \\(\\sigma_{\\vec{x}}^2\\)? Donde \\(\\sigma_{\\vec{X}}^2 = \\frac{1}{n}\\sum_{i=1}^n \\big( X_i - \\bar{X}\\big)^2\\). Hint: usa el ejercicio 2 de esta sección. Quiénes son \\(\\vec{x}, \\vec{y}\\) si su tabla de contingencia es la dada por Cuadro : Da un ejemplo de vectores \\(\\vec{x}\\) y \\(\\vec{y}\\) de tamaño \\(10\\) cuya \\(\\tau\\) de Kendall sea \\(0\\) pero estén completamente relacionados; es decir exista una función \\(g\\) tal que \\(\\vec{y} = g(\\vec{x})\\). Construye una función function en R que dado un vector x &lt;- c(x_1, x_2, ..., x_n) regrese la densidad kernel (bajo kernel gaussiano) asociada a x y evaluada en los puntos dados por el vector t &lt;- c(t_1, t_2, ..., t_m). Considera la siguiente base de datos de calificaciones. Calcula la mediana de calificaciones, media, varianza y el IQR. Calificación Cantidad de alumnos 10 3 9 4 8 12 7 11 6 1 5 4 A partir de la siguiente gráfica determina si los incisos son verdaderos o falsos o no se puede determinar: La correlación de Pearson de \\(\\vec{x}\\) y \\(\\vec{y}\\) es \\(-1\\) Verdadero Falso No se puede determinar La correlación de Spearman de \\(\\vec{x}\\) y \\(\\vec{y}\\) es \\(-1\\) Verdadero Falso No se puede determinar La tau de Kendall de \\(\\vec{x}\\) y \\(\\vec{y}\\) es \\(-1\\) Verdadero Falso No se puede determinar A partir de la siguiente gráfica determina si los incisos son verdaderos o falsos o no se puede determinar: El coeficiente de asimetría de \\(\\vec{x}\\) es positivo. Verdadero Falso No se puede determinar El coeficiente de curtosis de \\(\\vec{x}\\) es positivo. Verdadero Falso No se puede determinar La distribución de \\(\\vec{x}\\) tiene una sola moda. Verdadero Falso No se puede determinar Considera el problema de mínimos cuadrados donde ahora suponemos que existe una submatriz \\(X_{n \\times k}\\) de la base de datos \\(Z\\) y un vector columna \\(y\\) tal que cada entrada de las \\(y\\), \\(y_i\\) es una función lineal de las \\(x_{i,j}\\): \\[ y_i = \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\dots + \\beta_k x_{i,k} \\] Suponemos, además que las columnas de \\(X_{n \\times k}\\) son linealmente independientes (i.e. es de rango completo). En este caso la función de error a minimizar para estimar las \\(\\beta\\)s es: \\[ \\text{SSR}(\\beta_1, \\beta_2, \\dots, \\beta_k) = \\sum\\limits_{i=1}^n \\big( y_i - \\sum\\limits_{j=1}^k \\beta_j x_{i,j}\\big)^2 \\] Demuestra que en este caso el problema es equivalente a minimizar: \\[\\begin{equation}\\nonumber \\text{SSR}(\\vec{\\beta}) = (\\vec{y} - X \\vec{\\beta})^T (\\vec{y} - X \\vec{\\beta}) \\end{equation}\\] Obtén entonces que: \\[\\begin{equation}\\nonumber \\beta = (X^T X)^{-1} X^T \\vec{y} \\end{equation}\\] Finalmente, suponiendo que \\(y = \\alpha + \\gamma x + \\eta z\\) es un hiperplano de \\(x\\) y \\(z\\) calcula los coeficientes: x y z 1 7 -1 2 9 -2 3 12 -3 1 14 1 Verifica los coeficientes haciendo la regresión en R. References "],
["referencias.html", "Referencias", " Referencias "],
["programación-en-r.html", "A Programación en R A.1 Algunas ventajas de R y cosas no tan padres A.2 Bienvenidx a R, Taking Off Again (sí, así se llama esta versión) A.3 Instalando cosas A.4 Instalación de RStudio A.5 Primeros pasos en R usando RStudio A.6 Cálculos numéricos A.7 Variables A.8 Observaciones sobre la aritmética de punto flotante A.9 Leer y almacenar variables en R A.10 Instalación de paquetes A.11 Comentarios adicionales sobre el formato", " A Programación en R Figure A.1: R es un programa chido de estadística. FIN. Una de las primeras cosas que necesitamos saber es que R (por más que sus más ávidos defensores digan lo contrario) no es para todo. Si tú ya conoces otro lenguaje (sea Stata, Excel, SAS, Python, Matlab, Julia, etc) sabrás utilizar muchas de sus opciones. Estoy seguro que, de conocer uno de estos, te será muchísimo más fácil seguir sacando promedios en tu lenguaje favorito que en R, realizar regresiones lineales es probablemente más sencillo en Stata mientras que las gráficas de barras para mí son más simples en Excel, Python excede en aplicaciones de inteligencia artificial mientras que Matlab es más veloz que R, Julia tiene muchas cosas de ecuaciones diferenciales que nadie más. Lo que probablemente no sea más sencillo de hacer en otro lenguaje es realizar análisis estadístico, gráficas de todo tipo y modelos de simulación. Para eso, R es, indiscutiblemente, una de las mejores opciones para quienes no conocen de programación2. Finalmente, uno de los consejos más importantes que te puedo dar es que este curso no te va a servir si no practicas. Igual que como pasa con los idiomas uno no aprende R en una semana sin practicarlo después. Mi sugerencia es que, a la vez que sigues estas notas comiences a trabajar un proyecto tuyo específico junto con el buscador de Internet de tu preferencia a la mano y empieces a usar R en él. Practica3. A.1 Algunas ventajas de R y cosas no tan padres A.1.1 Puntos a favor de R Todo el mundo lo usa. Quizá éste es el punto más a favor. Si mucha gente lo conoce y lo utiliza, hay más opciones de ayuda. Los sitios de StackOverflow en inglés y en español son excelentes para pedir apoyo en R; los grupos de usuarios de Google son otra fuente muy buena. Entre más gente usa el programa; es más fácil obtener ayuda porque seguro alguien más tuvo hace ya tiempo el mismo problema que tú. Todas las personas que trabajan en estadística publican sus métodos y su código en R (eso, claro, cuando publican sus métodos). Es raro encontrar un nuevo método estadístico en el mundo y que no se pueda usar, de alguna forma, en R. Dentro de los lenguajes de programación R es de los más sencillos. Quienes lo hicieron realmente se preocuparon por su público (de no especialistas) y en general desarrollan para él. R es gratis. Y en esta época de austeridad, cualquier ahorro es bueno. Que sea gratis no significa que no esté respaldado: existen versiones de R respaldadas por grandes compañías como Microsoft Todo lo que se hace en R es público. R no tiene métodos secretos ni es una caja negra. Todo lo que hace cada una de las funciones de R, cualquiera lo puede revisar, por completo. En R puedes hacer libros o notas ¡como este! donde guardes todo tu trabajo, reportes automatizados e incluso documentos interactivos para facilitar el análisis de datos. R puede hacer gráficas bonitas: Por supuesto, no todo es miel sobre hojuelas con R. Particularmente, algunos de los problemas con el lenguaje: Figure A.2: La curva de aprendizaje de R es más empinada pero después de un rato vale la pena La curva de aprendizaje es mucho más empinada que para otros programas estadísticos (como Stata, SAS o SPSS) ¡particularmente si es tu primera vez programando! La mayor parte de las personas que trabajan en R no son programadores de verdad. Gran parte del código que te puedes encontrar en el mundo real está escrito con prisa para salir del aprieto sin mucha planeación, con pocos comentarios, falta de control de versiones y pocas herramientas de revisión. ¡Internet está lleno de creaturas espantosas escritas en R! Figure A.3: R puede ser muy lento pero eso te da oportunidad de hacer otras cosas ;) . R de ninguna manera es veloz por lo que algunos programas (lo veremos en simulación) pueden ser extremada (y dolorosamente) lentos. A.2 Bienvenidx a R, Taking Off Again (sí, así se llama esta versión) R es un lenguaje de cómputo y un programa estadístico libre, gratuito, de programación funcional (¿qué es eso?), orientado a objetos (what??) que mutó a partir de otros dos lenguajes conocidos como Scheme y S4. El primero de estos fue desarrollado en el MIT por Sussman y Steele mientras que el segundo surgió en los laboratorios Bell5 creado por Becker, Wilks y Chambers. R nació en junio de 1995 a partir del trabajo de Ross Ihaka y Robert Gentleman6. Desde su creación, la mayor parte del desarrollo de R ha sido trabajo completamente voluntario de la Fundación R, del equipo de R Core y de miles de usuarios que han creado funciones específicas para R conocidas como paquetes (packages). Actualmente el repositorio más importante de R, CRAN, contiene más de 16000 paquetes con distintas funciones para hacer ¡lo que quieras! Como todo el trabajo en R es voluntario hace falta: Una homologación en los métodos. Puedes encontrar varias funciones que supuestamente hacen exactamente lo mismo (como es el caso de emojifont, fontemoji y emoGG para graficar usando emojis). Estandarizar la notación. Algunos paquetes como aquellos del tidyverse (veremos más adeltna) utilizan pipes (%&gt;%); estos sólo funcionan en el tidyverse pero no fuera del mismo. Sin embargo, también es una gran ventaja que sean los usuarios de R quienes guían su desarrollo. El lenguaje va mutando según peticiones de las personas que lo usan. Si hay algo que te gustaría R tuviera y aún no existe ¡lo puedes proponer! A.3 Instalando cosas A.3.1 Instalación de R Figure A.4: Oficialmente, la página de R es de las páginas más feas del mundo. ¡No te dejes llevar por las apariencias! A lo largo de estas notas estaré trabajando con: R version 4.0.2 (2020-06-22) Taking Off Again. La más reciente versión de R la puedes encontrar en CRAN. Para ello ve al sitio y selecciona tu plataforma. Nota usuarios de Mac En algunas Mac, al abir R, aparece el siguiente mensaje de advertencia: During startup - Warning messages: 1: Setting LC_CTYPE failed [...] para solucionarlo ve a Aplicaciones y abre Terminal. Copia y pega en ella el siguiente texto: defaults write org.R-project.R force.LANG en_US.UTF-8 Da enter, cierra la Terminal y reinicia R. En el caso de Windows da clic en Download R for Windows y luego en install R for the first time. Finalmente, ejecuta el instalable que aparece al dar click en Download R 4.0.2 for Windows . Para este curso pudiera ser que requirieras las herramientas de desarrollador Rtools. En el caso de Mac selecciona Download R for (Mac) OS X y luego elige R-4.0.2.pkg. En Mac puede que necesites instalar adicionalmente XQuartz (según tu versión de Mac). Si tu Mac es una versión suficientemente antigua, sigue las instrucciones específicas de CRAN. En el caso de Linux al elegir Download R for Linux tendrás la opción de buscar tu distribución específica. Al elegirla, aparecerán instrucciones para tu terminal de comandos; síguelas. En el caso de Linux, según los paquetes de R que elijamos instalar en la computadora requerirás instalar paquetería adicional para tu distribución de Linux. R te informará de la paquetería necesaria conforme la requiera. Si tienes problemas para instalar puedes usar RStudio Cloud. A.4 Instalación de RStudio Figure A.5: RStudio es una empresa que se dedica a hacer cosas para R. RStudio es una interfaz gráfica (IDE) para R. Puedes pensar a R como el Bloc de Notas y a RStudio como Word. El Bloc tiene todas las capacidades que necesitas para poder escribir; empero, es muchísimo mejor trabajar tus papers en Word. De la misma manera, R tiene todas las capacidades para hacer estadística pero un formato horrible y RStudio se ha convertido en la más popular forma de usar R. Por supuesto que no es la única; algunas alternativas son Atom con ide-r, Eclipse con StatET y RKWard. En general es posible seguir estas notas sin que tengas RStudio pero, si es tu primera vez programando, no lo recomiendo. Si ya tienes experiencia con lenguajes como Python, Javascript, Java ó alguno de los mil C que existen, no tendrás ningún problema usando el editor de tu preferencia. Para descargar RStudio ve a su página y da clic en Download RStudio. Baja tu pantalla hasta donde dice Installers for Supported Platforms y elige tu plataforma: Windows, Mac OS X ó tu sabor de Linux preferido. Una vez descargado el archivo, ábrelo y sigue las instrucciones que aparecen en pantalla. A.5 Primeros pasos en R usando RStudio Una vez hayas instalado R y RStudio, abre RStudio7. Te enfrentarás a una pantalla similar a esta: Figure A.6: La primera vez que abres RStudio Si tu RStudio tiene sólo 3 páneles, como en mi caso, ve a la esquina superior izquierda (signo de hoja+) y elige un nuevo R Script Figure A.7: Elige hoja+ para crear un nuevo archivo Tendrás, entonces, 4 páneles como se ve a continuación: Figure A.8: RStudio &lt;3 El primer panel (esquina inferior izquierda) es la Consola. Aquí es donde se ejecutan las acciones. Prueba escribir 2 + 3 en él y presiona enter. Aparece el resultado de la suma. Definitivamente, R es la calculadora que más trabajo cuesta instalar. Figure A.9: La consola de R es la calculadora más difícil de instalar que existe. El segundo panel (esquina superior izquierda) es el panel con el Script. Aquí se escribe el programa pero no se ejecuta. Prueba escribir 10 + 9. ¿Ves que no pasa nada? Lo que acabas de hacer es crear un programa que, cuando se ejecute, hará la suma de 10 + 9. ¡Qué programa más aburrido! Sin embargo, no todo está perdido: presiona CTRL+Enter (Cmd+Enter en Mac) al final de la línea o bien da clic en Run y verás que, en la consola, aparece la instrucción y el resultado de la misma. El Script es una excelente fuente para tener un historial de lo que estás haciendo. Figure A.10: El Script sirve para salvar las instrucciones en el orden en que las vas a ejecutar. El tercer panel contiene el ambiente. Aquí aparecerán las variables que vayamos creando. Por ahora, para poner un ejemplo, importaremos el archivo Example1.csv (con valores simulados) disponible en Github dando clic en Import Dataset y From Text (base). Selecciona el archivo y elige las opciones en la ventana de previsualización que hagan que se vea bien. Nota que una vez realizada la importación aparece en el panel derecho Example1. Al dar clic podrás ver la base de datos. Las bases de datos y variables que utilices durante tus análisis aparecerán en esa sección. Figure A.11: El Ambiente muestra las variables (incluyendo bases de datos) que estás utilizando en este momento. A diferencia de otros programas estadísticos (o sea Stata) en R es posible tener múltiples bases de datos abiertas a la vez. Para entender mejor lo que ocurre en el último de los páneles, lo mejor es trabajar con nuestra base. Escribe en la consola plot(Example1) . En el cuarto pánel aparecerá una gráfica. El cuarto de los páneles para nosotros tendrá esa utilidad: mostrará las gráficas que hagamos así como la ayuda. Para ver la ayuda para las instrucciones de R puedes escribir ?. Prueba teclear ?plot en la consola. El signo de interrogación es un help() que muestra las instrucciones para usar una función. Figure A.12: La gráfica que aparece de hacer un plot de la base de datos de ejemplo. Figure A.13: El cuarto panel muestra respectivamente las gráficas y la ayuda. Mi sugerencia personal es que escribas todo lo que haces en el Script y que sólo utilices la consola para verificar valores. De esta manera podrás almacenar todas las instrucciones ejecutadas y volver a ellas cuando se requieran. Por último te sugiero utilizar # gatos para comentar tu código. Así, el código anterior lo podrías ver en la consola como: Comenta. Comenta. Comenta, por favor. Tu ser del futuro que regrese a sus archivos de R un mes después de haberlos hecho te lo agradecerá (y tu profe también). Finalmente y como aclaración para estas notas, el código de R aparece como: Mientras que los resultados de evaluar en R se ven con #: [1] 5 Así, la evaluación con su resultado se ve de la siguiente forma: [1] 5 A.6 Cálculos numéricos R sirve como calculadora para las operaciones usuales. En él puedes hacer sumas, [1] 43 Figure A.14: Ada Lovelace (1815-1852), la primera en diseñar un algoritmo computacional ¡y sin tener computadoras! restas, [1] -1 multiplicaciones, [1] 56 divisiones, [1] 2 sacar logaritmos naturales \\(\\ln\\), [1] 4.60517 o bien logaritmos en cualquier base,8 [1] 2 también puedes elevar a una potencia (por ejemplo hacer \\(6^3\\)), [1] 216 calcular la exponencial \\(e\\), [1] 2.718282 o bien exponenciar cualquier variable \\(e^{-3}\\), [1] 0.04978707 también puedes usar el número \\(\\pi\\). [1] 3.141593 No olvides que R usa el orden de las operaciones de matemáticas. Siempre es de izquierda a derecha con las siguientes excepciones: Primero se evalúa lo que está entre paréntesis. En segundo lugar se calculan potencias. Lo tercero en evaluarse son multiplicaciones y divisiones. Finalmente, se realizan sumas y restas. Por ejemplo, en la siguiente ecuación \\[ 2 - 2 \\cdot \\frac{(3^4 - 9)}{(5 + 4)} \\] se resuelven primero los paréntesis \\((3^4 - 9) = 81 - 9 = 72\\) y \\((5 + 4) = 9\\); luego se resuelve la división: \\(\\frac{72}{9}=8\\), se multiplica por el \\(2\\): \\(2 \\cdot 8 = 16\\) y finalmente se hace la resta: \\(2-16 = -14\\). A.6.1 Ejercicio Determina, sin evaluar, los resultados de los siguientes segmentos de código: Evalúa para comprobar tu respuesta. A.6.2 Ejercicio Calcula el área y el perímetro de un círculo de radio 5. Recuerda que la fórmula del área es \\(\\pi \\cdot r^2\\) donde \\(r\\) es el radio; mientras que la del perímetro es: \\(\\pi \\cdot d\\) donde \\(d\\) es el díametro (= dos veces el radio). A.6.3 Respuestas Área = 78.5398163397448 Perímetro = 31.4159265358979 A.7 Variables R es un programa orientado a objetos; esto quiere decir que R almacena la información en un conjunto de variables que pueden tener diferentes clases y opera con ellos según su clase. Por ejemplo, un conjunto de caracteres, entre comillas, es un Character (R lo piensa como texto) [1] “Hola” Un número (por ejemplo 2 tiene clase numeric)9. Hay que tener mucho cuidado con combinar floats con Strings: [1] 6 Figure A.15: El algoritmo diseñado por Ada Lovelace. ## Error in 2 + &quot;4&quot;: non-numeric argument to binary operator Si lo piensas, este último error ¡tiene todo el sentido! no puedes sumar un número a un texto. ¿O qué significaría 'Felices' * 4 ? La magia de R comienza con que puedes almacenar valores en variables. Por ejemplo, podemos asignar un valor a una variable: Hay dos formas de asignar valores, una es con la flecha de asignación \\(\\leftarrow\\) y otra con el signo de igual: Nota que, cuando realizamos operaciones, la asignación es la última que se realiza: Los valores que fueron asignados en las variables, R los recuerda y es posible calcular con ellos: [1] 16 [1] 8 Podemos preguntarnos por el valor de las variables numéricas mediante los operadores == (sí, son dos iguales), != (que es un \\(\\neq\\)) &gt;, &gt;=, &lt;= y &lt;: [1] FALSE El operador de asignación también se puede utilizar al revés \\(2 \\rightarrow x\\) pero no lo hagas, por favor. Nota que no estamos asignando el valor de x: [1] 10 Podemos preguntarnos por diferencia: [1] TRUE Así como por mayores, menores incluyendo posibles igualdades (i.e. los casos \\(\\geq\\) y \\(\\leq\\)) [1] TRUE [1] TRUE [1] FALSE [1] TRUE En todos los casos los resultados han sido TRUE ó FALSE. La clase de variables que toma valores TRUE ó FALSE se conoce como booleana. Hay que tener mucho cuidado con ellas porque, puedes acabar con resultados muy extraños: [1] 101 [1] 0 Aquí puedes encontrar una lista de malas prácticas en computación a evitar. Finalmente, nota que es posible reescribir una variable y cambiar su valor: [1] 10 [1] 0.5 A.7.1 Ejercicios Determina el valor que imprime R en cada caso, sin que corras los siguientes pedazos de código. Después, verifica tu respuesta con R: A.7.2 NIVEL 3 Determina, sin correr el programa, qué regresa la consola en este caso Comprueba con la consola tus resultados; puede que encuentres respuestas poco intuitivas. A.8 Observaciones sobre la aritmética de punto flotante Si hiciste el penúltimo ejercicio (el cual, obviamente hiciste y comprobaste con la consola) podrás haber notado una trampa. Analicemos qué ocurre; quizá hicimos mal la suma [1] 0.3 [1] 0.3 Aparentemente no hay nada malo ¿qué rayos le pasa a R? La respuesta está en la aritmética de punto flotante. Podemos pedirle a R que nos muestre los primeros 100 dígitos de la suma 0.1 + 0.1 + 0.1: Figure A.16: Réplica de la Z3, la primer computadora con punto flotante (1941). [1] 0.3000000000000000444089 El comando options(digits = 22) especifica que R debe imprimir en la consola 22 dígitos. No más. ¡Ahí está el detalle! R no sabe sumar. En general, ningún programa de computadora sabe hacerlo. Veamos otros ejemplos: [1] 3.999999999999999555911 [1] 0.2999999999999999888978 [1] Inf El problema está en cómo las computadoras representan los números. Ellas escriben los números en binario. Por ejemplo, 230 lo representan como 11100110 mientras que el 7 es: 111. El problema de las computadoras radica en que éstas tienen una memoria finita por lo que números muy grandes como: \\(124765731467098372654176\\) la computadora hace lo mejor por representarlos eligiendo el más cercano: [1] 124765731467098377420800 Un error de punto flotante en la vida real ocasionó en los años noventa, la explosión del cohete Ariane 5. Moraleja: hay que tener cuidado y respeto al punto flotante. No olvides cambiar la cantidad de dígitos que deseas que imprima R en su consola de vuelta: El mismo problema ocurre con números decimales cuya representación binaria es periódica; por ejemplo el \\(\\frac{1}{10}\\) en binario se representa como \\(0.0001100110011\\overline{0011}\\dots\\). Como es el cuento de nunca acabar con dicho número, R lo trunca y almacena sólo los primeros dígitos de ahí que, cada vez que escribes 0.1, R en realidad almacene el 0.1000000000000000055511 que es casi lo mismo pero no es estrictamente igual. Hay que tener mucho cuidado con esta inexactitud de las computadoras (inexactitud estudiada por la rama de Análisis Numérico) pues puede generar varios resultados imprevistos. A.8.1 ¿Cómo checar un if? En general lo que hacen las computadoras para comparar valores es que verifican que, en valor absoluto, el error sea pequeño. Recuerda que el valor absoluto de \\(x\\), \\(|x|\\), regresa siempre el positivo: \\[ |4| = 4 \\qquad \\textrm{y} \\qquad |-8| = 8 \\] Para verificar que algo es más o menos \\(0.3\\) suele usarse el valor absoluto10 de la siguiente manera: [1] TRUE donde 1.e-6 es notación corta para 0.000001 (también escrito como \\(1\\times 10^{-6}\\)). La pregunta que nos estamos haciendo es que si el error entre sumar \\(0.1+0.1+0.1\\) y \\(0.3\\) es muy pequeño \\(&lt; 0.000001\\): \\[ | (0.1 + 0.1 + 0.1) - 0.3 | &lt; 0.000001 \\] A.9 Leer y almacenar variables en R Para terminar esta sección, aprenderemos cómo guardar variables en R. Para eso, el concepto de directorio es uno de los más relevantes. En general, en computación, el directorio se refiere a la dirección en tu computadora donde estás trabajando. Por ejemplo, si estás en una carpeta en tu escritorio de nombre “Ejercicios_R” probablemente tu directorio sea ‘~/Desktop/Ejercicios_R/’ (en Mac) o bien ‘~\\Desktop\\Ejercicios_R\\’ en Windows11. La forma de saber tu directorio (en general) es ir a la carpeta que te interesa y con clic derecho ver propiedades (o escribir ls en la terminal Unix). R tiene un directorio default que quién sabe dónde está (depende de tu instalación, generalmente está donde tu Usuario). Usualmente lo mejor es elegir un directorio para cada uno de los proyectos que hagas. Para ello si estás en RStudio puedes utilizar Shift+Ctrl+H (Shift+Cmd+H en Mac) o bien ir a Session &gt; Set Working Directory &gt; Choose Directory y elegir el directorio donde deseas trabajar tu proyecto. Pensando que elegiste el escritorio (Desktop en mi computadora) notarás que en la consola aparece el comando setwd(\"~/Desktop\") (o bien con ‘\\’ si eres Windows). Mi sugerencia es que copies ese comando en tu Script para que, la próxima vez que lo corras ya tengas preestablecido el directorio. Podemos verificar el directorio elegido con getwd(): En general es buena práctica en R establecer, hasta arriba del Script, el comando de directorio. Esto con el propósito de que, cuando compartas un archivo, la persona a quien le fue compartido el archivo pueda rápidamente elegir su propio directorio en su computadora. Probemos guardar unas variables en un archivo dentro de nuestro directorio. Para ello utilizaremos el comando save. Si vas a tu directorio, notarás que el archivo MisVariables.rda acaba de ser creado. De esta forma R puede almacenar objetos creados en R que sólo R puede leer (más adelante veremos cómo exportar bases de datos y gráficas). Observa que en tu ambiente (si estás en RStudio puedes verlas en el panel 3) deben aparecer las variables que hemos usado hasta ahora: [1] “maximo” “minimo” “nsim” [4] “desv.est” “datos.arbol” “p.gorro” [7] “datos.linea” “outliers” “valores_simulados” [10] “datos.barras” “F_simulado” “numerador” [13] “conteo_delitos” “p.val” “denominador” [16] “mis.datos” “calidad_alimentos” “i” [19] “tiros” “modelo.lineal” “rr_neg” [22] “n_muestra” “cuartil1” “rango_alimentos” [25] “datos.coches” “cuartil3” “datos_a_predecir” [28] “razon.momios” “conteo.dia” “F_empirica” [31] “conteo_tipo” “jesimo.dato” “densidad_kernel” [34] “mtcars” “x” “y” [37] “z” “x.barra” “rango_servicio” [40] “elevar.cuadrado” “iqr” “n.longitud” [43] “moda” “plano” “soles” [46] “datos” “calidad_servicio” “dats” [49] “datos_restaurante” “vector.ejemplo.1” “vector.ejemplo.2” [52] “experimento” “fumadores” “Example1” [55] “lim.inf” “Fgorro” “rr” [58] “calif” “lim.sup” Podemos probar sumar nuestras variables y todo funciona súper: [1] 300 Limpiemos el ambiente. El comando equivalente al clear all en R es un poco más complicado de memorizar: Ahora, si vuelves a ver el ambiente, éste estará vacío: ¡hemos limpiado el historial! Nota que si intentamos operar con las variables, R ya no las recuerda: ## Error in eval(expr, envir, enclos): object &#39;x&#39; not found Así como hay que lavarse las manos antes de comer, es buen hábito limpiar todas las variables del ambiente de R antes de usarlo. Podemos leer la base de datos usando load: [1] 300 Por último, es necesario resaltar la importancia del directorio. Para ello crea una nueva carpeta en tu escritorio de nombre Mi_curso_de_R. Mueve el archivo \"MisVariables.rda\" dentro de la carpeta. Borra todo e intenta leer de nuevo el archivo: ## Warning in readChar(con, 5L, useBytes = TRUE): cannot open compressed file ## &#39;MisVariables.rda&#39;, probable reason &#39;No such file or directory&#39; ## Error in readChar(con, 5L, useBytes = TRUE): cannot open the connection Este error es porque R sigue pensando que nuestro directorio es el escritorio y está buscando el archivo ahí sin hallarlo. Para encontrarlo hay que cambiar el directorio a través de RStudio (ya sea Ctrl+Shift+H o Session &gt;Set Working Directory &gt; Choose Directory) o bien a través de comandos en R: A.9.1 Ejercicio Responde a las siguientes preguntas: ¿Qué es el directorio y por qué es necesario establecerlo? Si R me da el error 'No such file or directory' ¿qué hice mal? En RStudio, ¿qué hace Session &gt; Restart R? ¿cuál es la diferencia con rm(list = ls())? ¿Qué hace el comando cat(\"\\014\")? (Ojo puede que no haga nada). Si funciona, ¿cuál es la diferencia con rm(list = ls()) y con Restart R? A.10 Instalación de paquetes Un paquete de R es un conjunto de funciones adicionales elaboradas por los usuarios, las cuales permiten hacer cosas adicionales en R. Para instalarlos requieres de una conexión a Internet (o bien puedes instalarlos a partir de un archivo, por ejemplo, mediante una USB). El comando de instalación es install.packages seguido del nombre del paquete. Por ejemplo (y por ocio) descarguemos el paquete beepr para hacer reproducir sonidos en la computadora12. Para ello: [...] * DONE (beepr) The downloaded source packages are in ‘/algun/lugar/downloaded_packages’ Esto significa que el paquete ha sido instalado. Nos interesa usar la función beep que emite un sonido (??beep para ver la ayuda). Si la llamamos así tal cual, nos da error: R es incapaz de hallar la función porque aún no le hemos dicho dónde se encuentra. Para ello podemos llamar al paquete mediante la función library y decirle a R que incluya las funciones que se encuentran dentro de beepr: El comando library le dice a R ¡hey, voy a usar unas funciones que creó alguien más y que están dentro del paquete beepr! De esta manera, al correr beep(3), R ya sabe dónde hallar la función y por eso no arroja error. A.10.1 Ejercicios NIVEL 1 Instala los paquetes tidyverse en R. De tidyverse haz lo necesario para que el siguiente bloque de código te arroje una gráfica: NIVEL 3 Instala el paquete devtools (para hacerlo probablemente necesites instalar más cosas en tu computadora; averigua cuáles) Usa devtools para instalar el paquete emoGG desde Github. Verifica que tu instalación fue correcta haciendo la siguiente gráfica: A.11 Comentarios adicionales sobre el formato Así como en el español existen reglas de gramática para ponernos todos de acuerdo y entendernos entre todos, en R también existen sugerencias a seguir para escribir tu código. Las sugerencias que aquí aparecen fueron adaptadas de las que utiliza el equipo de Google. No escribas líneas de más de 80 caracteres (si se salió de tu pantalla, mejor continúa en el siguiente renglón). Coloca espacios entre operadores +,*,/,-,&lt;-,=, &lt;, &lt;=, &gt;, &gt;=, == y usa paréntesis para agrupar: Intenta alinear la asignación de variables para legibilidad: Utiliza nombres que evoquen la variable que representas No utilices un nombre demasiado similar para cosas diferentes. Comenta: Figure A.17: Trad: Un periodista se acerca a un programador a preguntarle ¿qué hace que un código sea malo? -Sin comentarios. Siempre pon las llamadas a los paquetes y el directorio al inicio de tu archivo para que otro usuario sepa qué necesita. Código limpio y legible: es siempre preferible a código escrito con prisas : Figure A.18: Yo, leyendo mi código no comentado y con mala edición 6 meses después de haberlo hecho. Siempre escribe tu código pensando que alguien más (y ese alguien más puedes ser tú) va a leerlo. ¡No olvides comentar! Modelos de simulación más avanzados suelen hacerse en C, C++ o Fortran por su velocidad; empero, es necesario conocer más de programación.↩︎ La práctica hace al maestro↩︎ De ahí que se llame R porque la R es una mejor letra que la S (todos lo sabemos) -Atte. Rodrigo, el autor de este documento.↩︎ Mejor conocidos ahora como AT&amp;T, la compañía celular que nunca tiene señal.↩︎ Sus nombres empiezan con la letra R ¿coincidencia?↩︎ Si decidiste no instalar RStudio salta al final de esta sección.↩︎ Recuerda que un logaritmo base \\(a\\) te dice a qué potencia \\(b\\) tuve que elevar \\(a\\) para llegar a \\(b\\). Por ejemplo \\(\\log_{10}(100) = 2\\) te dice que para llegar al \\(100\\) tuviste que hacer \\(10^2\\).↩︎ Puede ser float, int, double pero no nos preocuparemos por eso.↩︎ En R el comando abs toma el valor absoluto.↩︎ Windows usa backslash. Y hay toda una historia detrás de ello↩︎ En los siguientes capítulos descargaremos paquetes más interesantes; pero no desprecies la utilidad de beepr yo lo he usado en múltiples ocasiones para que la computadora me avise que ya terminó de correr un código.↩︎ "],
["repaso-de-proba.html", "B Repaso de Proba B.1 Funciones indicadoras B.2 Conteo B.3 Espacios de probabilidad B.4 Probabilidad condicional B.5 Independencia B.6 Variables aleatorias y función de distribución (acumulada) B.7 Funciones de masa de probabilidad B.8 Funciones de densidad B.9 Teorema de cambio de variable unidimensional B.10 Probabilidad Multivariada B.11 Esperanza, varianza y covarianza B.12 Condicionamiento por otra variable aleatoria B.13 Funciones características B.14 Convergencias B.15 Ley de los grandes números B.16 Teorema del límite central", " B Repaso de Proba B.1 Funciones indicadoras Dado un conjunto \\(A\\) definimos la función indicadora de \\(A\\) como sigue: \\[ \\mathbb{I}_A (x)= \\begin{cases} 1 &amp; \\text{ si } x \\in A \\\\ 0 &amp; \\text{ si } x \\not\\in A \\end{cases} \\] La función indicadora cumple las siguientes propiedades: Sean \\(A,B\\) conjuntos; luego: \\(\\mathbb{I}_{A \\cap B}(x) = \\mathbb{I}_{A}(x) \\cdot \\mathbb{I}_{B}(x)\\) \\(\\mathbb{I}_{A \\cup B}(x) = \\mathbb{I}_{A}(x) + \\mathbb{I}_{B}(x) - \\mathbb{I}_{A}(x) \\cdot \\mathbb{I}_{B}(x)\\) \\(\\mathbb{E}_X[\\mathbb{I}_A(X)] = \\mathbb{P}(X\\in A)\\) Demostración: 1. Si \\(x\\in A \\cap B\\) pasa que \\(\\mathbb{I}_{A \\cap B}(x) = 1\\); además, por hipótesis \\(x\\in A\\) y \\(x \\in B\\) lo que implica que \\(\\mathbb{I}_{A}(x) = 1\\) y \\(\\mathbb{I}_{B}(x) = 1\\); en caso contrario \\(\\mathbb{I}_{A \\cap B}(x) = 1\\) y como no está en el conjunto al menos uno \\(\\mathbb{I}_{A}(x)\\) ó \\(\\mathbb{I}_{B}(x)\\) es cero. Esto concluye la prueba. 2. Demostración es similar 3. Para cualquier variable aleatoria \\(X\\), \\(\\mathbb{I}_{A}(X)\\) sólo toma dos valores: \\(0\\) si \\(X\\not\\in A\\) y \\(1\\) si \\(X\\in A\\). Luego: \\[ \\mathbb{E}_X[\\mathbb{I}_A(X)] = 1 \\cdot \\mathbb{P}(X\\in A) + 0 \\cdot \\mathbb{P}(X\\not\\in A) = \\mathbb{P}(X\\in A) \\] B.2 Conteo Intentemos resumir todas las formas de contar que tenemos con un ejemplo de Casella and Berger (2002). En la lotería de Nueva York se eligen \\(6\\) de \\(44\\) números para un ticket. ¿Cuántos boletos de lotería posibles hay? Veamos algunas formas posibles de solución13: Ordenado y sin reemplazo Si sólo importa el orden y una vez que sale un número no se vuelve a meter a los posibles entonces tenemos: \\[ \\frac{44!}{(44-6)!} \\] Ordenado y con reemplazo En cada uno de los \\(6\\) lugares hay \\(44\\) números posibles: \\[ 44^6 \\] Sin orden y sin reemplazo Esto es una combinación por lo que la forma de extraerlo es: \\[ \\binom{44}{6} \\] Sin orden y con reemplazo Para resolver este caso podemos usar la técnica de las barras y los puntos. Coloquemos barras y los huecos entre ellas representan cada uno de los \\(44\\) números. \\[\\begin{equation}\\nonumber |\\underbrace{\\_}_{1}|\\underbrace{\\_}_{2}|\\underbrace{\\_}_{3}|\\cdots |\\underbrace{\\_}_{n}| \\end{equation}\\] Coloquemos puntos (\\(\\circ\\)) donde estén los números seleccionados. Por ejemplo la siguiente representa la combinación \\(113555\\) \\[\\begin{equation}\\nonumber |\\underbrace{\\circ \\circ}_{1}|\\underbrace{\\_}_{2}|\\underbrace{\\circ}_{3}||\\underbrace{\\_}_{4}|\\underbrace{\\circ \\circ \\circ}_{5}|\\cdots |\\underbrace{\\_}_{n}| \\end{equation}\\] Tenemos entonces que el problema se reduce a colocar \\(n - 1= 43\\) barritas (son un total de \\(45\\) pero la primera y la última no deben cambiar de lugar) y \\(k = 6\\) círculos por tanto colocamos \\(49\\) elementos en total. De estos, nos interesa poner \\(6\\) por lo que tenemos: \\[ \\binom{44 + 6 - 1}{6} \\] formas distintas. Esto nos lleva a la tabla siguiente: Para obtener una muestra de tamaño \\(k\\) a partir de un conjunto de tamaño \\(n &gt; 0\\) éstas son las opciones: \\(\\quad \\text{Con Reemplazo}\\) \\(\\quad \\text{Sin Reemplazo}\\) \\(\\quad \\text{Con Orden}\\) \\(\\quad n^k\\) \\(\\quad (n)_k\\) \\(\\quad \\text{Sin Orden}\\) \\(\\quad \\binom{n+k-1}{k}\\) \\(\\quad \\binom{n}{k}\\) B.3 Espacios de probabilidad Los ingredientes para un modelo probabilístico son \\(3\\): Un conjunto \\(\\Omega\\) conocido como espacio muestral el cual es el conjunto de los resultados de interés. Por ejemplo, en el tiro de un dado \\(\\Omega = \\{1,2,3,4,5,6\\}\\), para el lanzamiento de una moneda \\(\\Omega = \\{\\text{Águila},\\text{Sol}\\}\\) o bien en seleccionar un número uniforme entre \\(0\\) y \\(1\\) tenemos que \\(\\Omega = [0,1]\\). Una colección \\(\\mathcal{F}\\) de subconjuntos de \\(\\Omega\\) conocida como sigma-álgebra o bien como espacio de eventos la cual cumple las siguientes características: \\(\\Omega \\in \\mathcal{F}\\) Si \\(A\\in\\mathcal{F}\\) entonces \\(A^C \\in \\mathcal{F}\\) Si \\(A_1, A_2, \\dots\\) es una colección finita ó numerable de elementos de \\(\\mathcal{F}\\) entonces \\(\\bigcup_{n} A_n \\in \\mathcal{F}\\) Generalmente identificamos a la \\(\\mathcal{F}\\) con el potencia para conjuntos \\(\\Omega\\) finitos; para casos infinitos el teorema de Vitali nos dice que las cosas son más complicadas. Una función \\(\\mathbb{P}:\\mathcal{F} \\to [0,1]\\) que cumple que: \\(\\mathbb{P}(\\Omega) = 1\\). \\(\\mathbb{P}(A) \\geq 0\\) para todo \\(A \\in \\mathcal{F}\\). Si \\(A_1, A_2, \\dots\\) es una colección finita ó numerable de conjuntos disjuntos (\\(A_i\\cap A_j = \\emptyset\\) para \\(i \\neq j\\)) entonces \\(\\mathbb{P}(\\bigcup_{n} A_n) = \\sum\\limits_{n}\\mathbb{P}(A_n)\\). Estos últimos tres puntos se conocen como Axiomas de Kolmogorov. Una vez armados con los axiomas podíamos empezar a probar cosas con ellos; por ejemplo: Sea \\((\\Omega,\\mathcal{F},\\mathbb{P})\\) un espacio de probabilidad. Sea \\(A\\) evento de \\(\\mathcal{F}\\). Luego: \\[ \\mathbb{P}(A^C) = 1 - \\mathbb{P}(A). \\] Para verlo, podemos escribir \\(\\Omega = A\\cup A^C\\) de donde se sigue que: \\[ 1 = \\mathbb{P}(\\Omega) = \\mathbb{P}(A \\cup A^C) = \\mathbb{P}(A) + \\mathbb{P}(A^C); \\] si despejamos obtenemos el resultado deseado. También podemos probar, por ejemplo: \\[ \\mathbb{P}(A\\setminus B) = \\mathbb{P}(A) - \\mathbb{P}(A \\cap B) \\] si escribimos \\(A = (A\\setminus B) \\cup (A \\cap B)\\) de donde se sigue que: \\[ \\mathbb{P}(A) = \\mathbb{P}\\big((A\\setminus B) \\cup (A \\cap B) \\big) = \\mathbb{P}(A\\setminus B) + \\mathbb{P} (A \\cap B) \\] y despejamos para tener el resultado deseado. Una última cosa de importancia es tomar \\(A,B\\) eventos de \\(\\mathcal{F}\\). Luego: \\[ \\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B) \\] Para verlo, escribimos \\(A\\cup B\\) como \\(A\\cup B = (A\\setminus B)\\cup (A \\cap B)\\cup (B\\setminus A)\\) luego: \\[\\begin{equation}\\nonumber \\begin{aligned} \\mathbb{P}(A\\cup B) &amp; = \\mathbb{P}(A\\setminus B) + \\mathbb{P} (A \\cap B) + \\mathbb{P}(B\\setminus A) \\\\ &amp; = \\mathbb{P}(A) - \\mathbb{P}(A \\cap B) + \\mathbb{P} (A \\cap B) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B) \\\\ &amp; = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B) \\end{aligned} \\end{equation}\\] B.4 Probabilidad condicional Muchas veces la probabilidad cambia conforme obtenemos información extra. Por ejemplo, si consideramos los tiros de un dado \\(\\Omega = \\{1,2,3,4,5,6\\}\\) y se sabe que cayó par \\(B = \\{2,4,6 \\}\\), la probabilidad de obtener \\(2\\) ó \\(4\\) (el evento) \\(A = \\{ 2, 4\\}\\) cambia de probabilidad: \\[ \\mathbb{P}(A | B) = \\dfrac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)} \\] En particular hay dos teoremas principales con probabilidad condicional: la ley de probabilidad total que te permite reconstruirlas probabilidades originales a partir de las condicionales y el de Bayes. El teorema de Bayes puede deducirse a partir de un simple despeje pues notamos que: \\[ \\mathbb{P}(A | B) = \\dfrac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)} \\] y por otro lado: \\[ \\mathbb{P}(B | A) = \\dfrac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(A)} \\] Si despejamos del segundo, obtenemos: \\[ \\mathbb{P}(B | A)\\mathbb{P}(A) = \\mathbb{P}(A \\cap B) \\] Podemos sustituir la definición de intersección en \\(\\mathbb{P}(A|B)\\) y así obtener: \\[ \\mathbb{P}(A | B) = \\dfrac{\\mathbb{P}(B | A)\\mathbb{P}(A)}{\\mathbb{P}(B)} \\] Por otro lado, dada una partición \\(B_1, B_2, \\dots\\) finita o numerable de \\(\\Omega\\) podemos definir la probabilidad de \\(A\\) en términos de cada uno de los pedazos: \\[ \\mathbb{P}(A) = \\sum\\limits_{k} \\mathbb{P}(A | B_k) \\cdot \\mathbb{P}(B_k) \\] Esta identidad se sigue de que: \\[ \\mathbb{P}(A | B_k) = \\dfrac{\\mathbb{P}(A \\cap B_k)}{\\mathbb{P}(B_k)} \\] de donde podemos sustituir arriba y obtener: \\[ \\mathbb{P}(A) = \\sum\\limits_{k} \\dfrac{\\mathbb{P}(A \\cap B_k)}{\\mathbb{P}(B_k)} \\cdot \\mathbb{P}(B_k) = \\sum\\limits_{k} \\mathbb{P}(A \\cap B_k) = \\mathbb{P}\\big(A \\cap (\\bigcup_k B_k) \\big) = \\mathbb{P}\\big(A \\cap \\Omega \\big) \\] Tenemos entonces el teorema siguiente: Sean $B_1, B_2, $ eventos que forman una partición de \\(\\Omega\\); sea \\(A\\) un evento cualquiera; luego: \\[ \\mathbb{P}(A) = \\sum\\limits_{k} \\mathbb{P}(A | B_k) \\cdot \\mathbb{P}(B_k) \\] Usando probabilidad condicional podemos resolver problemas como el siguiente: Considera el conjunto \\(C = \\{1,2,\\dots, n\\}\\) para \\(n \\geq 2\\). Se extraen dos números \\(a\\) y \\(b\\) (primero el \\(a\\) y luego el \\(b\\)) con probabilidad uniforme sin reemplazo. Determina la probabilidad de que \\(a &gt; b\\). Podemos utilizar probabilidad condicional para representar el evento: \\[\\begin{equation}\\nonumber \\begin{aligned} \\mathbb{P}(a &gt; b) &amp; = \\sum\\limits_{k = 1}^{n} \\mathbb{P}(a &gt; b \\quad | \\quad a = k) \\mathbb{P}(a = k) \\end{aligned} \\end{equation}\\] Donde \\(\\mathbb{P}(a = k) = \\frac{1}{n}\\) para todos los \\(k\\) pues es uniforme (y es el primero en salir). Luego: \\[\\begin{equation}\\nonumber \\begin{aligned} \\mathbb{P}(a &gt; b) &amp; = \\sum\\limits_{k = 1}^{n} \\mathbb{P}(a &gt; b \\quad | \\quad a = k) \\mathbb{P}(a = k) \\\\ &amp; = \\dfrac{1}{n} \\sum\\limits_{k = 1}^{n} \\mathbb{P}(a &gt; b \\quad | \\quad a = k) \\\\ &amp; = \\dfrac{1}{n} \\sum\\limits_{k = 1}^{n} \\mathbb{P}(k &gt; b \\quad | \\quad a = k) \\\\ \\end{aligned} \\end{equation}\\] Notamos que cuando \\(k = 1\\) no hay forma de que \\(k &gt; b\\); cuando \\(k = 2\\) hay una única forma (que \\(b\\) valga \\(1\\)); cuando \\(k = 3\\) hay dos formas. En general para una \\(k\\) genérica hay \\(k-1\\) formas de seleccionar un \\(b\\) menor a \\(k\\) luego: \\[\\begin{equation}\\nonumber \\begin{aligned} \\mathbb{P}(a &gt; b) &amp; = \\dfrac{1}{n} \\sum\\limits_{k = 1}^{n} \\dfrac{k-1}{n} \\\\ &amp; = \\dfrac{1}{n^2} \\sum\\limits_{k = 1}^{n} k-1 \\\\ &amp; = \\dfrac{1}{n^2} \\sum\\limits_{k = 0}^{n-1} k \\\\ &amp; = \\dfrac{1}{n^2} \\dfrac{n(n-1)}{2} \\\\ &amp; = \\dfrac{n-1}{2n} \\end{aligned} \\end{equation}\\] B.5 Independencia Dos eventos \\(A,B\\) son independientes si: \\[ \\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\mathbb{P}(B) \\] Intuitivamente esto significa que saber \\(A\\) no me dice nada de \\(B\\) pues la independencia implica que: \\[ \\mathbb{P}(A | B) = \\mathbb{P}(A) \\] B.6 Variables aleatorias y función de distribución (acumulada) Para hablar de probabilidad uno de los ingredientes principales eran las variables aleatorias. Éstas son funciones (no son variables ni son aleatorias) de tal manera que su imagen inversa pertenece a la sigma-álgebra \\(\\mathcal{F}\\): Una función \\(X: \\Omega \\to \\mathbb{R}\\) es una variable aleatoria si: \\[ X^{-1}(A) = \\{ \\omega \\in \\Omega \\quad : \\quad X(\\omega) \\in A \\} \\in \\mathcal{F} \\] para todo \\(A\\subseteq\\textrm{Dom}_X\\) En general la pregunta \\(\\mathbb{P}(X \\in A)\\) la traducíamos a una pregunta sobre conjuntos: \\[ \\mathbb{P}(X \\in A) = \\mathbb{P}\\Big( \\{ \\omega \\in \\Omega \\quad : \\quad X(\\omega) \\in A \\} \\Big) \\] y esto nos permitía hablar de probabilidades. En particular, construíamos la función de distribución acumulada como sigue: Definimos la función de distribución acumulada de una variable aleatoria \\(X: \\Omega \\to \\mathbb{R}\\) como: \\[ F_X(x) = \\mathbb{P}(X \\leq x) \\] donde \\(X\\) es la variable aleatoria y \\(x\\in\\mathbb{R}\\) es un real. La función de distribución acumulada cumplía varias propiedades: \\(\\lim_{x \\to \\infty} F_X(x) = 1\\) \\(\\lim_{x \\to -\\infty} F_X(x) = 0\\) \\(F_X\\) es no decreciente. \\(F_X\\) es continua por la derecha. \\(F_X\\) tiene límites por la izquierda. Los puntos 4 y 5 se resumen diciendo que la función es càdlág. Tener la acumulada nos permitía calcular probabilidades de intervalos; por ejemplo: \\[ \\mathbb{P}(a &lt; X \\leq b) = F_X(b) - F_X(a) \\] o bien: \\[ \\mathbb{P}(X &lt; x) = \\lim_{z \\to x^-} F(z) \\] Las funciones de distribución acumulada más comunes se veían como en la imagen: Si una función de distribución acumulada \\(F_X\\) era continua entonces decíamos que la variable aleatoria asociada (\\(X\\)) es continua. En particular, la continuidad implica que: \\[ \\mathbb{P}(X = k) = 0 \\qquad \\forall k \\] B.7 Funciones de masa de probabilidad Si una variable aleatoria \\(X\\) tomaba una cantidad finita o numerable de valores decíamos que \\(X\\) es una variable aleatoria discreta. Dentro de las variables aleatorias discretas teníamos varios modelos. Una cosa importante de las variables aleatorias es la función de masa de probabilidad que se define como: Dada una variable aleatoria discreta \\(X\\) definimos la función de masa de probabilidad de \\(X\\) como la función \\(p:\\mathbb{R} \\to \\mathbb{R}\\) tal que: \\[ p(x) = \\mathbb{P}(X = x) \\] para todo \\(x \\in\\mathbb{R}\\). Algunos modelos importantes son: Sea \\(A = \\{ a_1, a_2, \\dots, a_n \\}\\) un conjunto finito de \\(n\\) elementos. Una variable aleatoria \\(X\\) tiene una distribución uniforme discreta si: \\[ \\mathbb{P}\\big( X = a_k \\big) = \\dfrac{1}{n} \\cdot \\mathbb{I}_{A}(a_k) \\qquad \\forall k \\in \\{ 1, 2, \\dots, n \\} \\] Un modelo particular salía de considerar el siguiente problema: Tenemos una moneda que cae Águila con probabilidad \\(p\\) y Sol con probabilidad \\((1-p)\\) (con \\(0 &lt; p &lt; 1\\)). Nos interesa saber cuál es la probabilidad de tener \\(k\\) Águilas en \\(n\\) tiros. Solución A fin de resolver este problema notamos que necesitamos acomodar las \\(k\\) águilas en los \\(n\\) tiros para ello hay \\(\\binom{n}{k}\\) formas de hacerlo; cada águila cae con probabilidad \\(p\\) y hay \\(k\\); como son independientes esto nos da \\(p^k\\); por otro lado hay \\(n-k\\) soles cada uno cayó con probabilidad \\((1-p)\\). Esta lógica da origen al modelo binomial: Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Binomial}(n,p)\\) si: \\[ \\mathbb{P}\\big(X = k \\big) = \\binom{n}{k} p^k (1-p)^{n-k} \\mathbb{I}_{\\{0,1,2,\\dots,n \\}}(k) \\] Una pregunta distinta que nos pudimos hacer fue: Tenemos una moneda que cae Águila con probabilidad \\(p\\) y Sol con probabilidad \\((1-p)\\) (con \\(0 &lt; p &lt; 1\\)). Arrojamos la moneda hasta obtener \\(r\\) Águilas y en ese momento nos detenemos. Determina la probabilidad de que se aviente la moneda \\(k\\) veces. Para ello notamos que la última Águila está fija por lo que sólo debemos poner las \\(r-1\\) Águilas en los \\(k-1\\) lugares restantes, \\(\\binom{k-1}{r-1}\\). Por otro lado, cada Águila tiene probabilidad \\(p\\) y como son \\(k\\) tiros independientes entonces tenemos \\(p^r\\); para los soles tenemos \\((1-p)^{k-r}\\). Esto nos genera el modelo Binomial Negativo: Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Binomial Negativa}(r,p)\\) si: \\[ \\mathbb{P}\\big(X = k \\big) = \\binom{k-1}{r-1} p^r (1-p)^{k-r} \\mathbb{I}_{\\{r, r+1, r+2, \\dots \\}}(k) \\] Finalmente, otro modelo que pudimos hacer con monedas es un caso específico del Binomial Negativo . Aquí la pregunta es, se tira una moneda que tiene probabilidad \\(p\\) de salir Águila hasta que se obtiene el águila. Contamos cuántos tiros ocurrieron hasta que ocurriera el primer Águila y la pregunta de interés es la probabilidad de haber realizado específicamente \\(k\\) tiros. Para ello necesitamos tener \\((k-1)\\) tiros que fueran sol: \\((1-p)^{k-1}\\) y un tiro que saliera águila \\(p\\). Esto nos genera el modelo geométrico: Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Geométrica}(p)\\) si: \\[ \\mathbb{P}\\big(X = k \\big) = (1-p)^k p \\cdot \\mathbb{I}_{\\mathbb{N}}(k). \\] Otro modelo de interés es el siguiente: Se tiene una población de tamaño \\(M\\) donde \\(N\\) individuos pertenecen al partido político AZUL y \\(M-N\\) pertenecen al VERDE Se toma una submuestra de tamaño \\(m\\). Determina la probabilidad de que haya \\(n\\) individuos del partido político AZUL. Para ello notamos que hay \\(\\binom{M}{m}\\) muestras totales. Por otro lado, necesitamos extraer de los \\(N\\) azules a una submuestra de \\(n\\): \\(\\binom{N}{n}\\); finalmente, de los \\(M\\) verdes necesitamos extraer una submuestra de \\(m\\), hay \\(\\binom{M-N}{m-n}\\) formas de hacerlo. Concluimos entonces con el modelo hipergeométrico: Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Hipergeométrica}(M,N,m)\\) si: \\[ \\mathbb{P}\\big(X = n \\big) = \\dfrac{\\binom{M-N}{m-n} \\binom{N}{n} }{\\binom{M}{m}} \\cdot \\mathbb{I}_{\\{0,1,\\dots, \\text{mín}\\{m, N\\} \\}} (n) \\] El modelo Poisson va a ser bastante útil. Para estudiarlo, consideremos un modelo. Vamos a pensar en un servidor de computación (piensa en una página de Internet) que recibe solicitudes de entrar a la página de manera independiente y aleatoria en un intervalo de tiempo entre \\(t = 0\\) y \\(t = 1\\). Como primera aproximación podemos dividir el intervalo en \\(n\\) pedazos cada uno de longitud \\(1/n\\) y asumir que, a fuerza, sólo una conexión se puede realizar en cada uno de esos pedazos. Finalmente, asumamos que la probabilidad \\(p\\) de que se haga una conexión es proporcional a la longitud del intervalo y sea \\(p = \\lambda / n\\). Con estas hipótesis, la probabilidad de tener \\(k\\) conexiones (\\(k\\) entero entre \\(0\\) y \\(n\\)) está dada por un modelo binomial: \\[\\begin{equation}\\nonumber \\begin{aligned} f_n(k) &amp; = \\binom{n}{k} \\Big( \\frac{\\lambda}{n} \\Big)^k \\Big(1 - \\frac{\\lambda}{n} \\Big)^k \\ &amp; = \\frac{\\lambda^k}{k!} \\Big( 1 - \\frac{\\lambda}{n})^n \\frac{n!}{n^k(n-k)!} \\Big( 1 - \\frac{\\lambda}{n})^{-k} \\end{aligned} \\end{equation}\\] de donde concluimos que si continuamos partiendo el intervalo en pedazos cada vez más pequeños obtenemos: \\[\\begin{equation}\\nonumber \\begin{aligned} \\lim_{n \\to \\infty} f_n(k) &amp; = \\frac{e^{-\\lambda} \\lambda^k}{k!} \\end{aligned} \\end{equation}\\] Esto resulta en el modelo Poisson: Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Poisson}(\\lambda)\\) si: \\[ \\mathbb{P}\\big(X = k \\big) = \\dfrac{\\lambda^k e^{-\\lambda}}{k!} \\mathbb{I}_{\\mathbb{N}\\cup \\{ 0 \\}}(k) \\] B.8 Funciones de densidad Por construcción, las variables aleatorias continuas no tienen una función de masa de probabilidad (recuerda que \\(\\mathbb{P}(X = k) = 0\\) si \\(X\\) es continua para todo \\(k\\)). Sin embargo, es posible definir, si \\(F_X\\) es diferenciable algo similar, la función de densidad. Para una variable aleatoria \\(X\\) con función de distribución acumulada \\(F_X\\) diferenciable, definimos la función de densidad como: \\[ f_X(x) = \\dfrac{d}{dx} F_X(x) \\] Notamos que una función de densidad no es una probabilidad y no necesariamente sigue las mismas reglas; lo único que se requiere es: \\(f_X(x) \\geq 0\\) para toda \\(x\\). \\(\\int\\limits_{-\\infty}^{\\infty} f_X(x) dx = 1\\). La primer función de densidad es la que a un intervalo \\([a,b]\\) (ya sea abierto, cerrado o como sea) asigna a cada subintervalo una probabilidad proporcional a su longitud. Éste es el modelo uniforme: Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Uniforme}(a,b)\\) si: \\[ f_X(x) = \\dfrac{1}{b-a}\\mathbb{I}_{(a,b)}(x) \\] Una generalización del modelo uniforme es el beta (eventualmente veremos de dónde sale): Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Beta}(\\alpha,\\beta)\\) si: \\[ f_X(x) = \\dfrac{x^{\\alpha - 1}(1-x)^{\\beta - 1}}{B(\\alpha, \\beta)}\\mathbb{I}_{(0,1)}(x) \\] donde \\[ B(\\alpha, \\beta) = \\dfrac{\\Gamma (\\alpha) \\Gamma (\\beta)}{\\Gamma (\\alpha + \\beta)} \\] Podemos deducir el modelo exponencial a partir de la descripción del Poisson. Volvamos al mismo problema del \\(\\textrm{Poisson}(\\lambda)\\) donde hay computadoras conectándose a un servidor. Sea \\(W\\) la variable aleatoria que denota el tiempo de espera hasta el primer evento. Analicemos su distribución acumulada; notamos que \\[ F_W(w) = \\mathbb{P}(W \\leq w) = 1 - \\mathbb{P}(W &gt; w) \\] Ahora, para que \\(W &gt; w\\) eso significa que ningún evento tuvo que haber ocurrido en los primeros \\(w\\) minutos (horas, lo que sea la unidad de tiempo). Y ese evento es equivalente a que nuestra variable aleatoria Poisson (tasa \\(\\lambda w\\))14 no tenga ningún arribo: \\[ \\mathbb{P}(X = 0) = \\dfrac{(\\lambda w)^0 e^{-\\lambda w}}{0!} = e^{-\\lambda} \\] De donde se obtiene la función de distribución acumulada: \\[ F_W(w) = 1 - e^{-\\lambda w} \\] De donde, al derivar respecto a \\(w\\), se obtiene el modelo exponencial: Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Exponencial}(\\lambda)\\) si: \\[ f_X(x) = \\lambda e^{-\\lambda x} \\mathbb{I}_{(0,\\infty)}(x) \\] Para deducir la distribución gamma, vamos a preguntarnos por exactamente el mismo proceso pero esta vez, en lugar de preguntarnos por el tiempo para la primer conexión nos preguntaremos por el tiempo para la \\(\\alpha\\)-ésima conexión. Para ello, sea \\(W_{\\alpha}\\) el tiempo hasta la \\(\\alpha\\)-ésima conexión. Usamos el mismo truco del complemento que la vez pasada: \\[ F_{W_{\\alpha}}(w) = \\mathbb{P}(W_{\\alpha} \\leq w) = 1 - \\mathbb{P}(W_{\\alpha} &gt; w) \\] Y notamos que para que \\(W_{\\alpha} &gt; w\\) entonces a lo más debieron haber \\(\\alpha-1\\) conexiones. Podemos reescribir: \\[ F_{W_{\\alpha}}(w) = 1 - \\mathbb{P}(W_{\\alpha} &gt; w) = 1 - \\sum\\limits_{k = 0}^{\\alpha - 1} \\dfrac{(\\lambda w)^k e^{-\\lambda w}}{k!} = 1 - e^{- \\lambda w} - \\sum\\limits_{k = 1}^{\\alpha - 1} \\dfrac{(\\lambda w)^k e^{-\\lambda w}}{k!} \\] Derivamos: \\[\\begin{equation}\\nonumber \\begin{aligned} \\dfrac{d}{dw}F_{W_{\\alpha}}(w) &amp; = -\\lambda e^{- \\lambda w} - \\sum\\limits_{k = 1}^{\\alpha - 1} \\dfrac{k \\lambda (\\lambda w)^{k-1} e^{-\\lambda w} - \\lambda (\\lambda w)^k e^{-\\lambda w}}{k!} \\ &amp; = -\\lambda e^{- \\lambda w} - \\lambda e^{- \\lambda w} \\sum\\limits_{k = 1}^{\\alpha - 1} \\underbrace{\\dfrac{(\\lambda w)^{k-1}}{(k-1)!} - \\dfrac{(\\lambda w)^k }{k!}}_{\\text{Telescópica}} \\ &amp; = -\\lambda e^{- \\lambda w} + \\lambda e^{- \\lambda w} \\Bigg( \\dfrac{(\\lambda w)^{\\alpha - 1} }{(\\alpha - 1)!} - 1 \\Bigg) \\ &amp; = \\lambda e^{- \\lambda w} \\dfrac{(\\lambda w)^{\\alpha - 1} }{(\\alpha - 1)!} \\ &amp; = \\dfrac{\\beta^{\\alpha} }{\\Gamma (\\alpha)} w^{\\alpha - 1} e^{- \\frac{w}{\\beta}} \\ \\end{aligned} \\end{equation}\\] donde tomamos \\(\\beta = \\frac{1}{\\lambda}\\). Esto sugiere el modelo gamma: Una variable aleatoria \\(W\\) tiene una distribución \\(\\text{Gamma}(\\alpha,\\beta)\\) si: \\[ f_W(w) = \\dfrac{\\beta^{\\alpha} }{\\Gamma (\\alpha)} w^{\\alpha - 1} e^{- \\frac{w}{\\beta}} \\mathbb{I}_{(0,\\infty)} \\] para \\(\\alpha,\\beta &gt; 0\\). Para deducir el modelo normal consideremos lo siguiente. Pensemos que estamos midiendo la posición de las estrellas en el cielo. Para ello hay dos formas. Bajo coordenadas cartesianas \\((x,y)\\) pensemos que el error de medición es independiente; es decir, si \\(f(x,y)\\) es la densidad de los errores entonces: \\[ \\rho (x,y) = f(x) f(y) \\] Por otro lado, asumamos que existe también una representación en coordenadas polares de la posición de la estrella: \\[ g (r, \\theta) = g(r) \\] donde el error de medición depende sólo del radio (no del ángulo). Notamos entonces que: \\[ f(x) f(y) = g\\Big( \\sqrt{x^2 + y^2} \\Big) \\] Si tomamos \\(y = 0\\) tenemos que \\(f(x) f(0) = g(x)\\) (asumo \\(x &gt; 0\\); los otros casos son similares). Podemos entonces sustituir: \\[ \\dfrac{f(x) f(y)}{f(0)^2} = \\dfrac{f\\Big( \\sqrt{x^2 + y^2} \\Big) }{f(0)} \\] Tomamos logaritmo: \\[ \\ln \\dfrac{f(x)}{f(0)} + \\ln \\dfrac{f(y)}{f(0)} = \\ln \\dfrac{f\\Big( \\sqrt{x^2 + y^2} \\Big) }{f(0)} \\] Notamos que una solución es que: \\[ \\ln \\dfrac{f(x)}{f(0)} = \\alpha x^2 \\] de donde despejamos y obtenemos: \\[ f(x) = \\frac{1}{f(0)} e^{\\alpha x^2} \\] Finalmente sabemos que debe integrar a \\(1\\) y por tanto esto fuerza a \\(\\alpha\\) a ser negativo. En particular tomaremos \\(\\alpha = -\\frac{1}{2}\\) \\[ f(x) = \\frac{1}{f(0)} e^{-\\frac{1}{2} x^2} \\] Y para que integre a \\(1\\):s \\[ f(x) = \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{1}{2} x^2} \\] Por último, notamos que si \\(Z\\sim \\textrm{Normal}(0,1)\\) entonces \\(X = \\sigma Z + \\mu\\) tiene la densidad dada por15: \\[ f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{1}{2\\sigma^2} (x - \\mu)^2} \\] Una variable aleatoria \\(X\\) tiene una distribución \\(\\text{Normal}(\\mu,\\sigma)\\) si: \\[ f_X(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}} \\] B.9 Teorema de cambio de variable unidimensional Supongamos que tenemos una variable aleatoria \\(X\\) y nos interesa ver cómo se ve la \\(X\\) después de aplicarle una función \\(\\phi\\). Por ejemplo, si \\(X\\sim\\textrm{Normal}(0,1)\\) la función de densidad de \\(e^X\\) está dada por: \\[ f_X(x) = \\dfrac{1}{x \\sqrt{2 \\pi \\sigma^2}}e^{-(\\ln(x) - \\mu)^2/2\\sigma^2} \\mathbb{I}_{(0,\\infty)}(x). \\] Lo cual cambia mucho la forma de la distribución: La pregunta es, cómo obtener la función de densidad de \\(X\\) si se conoce la función \\(\\phi\\); el teorema de cambio de variable nos da una respuesta cuando \\(\\phi\\) es monótona estrictamente creciente o bien estrictamente decreciente y diferenciable. Sea \\(X\\) una variable aleatoria continua y \\(\\phi\\) una función estrictamente creciente ó estrictamente decreciente y diferenciable. Entonces: \\[ f_{\\phi(X)}(t) = f_X( \\phi^{-1}(t) ) \\cdot \\left| \\dfrac{d}{dt} \\phi^{-1}(t) \\right| \\] DEM: Caso estrictamente decreciente Como \\(\\phi\\) es estrictamente decreciente es invertible y por tanto: \\[\\begin{equation}\\nonumber \\begin{aligned} F_{\\phi(X)}(t) &amp; = \\mathbb{P}(\\phi(X) \\leq t) \\\\ &amp; = \\mathbb{P}(X \\geq \\phi^{-1}(t) ) \\\\ &amp; = 1 - \\mathbb{P}(X \\leq \\phi^{-1}(t) ) \\\\ &amp; = 1 - F_X( \\phi^{-1}(t) ) \\end{aligned} \\end{equation}\\] luego derivamos respecto a \\(t\\): \\[\\begin{equation}\\nonumber \\begin{aligned} f_{\\phi(X)}(t) &amp; = \\dfrac{d}{dt} F_{\\phi(X)}(t) \\\\ &amp; = - \\dfrac{d}{dt} F_X( \\phi^{-1}(t) ) \\\\ &amp; = - f_X( \\phi^{-1}(t) ) \\cdot \\dfrac{d}{dt} \\phi^{-1}(t) \\\\ &amp; = f_X( \\phi^{-1}(t) ) \\cdot \\left| \\dfrac{d}{dt} \\phi^{-1}(t) \\right| \\end{aligned} \\end{equation}\\] Donde el valor absoluto sale de que \\(\\phi^{-1}(t) &lt; 0\\) por ser estrictamente decreciente la \\(\\phi\\). B.10 Probabilidad Multivariada De la misma manera que hablamos de una sola variable aleatoria podemos hablar de muchas como múltiples funciones de \\(\\Omega \\in \\mathbb{R}\\). Para una colección finita \\(\\{ X_i \\}_{i = 1}^n\\) de variables aleatorias podemos hablar de su función de distribución acumulada conjunta como: \\[ F_{\\vec{X}}(x_1, x_2, \\dots, x_n) = \\mathbb{P}\\big( X_1 \\leq x_1, X_2 \\leq x_2, \\dots, X_n \\leq x_n) \\] donde suponemos que \\(\\vec{X} = (X_1, X_2, \\dots, X_n)^T\\) es un vector aleatorio cuyas entradas son las variables de la colección anterior. En el caso de que las \\(n\\) variables sean discretas la función de masa conjunta está dada por: \\[ p_{\\vec{X}}(x_1, x_2, \\dots, x_n) = \\mathbb{P}(X_1 = x_1, X_2 = x_2, \\dots, X_n = x_n) \\] En el caso de que sean continuas (\\(F_{\\vec{X}}\\) diferenciable en sus \\(n\\) entradas) entonces la densidad está dada por: \\[ f_{\\vec{X}}(x_1, x_2, \\dots, x_n) = \\dfrac{\\partial^n}{\\partial x_1 \\partial x_2 \\dots \\partial x_n}F_{\\vec{X}}\\Bigg|_{(x_1, x_2, \\dots, x_n)} \\] En general la función de probabilidad conjunta siempre va a esta dada por: \\[ \\mathbb{P}(X_1 \\in A_1, X_2 \\in A_2, \\dots, X_n \\in A_n) = \\mathbb{P}\\Big(\\{ \\omega \\in \\Omega : X_1(\\omega) \\in A_1 \\text{ y } X_2(\\omega) \\in A_2 \\text{ y } \\dots \\text{ y } X_n(\\omega) \\in A_n \\}\\Big) \\] para \\(A_1, A_2, \\dots, A_n\\) medibles (bajo \\(X_1, X_2, \\dots, X_n\\) respectivamente). Dos variables aleatorias \\(X_i\\) y \\(X_j\\) (\\(i \\neq j\\)) son independientes si: \\[ \\mathbb{P}(X_i \\in A, X_i \\in B) = \\mathbb{P}(X_i \\in A) \\cdot \\mathbb{P}(X_j \\in B) \\] para \\(A,B\\) medibles. Una colección \\(\\{ X_i \\}_{i}\\) de variables aleatorias es completamente independiente si para cualquier subcolección finita \\(\\{ X_{i_k} \\}_{i_k}\\) se tiene que: \\[ \\mathbb{P}(X_{i_1} \\in A_{i_1}, X_{i_2} \\in A_{i_2}, \\dots, X_{i_n} \\in A_{i_n} ) = \\prod_{k = 1}^n \\mathbb{P}(X_{i_k} \\in A) \\] en el contexto de estas notas, a menos que se indique lo contrario, las variables aleatorias que utilicemos serán completamente independientes. Un aspecto interesante de la independencia es que permite partir las funciones de masa, densidad y distribución acumulada en dos funciones independientes. Así, si \\(X,Y\\) son independientes con masa conjunta \\(p\\): \\[ p_{X,Y}(x,y) = \\mathbb{P}(X = x, Y = y) = \\mathbb{P}(X = x)\\cdot\\mathbb{P}(Y = y) = p_X(x)\\cdot p_Y(y) \\] El resultado se mantiene para distribuciones: \\[ F_{X,Y}(x,y) = \\mathbb{P}(X \\leq x, Y \\leq y) = \\mathbb{P}(X \\leq x)\\cdot\\mathbb{P}(Y \\leq y) = F_X(x)\\cdot F_Y(y) \\] y si derivamos (en caso de \\(F\\) diferenciable), se mantiene para densidades: \\[ f_{X,Y}(x,y) = \\dfrac{\\partial^2}{\\partial x\\partial y} F_{X,Y}\\Big|_{(x,y)} = \\dfrac{\\partial^2}{\\partial x\\partial y} F_X(x)\\cdot F_Y(y)\\Big|_{(x,y)} = f_X(x) f_Y(y) \\] B.11 Esperanza, varianza y covarianza Para una función medible \\(g\\) de una variable aleatoria \\(X\\) definimos su valor esperado (si existe) como: \\[ \\mathbb{E}\\big[g(X)\\big] = \\begin{cases} \\sum\\limits_{x \\in \\text{Supp}(X)} g(x) \\cdot \\mathbb{P}(X = x) &amp; \\text{ si } X \\text{ discreta.} \\\\ \\int\\limits_{-\\infty}^{\\infty} g(x) \\cdot f_X(x) dx&amp; \\text{ si } X \\text{ continua} \\end{cases} \\] donde \\(f_X\\) es la densidad de \\(X\\) en el caso continuo y \\(\\text{Supp}(X)\\) es el conjunto imagen de \\(X\\) (el soporte): \\[ \\text{Supp}(X) = \\{ x : X(\\omega) = x \\text{ para } \\omega \\in \\Omega \\} \\] En el caso de conjuntos finitos de variables aleatorias la definción es similar: Para una función \\(g:\\mathbb{R}^n \\to \\mathbb{R}\\) multivariada de \\(n\\) variables aleatorias (sobre los reales) \\(X_1, X_2, \\dots, X_n\\) definimos su valor esperado (si existe y sin pérdida de generalidad suponiendo las primeras \\(j\\) son discretas y las últimas \\(n - (j + 1)\\) continuas) como: \\[\\begin{equation}\\nonumber \\begin{aligned} \\mathbb{E}\\big[ g(X_1, X_2, \\dots, X_n) \\big] = \\\\ &amp; \\int_{-\\infty}^{\\infty} \\dots \\int_{-\\infty}^{\\infty} \\sum_{x_j \\in \\text{Supp}(X_j)} \\dots \\sum_{x_1 \\in \\text{Supp}(X_1)} g(x_1, x_2, \\dots, x_n) p(x_1) \\dots p(x_j) f_{X_{j+1}}(x_{j+1}) \\dots f_{X_{n}}(x_{n}) dx_{j+1} \\dots dx_{n} \\end{aligned} \\end{equation}\\] donde \\(p(x_j)\\) es la masa de \\(X_j\\) (es decir \\(p(x_j) = \\mathbb{P}(X_j = x_j)\\). En el caso particular de dos variables aleatorias \\(X_1\\) y \\(X_2\\) podemos escribir la expresión de manera más sencilla: \\[\\begin{equation}\\nonumber \\begin{aligned} \\mathbb{E}\\big[ &amp; g(X_1, X_2) \\big] = \\begin{cases} \\int\\limits_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(x_1, x_2) f_{X_1}(x_1) f_{X_2}(x_2) dx_1 dx_2 &amp; \\text{ ambas continuas,} \\\\ \\\\ \\sum\\limits_{x \\in \\text{Supp}(X_1)} \\sum\\limits_{x \\in \\text{Supp}(X_2)} g(x_1, x_2) p(x_1) p(x_2) &amp; \\text{ ambas discretas,} \\\\ \\\\ \\int_{-\\infty}^{\\infty} \\sum\\limits_{x \\in \\text{Supp}(X_1)} g(x_1, x_2) p(x_1) p(x_2) &amp; X_1 \\text{ discreta, } X_2 \\text{ continua.} \\\\ \\end{cases} \\end{aligned} \\end{equation}\\] En particular, en el espacio de las variables aleatorias definimos un producto interno, la covarianza la cual está dada por: \\[ \\textrm{Cov}(X_1, X_2) = \\mathbb{E}\\Big[ \\big(X_1 - \\mathbb{E}[X_1]\\big) \\cdot \\big(X_2 - \\mathbb{E}[X_2]\\big) \\Big] \\] La varianza es un caso particular de la covarianza: cuando \\(X_1 = X_2\\): \\[ \\textrm{Cov}(X_1, X_1) = \\mathbb{E}\\Big[ \\big(X_1 - \\mathbb{E}[X_1]\\big)^2 \\Big] \\] B.11.1 Propiedades de valor esperado, varianza y covarianza El valor esperado al ser representable mediante sumas ó integrales cumple todas las propiedades de las sumas (resp integrales) en particular la linealidad: \\[ \\mathbb{E}\\Big[ a X + Y\\Big] = a \\mathbb{E}[X] + \\mathbb{E}[Y] \\] La demostración se hace exactamente igual en el caso de variables discretas, continuas (ó mezcla de una y una). Aquí muestro la de continuas con densidades \\(f_X\\) y \\(f_Y\\): \\[\\begin{equation} \\begin{aligned} \\mathbb{E}\\Big[ a X + Y\\Big] &amp; = \\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\infty} (a x + y) f_{X,Y}(x,y) dx dy \\\\ &amp; = a \\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\infty} x f_{X,Y}(x,y) dx dy + \\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\infty} y f_{X,Y}(x,y) dx dy \\\\ &amp; = a \\Big[ \\int\\limits_{-\\infty}^{\\infty} x f_X(x) dx \\Big] + \\int\\limits_{-\\infty}^{\\infty} y f_Y(y) dy \\\\ &amp; = a \\mathbb{E}[X] + \\mathbb{E}[Y] \\end{aligned} \\end{equation}\\] Otro resultado importante es que si dos variables aleatorias \\(X,Y\\) son independientes entonces el valor esperado del producto se parte: \\[ \\mathbb{E}[XY] = \\mathbb{E}[X] \\cdot \\mathbb{E}[Y] \\] La demostración se hace de manera idéntica en todos los casos. Aquí mostramos el caso de \\(X,Y\\) discretas: \\[\\begin{equation} \\begin{aligned} \\mathbb{E}\\Big[XY\\Big] &amp; = \\sum\\limits_{y \\in \\text{Sup}(Y)} \\sum\\limits_{x \\in \\text{Sup}(X)} xy \\mathbb{P}(X = x, Y = y) \\\\ &amp; = \\sum\\limits_{y \\in \\text{Sup}(Y)} \\sum\\limits_{x \\in \\text{Sup}(X)} xy \\mathbb{P}(X = x) \\mathbb{P}(Y = y) \\\\ &amp; = \\Big[\\sum\\limits_{y \\in \\text{Sup}(Y)} y \\mathbb{P}(Y = y)\\Big] \\Big[\\sum\\limits_{x \\in \\text{Sup}(X)} x \\mathbb{P}(X = x)\\Big] \\\\ &amp; = \\mathbb{E}[X] \\cdot \\mathbb{E}[Y] \\end{aligned} \\end{equation}\\] La linealidad nos permite reescribir la covarianza: \\[\\begin{equation} \\begin{aligned} \\textrm{Cov}(X_1, X_2) &amp; = \\mathbb{E}\\Big[ \\big(X_1 - \\mathbb{E}[X_1]\\big) \\cdot \\big(X_2 - \\mathbb{E}[X_2]\\big) \\Big] \\\\ &amp; = \\mathbb{E}\\Big[ X_1 X_2 \\Big] - \\mathbb{E}\\Big[X_1\\Big]\\mathbb{E}\\Big[X_2\\Big] - \\mathbb{E}\\Big[X_1\\Big]\\mathbb{E}\\Big[X_2\\Big] + \\mathbb{E}\\Big[X_1\\Big]\\mathbb{E}\\Big[X_2\\Big] \\\\ &amp; = \\mathbb{E}\\Big[ X_1 X_2 \\Big] - \\mathbb{E}\\Big[X_1\\Big]\\mathbb{E}\\Big[X_2\\Big] \\end{aligned} \\end{equation}\\] de tal forma que es claro que si \\(X_1\\) y \\(X_2\\) son independientes entonces \\(\\textrm{Cov}(X_1, X_2) = 0\\) por la propiedad anterior del valor esperado. OJO De manera general covarianza \\(0\\) no implica que las variables sean independientes como puede verse con las variables aleatorias siguientes: \\[ f_{X,Y}(x,y) = \\begin{cases} 1/8 &amp; \\text{ si } (x,y) \\in \\{ (-1,-1), (-1,1), (1, -1), (1,1)\\} \\\\ 1/2 &amp; \\text{ si } (x,y) = (0,0), \\\\ 0 &amp; \\text{ en otro caso} \\end{cases} \\] las cuales no son independientes pues \\(\\mathbb{P}(X = 0, Y = 0) = 1/2\\neq 1/4 = \\mathbb{P}(X = 0)\\cdot \\mathbb{P}(Y = 0)\\); sin embargo (ejercicio sugerido) la covarianza es \\(0\\). Una segunda propiedad de interés de la covarianza es que actúa como el producto interno (de hecho es uno): \\[ \\text{Cov}(a X + bY, cW + dV) = ac \\text{Cov}(X,W) + ad \\text{Cov}(X,V) + bc \\text{Cov}(Y,W) + bd \\text{Cov}(Y,V) \\] la cual se demuestra igual mediante la linealidad: \\[\\begin{equation} \\begin{aligned} \\textrm{Cov}(a &amp; X + bY, cW + dV) = \\mathbb{E}\\Big[ (a X + bY) (cW + dV) \\Big] - \\mathbb{E}\\Big[a X + bY\\Big]\\mathbb{E}\\Big[cW + dV\\Big] \\\\ &amp; = \\mathbb{E}\\Big[ ac XW + bc YW + ad XV + bd YV\\Big] - \\bigg( a \\mathbb{E}\\Big[ X \\Big] + b\\mathbb{E}\\Big[ Y\\Big]\\bigg)\\bigg( c\\mathbb{E}\\Big[W\\Big] + d\\mathbb{E}\\Big[V\\Big] \\bigg) \\\\ &amp; = ac \\text{Cov}(X,W) + ad \\text{Cov}(X,V) + bc \\text{Cov}(Y,W) + bd \\text{Cov}(Y,V) \\end{aligned} \\end{equation}\\] donde la última igualdad se sigue de agrupar los términos idénticos tras sus constantes. B.12 Condicionamiento por otra variable aleatoria A rellenarse pronto B.13 Funciones características A rellenarse pronto B.14 Convergencias A rellenarse pronto B.14.1 Teorema de continuidad de Lévy A rellenarse pronto B.15 Ley de los grandes números A rellenarse pronto B.16 Teorema del límite central A rellenarse pronto "]
]
