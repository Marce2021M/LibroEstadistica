<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 4 Muestreo Aleatorio Simple | Estadística I: Análisis exploratorio de datos y muestreo</title>
  <meta name="description" content="Libro de estadística aplicada: temas de análisis exploratorio y muestreo" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 4 Muestreo Aleatorio Simple | Estadística I: Análisis exploratorio de datos y muestreo" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Libro de estadística aplicada: temas de análisis exploratorio y muestreo" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 4 Muestreo Aleatorio Simple | Estadística I: Análisis exploratorio de datos y muestreo" />
  
  <meta name="twitter:description" content="Libro de estadística aplicada: temas de análisis exploratorio y muestreo" />
  

<meta name="author" content="Rodrigo Zepeda-Tello" />


<meta name="date" content="2021-08-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="análisis-exploratorio-de-datos.html"/>
<link rel="next" href="muestreo-aleatorio-estratificado.html"/>
<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Estadística I: Análisis exploratorio de datos y muestreo</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Historia del muestro</a></li>
<li class="chapter" data-level="2" data-path="conceptos.html"><a href="conceptos.html"><i class="fa fa-check"></i><b>2</b> Conceptos</a></li>
<li class="chapter" data-level="3" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html"><i class="fa fa-check"></i><b>3</b> Análisis Exploratorio de Datos</a>
<ul>
<li class="chapter" data-level="3.1" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#inicio"><i class="fa fa-check"></i><b>3.1</b> Inicio</a></li>
<li class="chapter" data-level="3.2" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#librerías"><i class="fa fa-check"></i><b>3.2</b> Librerías</a></li>
<li class="chapter" data-level="3.3" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#base-a-analizar"><i class="fa fa-check"></i><b>3.3</b> Base a analizar</a></li>
<li class="chapter" data-level="3.4" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#definiciones-y-notación"><i class="fa fa-check"></i><b>3.4</b> Definiciones y notación</a></li>
<li class="chapter" data-level="3.5" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#estadísticos-univariados"><i class="fa fa-check"></i><b>3.5</b> Estadísticos univariados</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#definición-estadístico"><i class="fa fa-check"></i><b>3.5.1</b> Definición [Estadístico]</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#ejercicio"><i class="fa fa-check"></i><b>3.6</b> Ejercicio</a></li>
<li class="chapter" data-level="3.7" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#ejercicios"><i class="fa fa-check"></i><b>3.7</b> Ejercicios</a></li>
<li class="chapter" data-level="3.8" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#gráficas-univariadas"><i class="fa fa-check"></i><b>3.8</b> Gráficas univariadas</a></li>
<li class="chapter" data-level="3.9" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#gráficas-bivariadas"><i class="fa fa-check"></i><b>3.9</b> Gráficas bivariadas</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#ejercicio-1"><i class="fa fa-check"></i><b>3.9.1</b> Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#estadísticos-bivariados"><i class="fa fa-check"></i><b>3.10</b> Estadísticos bivariados</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#ejercicio-2"><i class="fa fa-check"></i><b>3.10.1</b> Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#ejercicio-3"><i class="fa fa-check"></i><b>3.11</b> Ejercicio</a></li>
<li class="chapter" data-level="3.12" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#ajuste-funcional"><i class="fa fa-check"></i><b>3.12</b> Ajuste funcional</a>
<ul>
<li class="chapter" data-level="3.12.1" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#ejercicio-4"><i class="fa fa-check"></i><b>3.12.1</b> Ejercicio</a></li>
<li class="chapter" data-level="3.12.2" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#ejercicio-sugerido"><i class="fa fa-check"></i><b>3.12.2</b> Ejercicio sugerido</a></li>
</ul></li>
<li class="chapter" data-level="3.13" data-path="análisis-exploratorio-de-datos.html"><a href="análisis-exploratorio-de-datos.html#ejercicios-del-capítulo"><i class="fa fa-check"></i><b>3.13</b> Ejercicios del capítulo</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html"><i class="fa fa-check"></i><b>4</b> Muestreo Aleatorio Simple</a>
<ul>
<li class="chapter" data-level="4.1" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#inicio-1"><i class="fa fa-check"></i><b>4.1</b> Inicio</a></li>
<li class="chapter" data-level="4.2" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#librerías-1"><i class="fa fa-check"></i><b>4.2</b> Librerías</a></li>
<li class="chapter" data-level="4.3" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#notación"><i class="fa fa-check"></i><b>4.3</b> Notación</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#ejemplo"><i class="fa fa-check"></i><b>4.3.1</b> Ejemplo</a></li>
<li class="chapter" data-level="4.3.2" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#ejercicio-5"><i class="fa fa-check"></i><b>4.3.2</b> Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#muestreo-aleatorio-simple-sin-reemplazo-massr"><i class="fa fa-check"></i><b>4.4</b> Muestreo Aleatorio Simple sin Reemplazo (MAS/sR)</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#ejercicio-6"><i class="fa fa-check"></i><b>4.4.1</b> Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#teorema-del-límite-central-aplicación"><i class="fa fa-check"></i><b>4.5</b> Teorema del Límite Central (Aplicación)</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#estimación-de-intervalos-de-confianza-para-el-total"><i class="fa fa-check"></i><b>4.5.1</b> Estimación de intervalos de confianza para el total</a></li>
<li class="chapter" data-level="4.5.2" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#ejemplo-con-simulación"><i class="fa fa-check"></i><b>4.5.2</b> Ejemplo con simulación:</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#ejemplo-resumen-estimación-de-una-proporción-bajo-muestreo-aleatorio-simple-sin-reemplazo"><i class="fa fa-check"></i><b>4.6</b> Ejemplo Resumen: Estimación de una proporción bajo muestreo aleatorio simple sin reemplazo</a></li>
<li class="chapter" data-level="4.7" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#ejemplo-resumen-estimación-del-total-de-individuos-en-una-fotografía"><i class="fa fa-check"></i><b>4.7</b> Ejemplo Resumen: Estimación del total de individuos en una fotografía</a></li>
<li class="chapter" data-level="4.8" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#ejercicio-7"><i class="fa fa-check"></i><b>4.8</b> Ejercicio:</a></li>
<li class="chapter" data-level="4.9" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#ejemplo-resumen-estimación-de-una-región-crítica"><i class="fa fa-check"></i><b>4.9</b> Ejemplo Resumen: Estimación de una región crítica</a></li>
<li class="chapter" data-level="4.10" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#ejemplo-resumen-estimación-del-total-de-una-población"><i class="fa fa-check"></i><b>4.10</b> Ejemplo Resumen: Estimación del total de una población</a></li>
<li class="chapter" data-level="4.11" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#demostración-del-teorema-del-límite-central-para-muestras-finitas"><i class="fa fa-check"></i><b>4.11</b> Demostración del Teorema del Límite Central para Muestras Finitas</a></li>
<li class="chapter" data-level="4.12" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#muestreo-aleatorio-simple-bernoulli-be"><i class="fa fa-check"></i><b>4.12</b> Muestreo Aleatorio Simple Bernoulli (BE)</a>
<ul>
<li class="chapter" data-level="4.12.1" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#ejercicio-8"><i class="fa fa-check"></i><b>4.12.1</b> Ejercicio</a></li>
<li class="chapter" data-level="4.12.2" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#ejemplo-1"><i class="fa fa-check"></i><b>4.12.2</b> Ejemplo</a></li>
<li class="chapter" data-level="4.12.3" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#un-mejor-estimador-el-proporcional-al-tamaño"><i class="fa fa-check"></i><b>4.12.3</b> Un mejor estimador: el proporcional al tamaño</a></li>
</ul></li>
<li class="chapter" data-level="4.13" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#ejemplo-resumen-aduana"><i class="fa fa-check"></i><b>4.13</b> Ejemplo Resumen: Aduana</a></li>
<li class="chapter" data-level="4.14" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#muestreo-aleatorio-simple-con-reemplazo-mascr"><i class="fa fa-check"></i><b>4.14</b> Muestreo Aleatorio Simple con Reemplazo (MAS/cR)</a></li>
<li class="chapter" data-level="4.15" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#ejemplo-resumen-proporción-de-trabajadores-enfermos-con-o-sin-reemplazo"><i class="fa fa-check"></i><b>4.15</b> Ejemplo Resumen: Proporción de trabajadores enfermos con o sin reemplazo</a></li>
<li class="chapter" data-level="4.16" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#ejemplo-resumen-captura-recaptura-con-reemplazo"><i class="fa fa-check"></i><b>4.16</b> Ejemplo Resumen: Captura-Recaptura con reemplazo</a></li>
<li class="chapter" data-level="4.17" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#muestreo-aleatorio-simple-ponderado-masp"><i class="fa fa-check"></i><b>4.17</b> Muestreo Aleatorio Simple Ponderado (MAS/P)</a></li>
<li class="chapter" data-level="4.18" data-path="muestreo-aleatorio-simple.html"><a href="muestreo-aleatorio-simple.html#ejercicios-1"><i class="fa fa-check"></i><b>4.18</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="muestreo-aleatorio-estratificado.html"><a href="muestreo-aleatorio-estratificado.html"><i class="fa fa-check"></i><b>5</b> Muestreo Aleatorio Estratificado</a>
<ul>
<li class="chapter" data-level="5.1" data-path="muestreo-aleatorio-estratificado.html"><a href="muestreo-aleatorio-estratificado.html#introducción-a-muestreo-aleatorio-estratificado-mae"><i class="fa fa-check"></i><b>5.1</b> Introducción a Muestreo Aleatorio Estratificado (MAE)</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="muestreo-aleatorio-estratificado.html"><a href="muestreo-aleatorio-estratificado.html#ejemplo-2"><i class="fa fa-check"></i><b>5.1.1</b> Ejemplo</a></li>
<li class="chapter" data-level="5.1.2" data-path="muestreo-aleatorio-estratificado.html"><a href="muestreo-aleatorio-estratificado.html#ejercicio-sugerido-1"><i class="fa fa-check"></i><b>5.1.2</b> Ejercicio sugerido</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="muestreo-aleatorio-estratificado.html"><a href="muestreo-aleatorio-estratificado.html#alocación"><i class="fa fa-check"></i><b>5.2</b> Alocación</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="muestreo-aleatorio-estratificado.html"><a href="muestreo-aleatorio-estratificado.html#alocación-proporcional-al-tamaño"><i class="fa fa-check"></i><b>5.2.1</b> Alocación proporcional al tamaño</a></li>
<li class="chapter" data-level="5.2.2" data-path="muestreo-aleatorio-estratificado.html"><a href="muestreo-aleatorio-estratificado.html#alocación-óptima"><i class="fa fa-check"></i><b>5.2.2</b> Alocación óptima</a></li>
<li class="chapter" data-level="5.2.3" data-path="muestreo-aleatorio-estratificado.html"><a href="muestreo-aleatorio-estratificado.html#ejemplo-3"><i class="fa fa-check"></i><b>5.2.3</b> Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="muestreo-aleatorio-estratificado.html"><a href="muestreo-aleatorio-estratificado.html#ejercicio-de-clase"><i class="fa fa-check"></i><b>5.3</b> Ejercicio de clase:</a></li>
<li class="chapter" data-level="5.4" data-path="muestreo-aleatorio-estratificado.html"><a href="muestreo-aleatorio-estratificado.html#ejercicio-en-r-tipo-control"><i class="fa fa-check"></i><b>5.4</b> Ejercicio en R tipo control</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="intervalos-de-confianza-mediante-bootstrap.html"><a href="intervalos-de-confianza-mediante-bootstrap.html"><i class="fa fa-check"></i><b>6</b> Intervalos de Confianza mediante bootstrap</a>
<ul>
<li class="chapter" data-level="6.1" data-path="intervalos-de-confianza-mediante-bootstrap.html"><a href="intervalos-de-confianza-mediante-bootstrap.html#inicio-2"><i class="fa fa-check"></i><b>6.1</b> Inicio</a></li>
<li class="chapter" data-level="6.2" data-path="intervalos-de-confianza-mediante-bootstrap.html"><a href="intervalos-de-confianza-mediante-bootstrap.html#intervalos-asintóticos"><i class="fa fa-check"></i><b>6.2</b> Intervalos asintóticos</a></li>
<li class="chapter" data-level="6.3" data-path="intervalos-de-confianza-mediante-bootstrap.html"><a href="intervalos-de-confianza-mediante-bootstrap.html#muestreo-basado-en-modelos"><i class="fa fa-check"></i><b>6.3</b> Muestreo basado en modelos</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="intervalos-de-confianza-mediante-bootstrap.html"><a href="intervalos-de-confianza-mediante-bootstrap.html#ejemplo-poisson"><i class="fa fa-check"></i><b>6.3.1</b> Ejemplo Poisson</a></li>
<li class="chapter" data-level="6.3.2" data-path="intervalos-de-confianza-mediante-bootstrap.html"><a href="intervalos-de-confianza-mediante-bootstrap.html#ejercicio-9"><i class="fa fa-check"></i><b>6.3.2</b> Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="intervalos-de-confianza-mediante-bootstrap.html"><a href="intervalos-de-confianza-mediante-bootstrap.html#intervalos-bootstrap"><i class="fa fa-check"></i><b>6.4</b> Intervalos Bootstrap</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="intervalos-de-confianza-mediante-bootstrap.html"><a href="intervalos-de-confianza-mediante-bootstrap.html#boostrap-bajo-un-modelo"><i class="fa fa-check"></i><b>6.4.1</b> Boostrap bajo un modelo</a></li>
<li class="chapter" data-level="6.4.2" data-path="intervalos-de-confianza-mediante-bootstrap.html"><a href="intervalos-de-confianza-mediante-bootstrap.html#boostrap-de-una-muestra"><i class="fa fa-check"></i><b>6.4.2</b> Boostrap de una muestra</a></li>
<li class="chapter" data-level="6.4.3" data-path="intervalos-de-confianza-mediante-bootstrap.html"><a href="intervalos-de-confianza-mediante-bootstrap.html#ejemplo-4"><i class="fa fa-check"></i><b>6.4.3</b> Ejemplo</a></li>
<li class="chapter" data-level="6.4.4" data-path="intervalos-de-confianza-mediante-bootstrap.html"><a href="intervalos-de-confianza-mediante-bootstrap.html#ejercicio-10"><i class="fa fa-check"></i><b>6.4.4</b> Ejercicio</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="muestreo-aleatorio-multietápico.html"><a href="muestreo-aleatorio-multietápico.html"><i class="fa fa-check"></i><b>7</b> Muestreo Aleatorio Multietápico</a>
<ul>
<li class="chapter" data-level="7.1" data-path="muestreo-aleatorio-multietápico.html"><a href="muestreo-aleatorio-multietápico.html#muestreo-aleatorio-por-clusters-en-una-sola-etapa"><i class="fa fa-check"></i><b>7.1</b> Muestreo aleatorio por clusters en una sola etapa</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="muestreo-aleatorio-multietápico.html"><a href="muestreo-aleatorio-multietápico.html#ejemplo-5"><i class="fa fa-check"></i><b>7.1.1</b> Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="muestreo-aleatorio-multietápico.html"><a href="muestreo-aleatorio-multietápico.html#muestreo-aleatorio-por-clusters-bietápico-en-dos-etapas"><i class="fa fa-check"></i><b>7.2</b> Muestreo aleatorio por clusters bietápico (en dos etapas)</a></li>
<li class="chapter" data-level="7.3" data-path="muestreo-aleatorio-multietápico.html"><a href="muestreo-aleatorio-multietápico.html#ejemplo-disco-duro"><i class="fa fa-check"></i><b>7.3</b> Ejemplo: Disco duro</a></li>
<li class="chapter" data-level="7.4" data-path="muestreo-aleatorio-multietápico.html"><a href="muestreo-aleatorio-multietápico.html#ejemplo-encuesta-nacional-de-salud"><i class="fa fa-check"></i><b>7.4</b> Ejemplo: Encuesta Nacional de Salud</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="referencias.html"><a href="referencias.html"><i class="fa fa-check"></i>Referencias</a></li>
<li class="appendix"><span><b>Apéndice</b></span></li>
<li class="chapter" data-level="A" data-path="programación-en-r.html"><a href="programación-en-r.html"><i class="fa fa-check"></i><b>A</b> Programación en <code>R</code></a>
<ul>
<li class="chapter" data-level="A.1" data-path="programación-en-r.html"><a href="programación-en-r.html#algunas-ventajas-de-r-y-cosas-no-tan-padres"><i class="fa fa-check"></i><b>A.1</b> Algunas ventajas de <code>R</code> y cosas no tan padres</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="programación-en-r.html"><a href="programación-en-r.html#puntos-a-favor-de-r"><i class="fa fa-check"></i><b>A.1.1</b> Puntos a favor de <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="programación-en-r.html"><a href="programación-en-r.html#bienvenidx-a-r-camp-pontanezen-sí-así-se-llama-esta-versión"><i class="fa fa-check"></i><b>A.2</b> Bienvenidx a <code>R</code>, Camp Pontanezen (sí, así se llama esta versión)</a></li>
<li class="chapter" data-level="A.3" data-path="programación-en-r.html"><a href="programación-en-r.html#instalando-cosas"><i class="fa fa-check"></i><b>A.3</b> Instalando cosas</a>
<ul>
<li class="chapter" data-level="A.3.1" data-path="programación-en-r.html"><a href="programación-en-r.html#instalación-de-r"><i class="fa fa-check"></i><b>A.3.1</b> Instalación de <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="programación-en-r.html"><a href="programación-en-r.html#instalación-de-rstudio"><i class="fa fa-check"></i><b>A.4</b> Instalación de <code>RStudio</code></a></li>
<li class="chapter" data-level="A.5" data-path="programación-en-r.html"><a href="programación-en-r.html#primeros-pasos-en-r-usando-rstudio"><i class="fa fa-check"></i><b>A.5</b> Primeros pasos en <code>R</code> usando <code>RStudio</code></a></li>
<li class="chapter" data-level="A.6" data-path="programación-en-r.html"><a href="programación-en-r.html#cálculos-numéricos"><i class="fa fa-check"></i><b>A.6</b> Cálculos numéricos</a>
<ul>
<li class="chapter" data-level="A.6.1" data-path="programación-en-r.html"><a href="programación-en-r.html#ejercicio-11"><i class="fa fa-check"></i><b>A.6.1</b> Ejercicio</a></li>
<li class="chapter" data-level="A.6.2" data-path="programación-en-r.html"><a href="programación-en-r.html#ejercicio-12"><i class="fa fa-check"></i><b>A.6.2</b> Ejercicio</a></li>
<li class="chapter" data-level="A.6.3" data-path="programación-en-r.html"><a href="programación-en-r.html#respuestas"><i class="fa fa-check"></i><b>A.6.3</b> Respuestas</a></li>
</ul></li>
<li class="chapter" data-level="A.7" data-path="programación-en-r.html"><a href="programación-en-r.html#variables"><i class="fa fa-check"></i><b>A.7</b> Variables</a>
<ul>
<li class="chapter" data-level="A.7.1" data-path="programación-en-r.html"><a href="programación-en-r.html#ejercicios-2"><i class="fa fa-check"></i><b>A.7.1</b> Ejercicios</a></li>
<li class="chapter" data-level="A.7.2" data-path="programación-en-r.html"><a href="programación-en-r.html#nivel-3"><i class="fa fa-check"></i><b>A.7.2</b> NIVEL 3</a></li>
</ul></li>
<li class="chapter" data-level="A.8" data-path="programación-en-r.html"><a href="programación-en-r.html#observaciones-sobre-la-aritmética-de-punto-flotante"><i class="fa fa-check"></i><b>A.8</b> Observaciones sobre la aritmética de punto flotante</a>
<ul>
<li class="chapter" data-level="A.8.1" data-path="programación-en-r.html"><a href="programación-en-r.html#cómo-checar-un-if"><i class="fa fa-check"></i><b>A.8.1</b> ¿Cómo checar un if?</a></li>
</ul></li>
<li class="chapter" data-level="A.9" data-path="programación-en-r.html"><a href="programación-en-r.html#leer-y-almacenar-variables-en-r"><i class="fa fa-check"></i><b>A.9</b> Leer y almacenar variables en <code>R</code></a>
<ul>
<li class="chapter" data-level="A.9.1" data-path="programación-en-r.html"><a href="programación-en-r.html#ejercicio-13"><i class="fa fa-check"></i><b>A.9.1</b> Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="A.10" data-path="programación-en-r.html"><a href="programación-en-r.html#instalación-de-paquetes"><i class="fa fa-check"></i><b>A.10</b> Instalación de paquetes</a>
<ul>
<li class="chapter" data-level="A.10.1" data-path="programación-en-r.html"><a href="programación-en-r.html#ejercicios-3"><i class="fa fa-check"></i><b>A.10.1</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="A.11" data-path="programación-en-r.html"><a href="programación-en-r.html#comentarios-adicionales-sobre-el-formato"><i class="fa fa-check"></i><b>A.11</b> Comentarios adicionales sobre el formato</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="repaso-de-proba.html"><a href="repaso-de-proba.html"><i class="fa fa-check"></i><b>B</b> Repaso de Proba</a>
<ul>
<li class="chapter" data-level="B.1" data-path="repaso-de-proba.html"><a href="repaso-de-proba.html#funciones-indicadoras"><i class="fa fa-check"></i><b>B.1</b> Funciones indicadoras</a></li>
<li class="chapter" data-level="B.2" data-path="repaso-de-proba.html"><a href="repaso-de-proba.html#conteo"><i class="fa fa-check"></i><b>B.2</b> Conteo</a></li>
<li class="chapter" data-level="B.3" data-path="repaso-de-proba.html"><a href="repaso-de-proba.html#espacios-de-probabilidad"><i class="fa fa-check"></i><b>B.3</b> Espacios de probabilidad</a></li>
<li class="chapter" data-level="B.4" data-path="repaso-de-proba.html"><a href="repaso-de-proba.html#probabilidad-condicional"><i class="fa fa-check"></i><b>B.4</b> Probabilidad condicional</a></li>
<li class="chapter" data-level="B.5" data-path="repaso-de-proba.html"><a href="repaso-de-proba.html#independencia"><i class="fa fa-check"></i><b>B.5</b> Independencia</a></li>
<li class="chapter" data-level="B.6" data-path="repaso-de-proba.html"><a href="repaso-de-proba.html#variables-aleatorias-y-función-de-distribución-acumulada"><i class="fa fa-check"></i><b>B.6</b> Variables aleatorias y función de distribución (acumulada)</a></li>
<li class="chapter" data-level="B.7" data-path="repaso-de-proba.html"><a href="repaso-de-proba.html#funciones-de-masa-de-probabilidad"><i class="fa fa-check"></i><b>B.7</b> Funciones de masa de probabilidad</a></li>
<li class="chapter" data-level="B.8" data-path="repaso-de-proba.html"><a href="repaso-de-proba.html#funciones-de-densidad"><i class="fa fa-check"></i><b>B.8</b> Funciones de densidad</a></li>
<li class="chapter" data-level="B.9" data-path="repaso-de-proba.html"><a href="repaso-de-proba.html#teorema-de-cambio-de-variable-unidimensional"><i class="fa fa-check"></i><b>B.9</b> Teorema de cambio de variable unidimensional</a></li>
<li class="chapter" data-level="B.10" data-path="repaso-de-proba.html"><a href="repaso-de-proba.html#probabilidad-multivariada"><i class="fa fa-check"></i><b>B.10</b> Probabilidad Multivariada</a></li>
<li class="chapter" data-level="B.11" data-path="repaso-de-proba.html"><a href="repaso-de-proba.html#esperanza-varianza-y-covarianza"><i class="fa fa-check"></i><b>B.11</b> Esperanza, varianza y covarianza</a>
<ul>
<li class="chapter" data-level="B.11.1" data-path="repaso-de-proba.html"><a href="repaso-de-proba.html#propiedades-de-valor-esperado-varianza-y-covarianza"><i class="fa fa-check"></i><b>B.11.1</b> Propiedades de valor esperado, varianza y covarianza</a></li>
</ul></li>
<li class="chapter" data-level="B.12" data-path="repaso-de-proba.html"><a href="repaso-de-proba.html#condicionamiento-por-otra-variable-aleatoria"><i class="fa fa-check"></i><b>B.12</b> Condicionamiento por otra variable aleatoria</a></li>
<li class="chapter" data-level="B.13" data-path="repaso-de-proba.html"><a href="repaso-de-proba.html#funciones-características"><i class="fa fa-check"></i><b>B.13</b> Funciones características</a></li>
<li class="chapter" data-level="B.14" data-path="repaso-de-proba.html"><a href="repaso-de-proba.html#convergencias"><i class="fa fa-check"></i><b>B.14</b> Convergencias</a>
<ul>
<li class="chapter" data-level="B.14.1" data-path="repaso-de-proba.html"><a href="repaso-de-proba.html#teorema-de-continuidad-de-lévy"><i class="fa fa-check"></i><b>B.14.1</b> Teorema de continuidad de Lévy</a></li>
</ul></li>
<li class="chapter" data-level="B.15" data-path="repaso-de-proba.html"><a href="repaso-de-proba.html#ley-de-los-grandes-números"><i class="fa fa-check"></i><b>B.15</b> Ley de los grandes números</a></li>
<li class="chapter" data-level="B.16" data-path="repaso-de-proba.html"><a href="repaso-de-proba.html#teorema-del-límite-central"><i class="fa fa-check"></i><b>B.16</b> Teorema del límite central</a>
<ul>
<li class="chapter" data-level="B.16.1" data-path="repaso-de-proba.html"><a href="repaso-de-proba.html#programación-en-r-del-teorema-del-límite-central-con-variables-aleatorias-independientes-idénticamente-distribuidas"><i class="fa fa-check"></i><b>B.16.1</b> Programación en <code>R</code> del teorema del límite central con variables aleatorias independientes idénticamente distribuidas</a></li>
<li class="chapter" data-level="B.16.2" data-path="repaso-de-proba.html"><a href="repaso-de-proba.html#ejercicio-14"><i class="fa fa-check"></i><b>B.16.2</b> Ejercicio</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/RodrigoZepeda/LibroEstadistica" target="blank">Rodrigo Zepeda Tello</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Estadística I: Análisis exploratorio de datos y muestreo</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="muestreo-aleatorio-simple" class="section level1" number="4">
<h1><span class="header-section-number">Capítulo 4</span> Muestreo Aleatorio Simple</h1>
<div id="inicio-1" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Inicio</h2>
<p>Siempre que inicies un nuevo trabajo en <code>R</code> ¡no olvides borrar el historial!</p>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="muestreo-aleatorio-simple.html#cb158-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>()) <span class="co">#Clear all</span></span></code></pre></div>
</div>
<div id="librerías-1" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Librerías</h2>
<p>Para este análisis vamos a tener que llamar a las siguientes librerías previamente instaladas (por única vez) con <code>install.packages</code>:</p>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="muestreo-aleatorio-simple.html#cb159-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb159-2"><a href="muestreo-aleatorio-simple.html#cb159-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb159-3"><a href="muestreo-aleatorio-simple.html#cb159-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(imager)</span>
<span id="cb159-4"><a href="muestreo-aleatorio-simple.html#cb159-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rlist)</span>
<span id="cb159-5"><a href="muestreo-aleatorio-simple.html#cb159-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb159-6"><a href="muestreo-aleatorio-simple.html#cb159-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kableExtra)</span></code></pre></div>
</div>
<div id="notación" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Notación</h2>
<p>Supongamos una matriz de datos de tamaño <span class="math inline">\(N \times L\)</span> dada por:
<span class="math display">\[
U = \begin{pmatrix} 
z_1 \Big| z_2 \Big| \dots \Big| z_{\ell}
\end{pmatrix} 
\]</span></p>
<p>donde las <span class="math inline">\(z_i\)</span> representan las columnas de la matriz. La <span class="math inline">\(U\)</span> será conocida como la <strong>matriz universo</strong> (el <em>universo</em> ó <em>la población</em>) si contiene <em>toda</em> la información de la población. Intuitivamente, la matriz <span class="math inline">\(U\)</span> son todos los datos de un censo: esta es una matriz <em>ideal</em> donde están todos los datos.</p>
<p>A cualquier permutación en las filas de una submatriz <span class="math inline">\(S\)</span> (tamaño <span class="math inline">\(n \times \ell\)</span> con <span class="math inline">\(0 &lt; n \leq N\)</span> y <span class="math inline">\(0 &lt; \ell \leq L\)</span>) de <span class="math inline">\(U\)</span> se le conoce como una <em>muestra</em> de <span class="math inline">\(U\)</span>. Si <span class="math inline">\(S\)</span> es una variable aleatoria (por ejemplo, porque se construyó a partir de un mecanismo de aleatoreidad) decimos que <span class="math inline">\(S\)</span> es una <strong>muestra aleatoria</strong> (denotamos <span class="math inline">\(\mathcal{S}\)</span> a la variable aleatoria y <span class="math inline">\(S\)</span> a un valor específico de la misma).<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>En particular en esta sección (y hasta establecer lo contrario) consideraremos que el universo <span class="math inline">\(U\)</span> es de tamaño <span class="math inline">\(N \times 1\)</span> y la submatriz <span class="math inline">\(S\)</span> (resp. <span class="math inline">\(\mathcal{S}\)</span>) es de tamaño <span class="math inline">\(n \times 1\)</span>. En notación <span class="math inline">\(U = (x_1, x_2, \dots, x_N)^T\)</span> y <em>técnicamente</em> <span class="math inline">\(S = (x_{i_1}, x_{i_2}, \dots, x_{i_n})^T\)</span> para un conjunto de índices <span class="math inline">\(i_1, i_2, \dots, i_n\)</span>. Sin embargo para simplificar la notación consideraremos que en <span class="math inline">\(S\)</span> están los primeros <span class="math inline">\(n\)</span> de los <span class="math inline">\(x_i\)</span>; es decir:
<span class="math display">\[
S = (x_1, x_2, \dots, x_n)^T
\]</span></p>
<p>Cuando estemos hablando de la muestra <em>como variable aleatoria</em> <span class="math inline">\(\mathcal{S}\)</span> y no como <em>valores observados (fijos)</em> <span class="math inline">\(S\)</span>, denotaremos:
<span class="math display">\[
\mathcal{S} = (X_1, X_2, \dots, X_n)^T
\]</span>
donde <span class="math inline">\(\mathcal{S}\)</span> representa la muestra posible y cada <span class="math inline">\(X_i\)</span> es una variable aleatoria con el valor posible de la i-ésima entrada.</p>
<p>Un <strong>esquema muestral</strong> es una función <span class="math inline">\(\mathbb{P}\)</span> de probabilidad definida en el conjunto de submatrices de <span class="math inline">\(U\)</span>. Ésta es el punto medular de todas las estrategias de muestreo: distintos esquemas muestrales generan diferentes distribuciones y pueden llevar a distintas inferencias sobre un fenómeno.</p>
<div id="ejemplo" class="section level3" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Ejemplo</h3>
<p>Considera la matriz universo con tres letras:
<span class="math display">\[
U = \begin{pmatrix}
\text{A} \\
\text{B} \\
\text{C} 
\end{pmatrix}
\]</span>
Ésta es la matriz universo. Las submatrices<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> que pueden crearse a partir de dicho universo son:</p>
<ol style="list-style-type: decimal">
<li><p>De dimensión <span class="math inline">\(n = 1\)</span>: <span class="math inline">\(S^1 = (\text{A})^T\)</span>, <span class="math inline">\(S^2 = (\text{B})^T\)</span>, <span class="math inline">\(S^3 = (\text{C})^T\)</span>.</p></li>
<li><p>De dimensión <span class="math inline">\(n = 2\)</span>: <span class="math inline">\(S^4 = (\text{A}, \text{B})^T\)</span>, <span class="math inline">\(S^5 = (\text{A}, \text{C})^T\)</span>, <span class="math inline">\(S^6 = (\text{B}, \text{C})^T\)</span>, <span class="math inline">\(S^7 = (\text{B}, \text{A})^T\)</span>, <span class="math inline">\(S^8 = (\text{C}, \text{A})^T\)</span>, <span class="math inline">\(S^9 = (\text{C}, \text{B})^T\)</span>.</p></li>
<li><p>De dimensión <span class="math inline">\(n = 3\)</span>: <span class="math inline">\(S^{10} = (\text{A}, \text{B}, \text{C})^T\)</span>, <span class="math inline">\(S^{11} = (\text{B}, \text{A}, \text{C})^T\)</span>, <span class="math inline">\(S^{12} = (\text{A}, \text{C}, \text{B})^T\)</span>, <span class="math inline">\(S^{13} = (\text{C}, \text{B}, \text{A})^T\)</span>, <span class="math inline">\(S^{14} = (\text{B}, \text{C}, \text{A})^T\)</span>, <span class="math inline">\(S^{15} = (\text{C}, \text{A}, \text{B})^T\)</span>,</p></li>
</ol>
<p>Un esquema muestral sería la función de probabilidad:
<span class="math display">\[
\mathbb{P}(\mathcal{S} = S^k) = \begin{cases}
0.1 &amp; \text{ si } k = 1, \\
0.2 &amp; \text{ si } k = 3, \\
0.5 &amp; \text{ si } k = 11, \\
0.2 &amp; \text{ si } k = 15, \\
0   &amp; \text{ en otro caso.}
\end{cases}
\]</span></p>
<p>Otro esquema muestral posible sería:
<span class="math display">\[
\mathbb{P}(\mathcal{S} = S^k) = \begin{cases}
\frac{1}{3} &amp; \text{ si } k = 1, \\
\frac{1}{3} &amp; \text{ si } k = 2, \\
\frac{1}{3} &amp; \text{ si } k = 3, \\
0   &amp; \text{ en otro caso.}
\end{cases}
\]</span>
Este último esquema, intuitivamente, corresponde a la selección aleatoria de un elemento de <span class="math inline">\(U\)</span> con una probabilidad uniforme de que cada elemento salga.</p>
<p>A fin de simplificar el problema (y hasta que se diga lo contrario) agregaremos la hipótesis de <strong>intercambiabilidad</strong>; es decir, consideraremos es irrelevante el orden de las filas de las submatrices de datos. Por ejemplo, bajo intercambiabilidad, <span class="math inline">\(S^4 = (\text{A}, \text{B})^T\)</span> es <em>la misma matriz</em> que <span class="math inline">\(S^7 = (\text{B}, \text{A})^T\)</span>.</p>
<blockquote>
<p>Un ejemplo de muestra donde el orden sí importa (<em>i.e.</em> no son intercambiables) es cuando se realizan exámenes orales según una selección aleatoria de la lista. La tercera persona en presentar el examen estará informada por el <em>¿qué te preguntó el profe?</em>, <em>¿estuvo difícil?</em> que las primeras dos le cuenten.</p>
</blockquote>
<p>Bajo intercambiabilidad, los esquemas muestrales estarán definidos únicamente sobre los siguientes vectores:</p>
<ol style="list-style-type: decimal">
<li><p>De dimensión <span class="math inline">\(n = 1\)</span>: <span class="math inline">\(S^1 = (\text{A})^T\)</span>, <span class="math inline">\(S^2 = (\text{B})^T\)</span>, <span class="math inline">\(S^3 = (\text{C})^T\)</span>.</p></li>
<li><p>De dimensión <span class="math inline">\(n = 2\)</span>: <span class="math inline">\(S^4 = (\text{A}, \text{B})^T\)</span>, <span class="math inline">\(S^5 = (\text{A}, \text{C})^T\)</span>, <span class="math inline">\(S^6 = (\text{B}, \text{C})^T\)</span>.</p></li>
<li><p>De dimensión <span class="math inline">\(n = 3\)</span>: <span class="math inline">\(S^{7} = (\text{A}, \text{B}, \text{C})^T\)</span>.</p></li>
</ol>
<p>En este caso un esquema muestral sería:
<span class="math display">\[
\mathbb{P}(\mathcal{S} = S^k) = \begin{cases}
\frac{1}{16} &amp; \text{ si } k = 1, \\
\frac{3}{16} &amp; \text{ si } k = 2, \\
0 &amp; \text{ si } k = 3, \\
\frac{7}{16} &amp; \text{ si } k = 4, \\
\frac{1}{16} &amp; \text{ si } k = 5, \\
\frac{4}{16} &amp; \text{ si } k = 6, \\
0   &amp; \text{ en otro caso.}
\end{cases}
\]</span>
Dado un elemento <span class="math inline">\(x_i\)</span> del universo, podemos preguntarnos por la probabilidad de que dicho <span class="math inline">\(x_i\)</span> esté en la muestra. Siguiendo el ejemplo anterior:
<span class="math display">\[
\mathbb{P}(\text{A} \in \mathcal{S}) = \mathbb{P}(\mathcal{S} = S^1) + \mathbb{P}(\mathcal{S} = S^4) + \mathbb{P}(\mathcal{S} = S^5) + \mathbb{P}(\mathcal{S} = S^7) = \frac{9}{16}.
\]</span></p>
<p>Como notación, para una población <span class="math inline">\(U = (x_1, x_2, \dots, x_N)^T\)</span> y una muestra aleatoria <span class="math inline">\(\mathcal{S}\)</span> denotamos la probabilidad de que <span class="math inline">\(x_k\)</span> esté en la muestra como:
<span class="math display">\[
\pi_k = \mathbb{P}(x_k \in \mathcal{S})
\]</span>
Estas probabilidades (para <span class="math inline">\(k = 1,2,\dots, N\)</span>) se conocen como <strong>probabilidades de inclusión de primer orden</strong>. La probabilidad conjunta de que <span class="math inline">\(x_k\)</span> y <span class="math inline">\(x_l\)</span> (ambos) estén en la muestra (<strong>probabilidad de inclusión de segundo orden</strong>) está dada por:
<span class="math display">\[
\pi_{k,l} = \mathbb{P}(x_k \in \mathcal{S}, x_l \in \mathcal{S})
\]</span>
Notamos que por definición <span class="math inline">\(\pi_{kk} = \pi_k\)</span>. Análogamente se pueden crear probabilidades de inclusión de cualquier orden deseado.</p>
<p>Finalmente, una población <span class="math inline">\(U = (x_1, x_2, \dots, x_N)^T\)</span> y una muestra aleatoria <span class="math inline">\(\mathcal{S}\)</span> definimos la variable indicadora de que <span class="math inline">\(x_k\)</span> esté en la muestra como:
<span class="math display">\[
\mathbb{I}_{\mathcal{S}}(x_k) = \begin{cases}
1 &amp; \text{ si } x_k \in \mathcal{S} \\
0 &amp; \text{ si } x_k \not\in \mathcal{S} \\
\end{cases}
\]</span>
Notamos que para una muestra aleatoria <span class="math inline">\(\mathcal{S}\)</span> las indicadoras tienen una distribución conocida:
<span class="math display">\[
\mathbb{I}_{\mathcal{S}}(x_k) \sim \text{Bernoulli}(\pi_k)
\]</span>
pues
<span class="math display">\[
\mathbb{P}\big( \mathbb{I}_{\mathcal{S}}(x_k)  = 1\big) = \mathbb{P}\big(x_k \in \mathcal{S}) = \pi_k
\]</span>
Como las <span class="math inline">\(\mathbb{I}_{\mathcal{S}}(x_k)\)</span> son Bernoulli podemos <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">calcular su varianza</a>:
<span class="math display">\[
\text{Var}\Big( \mathbb{I}_{\mathcal{S}}(x_k)\Big) = \pi_k (1 - \pi_k)
\]</span>
Finalmente, recordamos que la covarianza entre dos variables aleatorias <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span> se define como:
<span class="math display">\[
\text{Cov}(X,Y) = \mathbb{E}[XY] - \mathbb{E}[X]\cdot\mathbb{E}[Y]
\]</span>
Por lo que calculamos la covarianza entre dos indicadoras (de <span class="math inline">\(x_k\)</span> y <span class="math inline">\(x_l\)</span>):
<span class="math display">\[\begin{equation}
\begin{aligned}
\text{Cov}\Big(\mathbb{I}_{\mathcal{S}}(x_k), \mathbb{I}_{\mathcal{S}}(x_l) \Big) &amp; = \mathbb{E}\Big[\mathbb{I}_{\mathcal{S}}(x_k) \cdot \mathbb{I}_{\mathcal{S}}(x_l)\Big] -   \mathbb{E}\Big[\mathbb{I}_{\mathcal{S}}(x_k)\Big] \mathbb{E}\Big[\mathbb{I}_{\mathcal{S}}(x_l)\Big]
\\ &amp; = 1 \cdot \mathbb{P}\Big( \mathbb{I}_{\mathcal{S}}(x_k) \cdot \mathbb{I}_{\mathcal{S}}(x_l) = 1) + 0 \cdot \mathbb{P}\Big( \mathbb{I}_{\mathcal{S}}(x_k) \cdot \mathbb{I}_{\mathcal{S}}(x_l) = 0) - \pi_k \pi_l \\
&amp; = \mathbb{P}\Big( \mathbb{I}_{\mathcal{S}}(x_k)  = 1, \mathbb{I}_{\mathcal{S}}(x_l) = 1) - \pi_k \pi_l \\
&amp; = \pi_{k,l} -\pi_k \pi_l
\end{aligned}
\end{equation}\]</span>
La cantidad <span class="math inline">\(\pi_{k,l} -\pi_k \pi_l\)</span> usualmente se denota <span class="math inline">\(\Delta_{k,l}\)</span>:
<span class="math display">\[
\Delta_{k,l} = \pi_{k,l} -\pi_k \pi_l
\]</span></p>
<p>A continuación hablaremos de algunos esquemas de muestreo comunmente utilizados y, finalmente, llegaremos a una generalización de los mismos.</p>
</div>
<div id="ejercicio-5" class="section level3" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Ejercicio</h3>
<p>Demuestra las siguientes propiedades de los <span class="math inline">\(\pi_k\)</span> para un diseño muestral <span class="math inline">\(\mathbb{P}\)</span> con tamaño fijo de la muestra <span class="math inline">\(n\in\mathbb{N}\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\sum\limits_{k=1}^N \pi_k = n\)</span></p></li>
<li><p><span class="math inline">\(\sum\limits_{\substack{k=1 \\\\ k \neq l}}^N \sum\limits_{l=1}^N \pi_{k,l} = n(n-1)\)</span></p></li>
<li><p><span class="math inline">\(\sum\limits_{\substack{l=1 \\\\ l \neq k}}^N \pi_{k,l} = (n-1) \pi_k\)</span></p></li>
</ol>
</div>
</div>
<div id="muestreo-aleatorio-simple-sin-reemplazo-massr" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Muestreo Aleatorio Simple sin Reemplazo (MAS/sR)</h2>
<p>Vamos a considerar una de las formas más sencillas de muestreo: el aleatorio simple <em>sin reemplazo</em> . Para ello seleccionamos de <span class="math inline">\(U = (x_1, x_2, \dots, x_N)^T\)</span> a <span class="math inline">\(n\in\mathbb{N}\)</span> (fijo) observaciones asignándole la probabilidad de ser seleccionada a cada una de <span class="math inline">\(\frac{1}{N}\)</span>. Una vez se selecciona la primera, se selecciona una de las que restan de <span class="math inline">\(U\)</span> con probabilidad <span class="math inline">\(\frac{1}{N-1}\)</span>. El proceso se repite hasta extraer <span class="math inline">\(n\)</span> elementos.</p>
<p>Comencemos por un ejemplo, supongamos tenemos una población de cinco personas:
<span class="math display">\[
U = \Big( \text{Ana}, \text{Beto}, \text{Carlos}, \text{Diana}, \text{Enriqueta}\Big)^T
\]</span>
Si queremos tomar una muestra de <span class="math inline">\(3\)</span> personas sin reemplazo, las muestras posibles son:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\Big( \text{Ana}, \text{Beto}, \text{Carlos}\Big)^T\)</span></p></li>
<li><p><span class="math inline">\(\Big( \text{Ana}, \text{Carlos}, \text{Diana}\Big)^T\)</span></p></li>
<li><p><span class="math inline">\(\Big( \text{Ana}, \text{Beto}, \text{Diana}\Big)^T\)</span></p></li>
<li><p><span class="math inline">\(\Big( \text{Ana}, \text{Beto}, \text{Enriqueta}\Big)^T\)</span></p></li>
<li><p><span class="math inline">\(\Big( \text{Ana}, \text{Carlos}, \text{Enriqueta}\Big)^T\)</span></p></li>
<li><p><span class="math inline">\(\Big( \text{Ana}, \text{Diana}, \text{Enriqueta}\Big)^T\)</span></p></li>
<li><p><span class="math inline">\(\Big( \text{Beto}, \text{Carlos}, \text{Diana}\Big)^T\)</span></p></li>
<li><p><span class="math inline">\(\Big( \text{Beto}, \text{Diana}, \text{Enriqueta}\Big)^T\)</span></p></li>
<li><p><span class="math inline">\(\Big( \text{Beto}, \text{Carlos}, \text{Enriqueta}\Big)^T\)</span></p></li>
<li><p><span class="math inline">\(\Big( \text{Carlos}, \text{Diana}, \text{Enriqueta}\Big)^T\)</span></p></li>
</ol>
<p>Obtener una muestra aleatoria se puede hacer en <code>R</code> con un vector mediante <code>sample</code>:</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="muestreo-aleatorio-simple.html#cb160-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Vector de nombres</span></span>
<span id="cb160-2"><a href="muestreo-aleatorio-simple.html#cb160-2" aria-hidden="true" tabindex="-1"></a>nombres <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Ana&quot;</span>,<span class="st">&quot;Beto&quot;</span>,<span class="st">&quot;Carlos&quot;</span>,<span class="st">&quot;Diana&quot;</span>,<span class="st">&quot;Enriqueta&quot;</span>)</span>
<span id="cb160-3"><a href="muestreo-aleatorio-simple.html#cb160-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb160-4"><a href="muestreo-aleatorio-simple.html#cb160-4" aria-hidden="true" tabindex="-1"></a><span class="co">#Muestra</span></span>
<span id="cb160-5"><a href="muestreo-aleatorio-simple.html#cb160-5" aria-hidden="true" tabindex="-1"></a><span class="fu">sample</span>(nombres, <span class="dv">3</span>, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Beto&quot;  &quot;Ana&quot;   &quot;Diana&quot;</code></pre>
<p>Formalmente, un esquema de muestreo es <strong>aleatorio simple sin reemplazo</strong> si dada una constante <span class="math inline">\(n \in \mathbb{N}\)</span> (con <span class="math inline">\(0 &lt; n \leq N\)</span>) se tiene:</p>
<p><span class="math display">\[
\mathbb{P}\big( \mathcal{S} = S \big) = \begin{cases}
\frac{1}{\binom{N}{n}} &amp; \text{ si } \#S = n \\
0 &amp; \text{ en otro caso.}
\end{cases}
\]</span>
En el caso de muestreo aleatorio simple sin reemplazo podemos calcular las probabilidades de inclusión como siguen:
<span class="math display">\[
\pi_k = \mathbb{P}(x_k \in \mathcal{S}) = \sum\limits_{i=1}^{M_1} \frac{1}{\binom{N}{n}} = \frac{\binom{N-1}{n-1}}{\binom{N}{n}} = \frac{n}{N} = f
\]</span>
donde la tercera igualdad se sigue de que hay <span class="math inline">\(M_1 = \binom{N-1}{n-1}\)</span> muestras que contienen al <span class="math inline">\(x_k\)</span>. (La lógica es, fijo el <span class="math inline">\(x_k\)</span> y entonces me quedan <span class="math inline">\(N-1\)</span> valores de <span class="math inline">\(x\)</span> a acomodar en <span class="math inline">\(n-1\)</span> espacios). Por otro lado:
<span class="math display">\[
\pi_{k,j} = \mathbb{P}(x_k \in \mathcal{S}, x_j \in S) = \sum\limits_{i=1}^{M_2}  \frac{1}{\binom{N}{n}} = \frac{\binom{N-2}{n-2}}{\binom{N}{n}} = \dfrac{n(n-1)}{N(N-1)}
\]</span>
pues hay <span class="math inline">\(M_2 = \binom{N-2}{n-2}\)</span> muestras conteniendo a <span class="math inline">\(x_k\)</span> y <span class="math inline">\(x_j\)</span> a la vez.</p>
<p>Para estimar el total poblacional dado por:
<span class="math display">\[
t = \sum\limits_{i=1}^N x_i
\]</span>
bajo <em>MAS/sR</em> podemos tomar:
<span class="math display">\[
\hat{t} = N \cdot \bar{x}_{\mathcal{S}} = N \frac{1}{n} \sum\limits_{k = 1}^n x_k=  \sum\limits_{k = 1}^n \dfrac{x_k}{n/N} =  
\sum\limits_{k=1}^N \frac{x_k}{\pi_k} \cdot \mathbb{I}_{\mathcal{S}}(x_k)
\]</span>
Notamos entonces que el estimador <span class="math inline">\(\hat{t}\)</span> es una variable aleatoria pues depende de las indicadoras de la muestra. En particular:
<span class="math display">\[
\mathbb{E}\big[ \hat{t} \big] = \mathbb{E}\bigg[\sum\limits_{k=1}^N \frac{x_k}{\pi_k} \cdot \mathbb{I}_{\mathcal{S}}(x_k) \bigg] = \sum\limits_{k=1}^N \frac{x_k}{\pi_k} \underbrace{\mathbb{E}\bigg[\mathbb{I}_{\mathcal{S}}(x_k) \bigg]}_{\pi_k}  = t
\]</span>
de donde se sigue que en promedio el estimador <span class="math inline">\(\hat{t}\)</span> vale el total.</p>
<p><strong>Definición: [Insesgado]</strong> Un estimador <span class="math inline">\(\hat{\theta}\)</span> es un estimador insesgado de <span class="math inline">\(\theta\)</span> si:
<span class="math display">\[
\mathbb{E}\big[ \hat{\theta} - \theta] = 0
\]</span>
En nuestro caso <span class="math inline">\(\hat{t}\)</span> es <em>insesgado</em>. En general, la cantidad <span class="math inline">\(\mathbb{E}\big[ \hat{\theta} - \theta]\)</span> se conoce como <em>el sesgo</em> .</p>
<p>De manera numérica, podemos simular la estimación del total en <code>1000</code> simulaciones como sigue:</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="muestreo-aleatorio-simple.html#cb162-1" aria-hidden="true" tabindex="-1"></a>nsim <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb162-2"><a href="muestreo-aleatorio-simple.html#cb162-2" aria-hidden="true" tabindex="-1"></a>N    <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb162-3"><a href="muestreo-aleatorio-simple.html#cb162-3" aria-hidden="true" tabindex="-1"></a>n    <span class="ot">&lt;-</span> <span class="dv">100</span> </span>
<span id="cb162-4"><a href="muestreo-aleatorio-simple.html#cb162-4" aria-hidden="true" tabindex="-1"></a>base.completa <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">rnorm</span>(N))</span>
<span id="cb162-5"><a href="muestreo-aleatorio-simple.html#cb162-5" aria-hidden="true" tabindex="-1"></a>total         <span class="ot">&lt;-</span> <span class="fu">sum</span>(base.completa<span class="sc">$</span>x)</span>
<span id="cb162-6"><a href="muestreo-aleatorio-simple.html#cb162-6" aria-hidden="true" tabindex="-1"></a>total.muestra <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, nsim)</span>
<span id="cb162-7"><a href="muestreo-aleatorio-simple.html#cb162-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsim){</span>
<span id="cb162-8"><a href="muestreo-aleatorio-simple.html#cb162-8" aria-hidden="true" tabindex="-1"></a>  muestra          <span class="ot">&lt;-</span> <span class="fu">sample</span>(base.completa<span class="sc">$</span>x, n)</span>
<span id="cb162-9"><a href="muestreo-aleatorio-simple.html#cb162-9" aria-hidden="true" tabindex="-1"></a>  total.muestra[i] <span class="ot">&lt;-</span> N<span class="sc">*</span><span class="fu">mean</span>(muestra)</span>
<span id="cb162-10"><a href="muestreo-aleatorio-simple.html#cb162-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb162-11"><a href="muestreo-aleatorio-simple.html#cb162-11" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(total.muestra)</span></code></pre></div>
<pre><code>## [1] -78.75412</code></pre>
<p>Podemos ver las simulaciones como sigue:</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="muestreo-aleatorio-simple.html#cb164-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb164-2"><a href="muestreo-aleatorio-simple.html#cb164-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">x =</span> total.muestra, <span class="at">y =</span> ..density..), <span class="at">fill =</span> <span class="st">&quot;#008B8B&quot;</span>, </span>
<span id="cb164-3"><a href="muestreo-aleatorio-simple.html#cb164-3" aria-hidden="true" tabindex="-1"></a>                 <span class="at">color =</span> <span class="st">&quot;white&quot;</span>, <span class="at">binwidth =</span> <span class="dv">40</span>) <span class="sc">+</span> </span>
<span id="cb164-4"><a href="muestreo-aleatorio-simple.html#cb164-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="fu">aes</span>(<span class="at">xintercept =</span> total), <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="sc">+</span></span>
<span id="cb164-5"><a href="muestreo-aleatorio-simple.html#cb164-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() </span></code></pre></div>
<p><img src="Introduccion_a_Muestreo_files/figure-html/unnamed-chunk-150-1.png" width="672" /></p>
<p>Como podrás notar la <span class="math inline">\(\hat{t}\)</span> es una variable aleatoria y por tanto tiene varianza. De hecho:
<span class="math display">\[
\textrm{Var}(\hat{t}) = \sum\limits_{k = 1}^N \sum\limits_{l = 1}^N \Delta_{k,l} \frac{x_k}{\pi_k} \frac{x_l}{\pi_l}
\]</span>
Para demostrarlo seguimos las igualdades:
<span class="math display">\[\begin{equation}\nonumber
\begin{aligned}
\textrm{Var}(\hat{t})  &amp; = \textrm{Var}\Bigg( \sum\limits_{k=1}^N \frac{x_k}{\pi_k} \cdot \mathbb{I}_{\mathcal{S}}(x_k) \Bigg)
\\ &amp; = \sum\limits_{k=1}^N  \frac{x_k^2}{\pi_k^2} \cdot \textrm{Var}\Big(\mathbb{I}_{\mathcal{S}}(x_k) \Big) + \sum\limits_{k = 1}^N \sum\limits_{\substack{l = 1 \\ \\ l \neq k}}^{N}  \frac{x_k}{\pi_k}  \frac{x_l}{\pi_l} \cdot \textrm{Cov}\Big(\mathbb{I}_{\mathcal{S}}(x_k), \mathbb{I}_{\mathcal{S}}(x_l) \Big)
\\ &amp; = \sum\limits_{k=1}^N  \frac{x_k^2}{\pi_k^2} \cdot \underbrace{\pi_k (1 - \pi_k)}_{\Delta_{k,k}} + \sum\limits_{k = 1}^N \sum\limits_{\substack{l = 1 \\ \\ l \neq k}}^{N} \frac{x_k}{\pi_k}  \frac{x_l}{\pi_l} \cdot \underbrace{\textrm{Cov}\Big(\mathbb{I}_{\mathcal{S}}(x_k), \mathbb{I}_{\mathcal{S}}(x_l) \Big)}_{\Delta_{k,l}}
\\ &amp; = \sum\limits_{k = 1}^N \sum\limits_{\substack{l = 1 \\ \\ l \neq k}}^{N}\Delta_{k,l} \frac{x_k}{\pi_k} \frac{x_l}{\pi_l}
\end{aligned}
\end{equation}\]</span></p>
<p>Numéricamente, en el ejemplo anterior la varianza (simulada) de <span class="math inline">\(\hat{t}\)</span> es:</p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="muestreo-aleatorio-simple.html#cb165-1" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(total.muestra)</span></code></pre></div>
<pre><code>## [1] 8515.982</code></pre>
<p>mientras que la <em>real</em> está dada por (ver ejercicio más adelante):</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="muestreo-aleatorio-simple.html#cb167-1" aria-hidden="true" tabindex="-1"></a>f        <span class="ot">&lt;-</span> n<span class="sc">/</span>N</span>
<span id="cb167-2"><a href="muestreo-aleatorio-simple.html#cb167-2" aria-hidden="true" tabindex="-1"></a>varianza <span class="ot">&lt;-</span> N<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span> <span class="sc">-</span> f)<span class="sc">/</span>n<span class="sc">*</span><span class="fu">var</span>(base.completa<span class="sc">$</span>x)</span>
<span id="cb167-3"><a href="muestreo-aleatorio-simple.html#cb167-3" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(varianza)</span></code></pre></div>
<pre><code>## [1] 8278.693</code></pre>
<p>Nota que tenemos un problema: para estimar <span class="math inline">\(\textrm{Var}(\hat{t})\)</span> necesitamos conocer todas las <span class="math inline">\(x_k\)</span> de la población ¡lo cual es imposible! Entonces necesitamos un estimador de la varianza de <span class="math inline">\(\hat{t}\)</span> para lo cual proponemos:</p>
<p><span class="math display">\[
\widehat{\textrm{Var}}(\hat{t}) = \sum\limits_{k = 1}^n \sum\limits_{l = 1}^n \dfrac{\Delta_{k,l}}{\pi_{k,l}} \frac{x_k}{\pi_k} \frac{x_l}{\pi_l}
\]</span></p>
<p>Para demostrar que el estimador es insesgado tomamos el valor esperado y agregamos las variables indicadoras correspondientes:</p>
<p><span class="math display">\[
\widehat{\textrm{Var}}(\hat{t})  = \sum\limits_{k = 1}^N \sum\limits_{l = 1}^N \dfrac{\Delta_{k,l}}{\pi_{k,l}} \frac{x_k}{\pi_k} \frac{x_l}{\pi_l} \mathbb{I}_{\mathcal{S}}(x_k) \mathbb{I}_{\mathcal{S}}(x_l) 
\]</span>
Se sigue la demostración:</p>
<p><span class="math display">\[\begin{equation}\nonumber
\begin{aligned}
\mathbb{E}\Big[\widehat{\textrm{Var}}(\hat{t})  \Big] &amp; = \mathbb{E}\bigg[ \sum\limits_{k = 1}^N \sum\limits_{l = 1}^N \dfrac{\Delta_{k,l}}{\pi_{k,l}} \frac{x_k}{\pi_k} \frac{x_l}{\pi_l} \mathbb{I}_{\mathcal{S}}(x_k) \mathbb{I}_{\mathcal{S}}(x_l) \bigg] \\
&amp; = \sum\limits_{k = 1}^N \sum\limits_{l = 1}^N \dfrac{\Delta_{k,l}}{\pi_{k,l}} \frac{x_k}{\pi_k} \frac{x_l}{\pi_l} \underbrace{\mathbb{E}\bigg[  \mathbb{I}_{\mathcal{S}}(x_k) \mathbb{I}_{\mathcal{S}}(x_l) \bigg]}_{*} \\
\end{aligned}
\end{equation}\]</span>
donde notamos que:</p>
<p><span class="math display">\[\begin{equation}\nonumber
\begin{aligned}
* = \mathbb{E}\bigg[  \mathbb{I}_{\mathcal{S}}(x_k) \mathbb{I}_{\mathcal{S}}(x_l) \bigg] &amp; = \textrm{Cov}\Big( \mathbb{I}_{\mathcal{S}}(x_k), \mathbb{I}_{\mathcal{S}}(x_l) \Big) + \mathbb{E}\Big[ \mathbb{I}_{\mathcal{S}}(x_k)\Big] \mathbb{E}\Big[ \mathbb{I}_{\mathcal{S}}(x_l)\Big] \\ &amp; = \pi_{k,l} - \pi_k \pi_l + \pi_k\pi_l 
\\ &amp; = \pi_{k,l}
\end{aligned}
\end{equation}\]</span></p>
<p>de donde se sigue:</p>
<p><span class="math display">\[\begin{equation}\nonumber
\begin{aligned}
\mathbb{E}\Big[\widehat{\textrm{Var}}(\hat{t})  \Big] &amp; = \sum\limits_{k = 1}^N \sum\limits_{l = 1}^N \dfrac{\Delta_{k,l}}{\pi_{k,l}} \frac{x_k}{\pi_k} \frac{x_l}{\pi_l}\underbrace{ \pi_{k,l}}_{*} 
\\ &amp; = \sum\limits_{k = 1}^N \sum\limits_{l = 1}^N \Delta_{k,l} \frac{x_k}{\pi_k} \frac{x_l}{\pi_l} = \textrm{Var}(\hat{t})
\end{aligned}
\end{equation}\]</span></p>
<p>Podemos calcular la varianza estimada para una muestra aleatoria simple sin reemplazo como sigue (ver ejercicio):</p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="muestreo-aleatorio-simple.html#cb169-1" aria-hidden="true" tabindex="-1"></a>f        <span class="ot">&lt;-</span> n<span class="sc">/</span>N</span>
<span id="cb169-2"><a href="muestreo-aleatorio-simple.html#cb169-2" aria-hidden="true" tabindex="-1"></a>varianza <span class="ot">&lt;-</span> N<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span> <span class="sc">-</span> f)<span class="sc">/</span>n<span class="sc">*</span><span class="fu">var</span>(muestra)</span>
<span id="cb169-3"><a href="muestreo-aleatorio-simple.html#cb169-3" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(varianza)</span></code></pre></div>
<pre><code>## [1] 7811.204</code></pre>
<p><em>Observaciones</em></p>
<ol style="list-style-type: decimal">
<li><p>La media muestral <span class="math inline">\(\bar{x}_{\mathcal{S}} = \frac{1}{n}\sum\limits_{i = 1}^{n} x_i\)</span> es un estimador insesgado de la media poblacional <span class="math inline">\(\bar{x}_{\mathcal{U}} = \frac{1}{N}\sum\limits_{i = 1}^{N} x_i\)</span>. Se sigue de una factorización de <span class="math inline">\(n\)</span> del total (<span class="math inline">\(t\)</span> y <span class="math inline">\(\hat{t}\)</span> respectivamente).</p></li>
<li><p>Se puede obtener <span class="math inline">\(\textrm{Var}(\bar{x}_{\mathcal{S}})\)</span> y <span class="math inline">\(\widehat{\textrm{Var}}(\bar{x}_{\mathcal{S}})\)</span> factorizando las <span class="math inline">\(n\)</span> de manera cuadrática del <span class="math inline">\(\hat{t}\)</span>.</p></li>
</ol>
<div id="ejercicio-6" class="section level3" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> Ejercicio</h3>
<p>Definimos:
<span class="math display">\[
s_{x,\mathcal{U}}^2 = \dfrac{1}{N-1} \sum\limits_{k = 1}^N \big( x_k - \bar{x}_{\mathcal{U}})^2
\]</span>
como la <strong>varianza poblacional ajustada</strong> y
<span class="math display">\[
s_{x,\mathcal{S}}^2 = \dfrac{1}{n-1} \sum\limits_{k = 1}^n \big( x_k - \bar{x}_{\mathcal{S}})^2
\]</span>
como la <strong>varianza muestral ajustada</strong>. Sea <span class="math inline">\(f = \frac{n}{N}\)</span> la <strong>fracción muestral</strong>. Demuestra que en el caso de muestreo aleatorio simple sin reemplazo:
<span class="math display">\[
\textrm{Var}(\hat{t}) = N^2\dfrac{1-f}{n} s^2_{x,\mathcal{U}}
\]</span>
mientras que el estimador insesgado se transforma en:
<span class="math display">\[
\widehat{\textrm{Var}}(\hat{t}) = N^2\dfrac{1-f}{n} s^2_{x,\mathcal{S}}
\]</span></p>
</div>
</div>
<div id="teorema-del-límite-central-aplicación" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Teorema del Límite Central (Aplicación)</h2>
<!--En esta sección haremos una pequeña disgresión hacia temas de Probabilidad II. Como ésta no es requisito para la materia, veremos algunas demostraciones que se ven en dicho curso y un resultado que se obtiene en el mismo. Éste nos permitirá hablar de intervalos de confianza: intervalos aleatorios donde con cierta probabilidad aparece el valor deseado. Existen varios **teoremas de límite central** los cuales establecen comportamientos para sumas de variables aleatorias. Si bien, los resultados que aquí veamos se pueden generalizar a resultados no independientes a [variables intercambiables](https://projecteuclid.org/euclid.aop/1176992260), o [a muestro de poblaciones finitas](https://amstat.tandfonline.com/doi/abs/10.1198/000313001753272330#.XvrQlSFR3OQ) (no necesariamente independientes) plantearemos los teoremas y sus pruebas para variables aleatorias independientes con media y varianza finita (a fin de simplificar las demostraciones). Las demostraciones de los teoremas que en realidad aplican para nuestros casos pueden obtenerse de [Rosen](https://projecteuclid.org/download/pdf_1/euclid.afm/1485893466), -->
<p>En esta sección hablaremos del teorema central del límite correspondiente a muestreo aleatorio simple con poblaciones finitas. Éste no es el mismo que el de Proba 2 (en términos de hipótesis) aunque las conclusiones sean las mismas. El teorema de Proba 2 establece que si se tiene una colección <span class="math inline">\(\{ X_i \}\)</span> de variables aleatorias independientes idénticamente distribuidas (todas con distribución acumulada <span class="math inline">\(F_X\)</span>) con media <span class="math inline">\(\mu\)</span> y varianza <span class="math inline">\(\sigma^2 &lt; \infty\)</span>, entonces, si definimos <span class="math inline">\(Z\)</span> como:</p>
<p><span class="math display">\[
Z =\lim_{n \to \infty} \sqrt{\dfrac{n}{\sigma^2}} \cdot  \Big( \frac{1}{n}\sum_{i = 1}^n X_i - \mu\Big)
\]</span>
se tiene que <span class="math inline">\(Z \sim \textrm{Normal}(0,1)\)</span>.</p>
<p>En este teorema central podemos observar que hay algo muy parecido a la media muestral embebido en el teorema (la <span class="math inline">\(\frac{1}{n}\sum_{i = 1}^n X_i\)</span>) <em>pero</em> no es exactamente la media muestral (aquí se supone que todas las <span class="math inline">\(X_i\)</span> son independientes con distribución <span class="math inline">\(F_X\)</span> y en el caso de muestreo aleatorio sin reemplazo se sabe que las indicadoras <strong>NO</strong> son independientes y que de hecho tampoco son idénticamente distribuidas cuando analizamos <span class="math inline">\(\sum_{i = 1}^{n} x_i \mathbb{I}_{\mathcal{S}}(x_i)\)</span>). Entonces <em>técnicamente</em> no podemos aplicar el teorema central del límite así como está a nuestra muestra. Sin embargo, Hàjek (y más tarde <a href="https://projecteuclid.org/download/pdf_1/euclid.afm/1485893466">Rosen</a> ) encontraron condiciones <em>sin tener que pedir independencia ni distribución idéntica</em> que permiten sustituir las <span class="math inline">\(X_i\)</span> por las de la media muestral (<span class="math inline">\(x_i \mathbb{I}_{\mathcal{S}}(x_i)\)</span>) y que, cuando <span class="math inline">\(N\)</span> y <span class="math inline">\(n\)</span> tienden a infinito “de buena manera,” se tiene algo similar a esta expresión (<strong>OJO</strong> no es una expresión <em>correcta</em> pero es la idea):</p>
<p><span class="math display">\[
Z =\lim_{N, n \to \infty} \sqrt{\frac{1}{\textrm{Var}(\bar{x}_{\mathcal{S}})}} \cdot  \Big( \frac{1}{n}\sum_{i = 1}^N x_i \mathbb{I}_{\mathcal{S}}(x_i) - \bar{x}_{\mathcal{U}}\Big)
\]</span>
donde <span class="math inline">\(\mu = \sum_{k = 1}^N x_k\)</span> es la media poblacional y <span class="math inline">\(\sigma^2 = \frac{1}{N} \sum_{k = 1}^N (x_k - \mu)^2\)</span> la varianza poblacional no ajustada. La demostración propia de este teorema la posponemos para una sección posterior. Por ahora, ejemplificaremos el teorema del límite central en <code>R</code>, utilizaremos la expresión anterior para deducir y explicar el concepto de intervalo de confianza y, finalmente, haremos un ejemplo de estimación de intervalo.</p>
<div id="estimación-de-intervalos-de-confianza-para-el-total" class="section level3" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> Estimación de intervalos de confianza para el total</h3>
<p>Un intervalo de confianza de <span class="math inline">\((1 - \alpha)\times 100 \%\)</span> de un estimador poblacional desconocido <span class="math inline">\(\theta = \theta(x_1, x_2, \dots, x_N)\)</span> (constante) es un intervalo aleatorio de la forma <span class="math inline">\(\big[ L(\mathcal{S}), U(\mathcal{S}) \big]\)</span> (donde <span class="math inline">\(L, U\)</span> son variables aleatorias que dependen de la muestra) tal que
<span class="math display">\[
\mathbb{P}\Big( \theta \in \big[ L(\mathcal{S}), U(\mathcal{S}) \big]\Big) = 1 - \alpha
\]</span>
Notamos que lo aleatorio del intervalo son las cotas del mismo y que, dadas distintas muestras <span class="math inline">\(\mathcal{S}\)</span> el valor de interés <span class="math inline">\(\theta\)</span> no siempre va a caer ahí. La idea de un intervalo es poder dar una cota de más o menos dónde anda un valor. Veamos un ejemplo con el total.</p>
<p>Recordamos que el estimador del total es insesgado <span class="math inline">\(\mathbb{E}\big[ \hat{t} \big] = t\)</span> y que por definición:
<span class="math display">\[
\hat{t} = N \frac{1}{n}\sum\limits_{i = 1}^N x_i \cdot \mathbb{I}_{\mathcal{S}}(x_i) 
\]</span></p>
<p>luego usando la versión de muestreo finito del teorema central del límite (factorizando <span class="math inline">\(N\)</span>) tenemos que:</p>
<p><span class="math display">\[
\sqrt{\frac{1}{\textrm{Var}(\bar{x}_{\mathcal{S}})}} \cdot  \Big( \frac{1}{n}\sum_{i = 1}^N x_i \mathbb{I}_{\mathcal{S}}(x_i) - \bar{x}_{\mathcal{U}}\Big) = 
 \cdot  N\dfrac{\Big( \frac{1}{n}\sum_{i = 1}^N x_i \mathbb{I}_{\mathcal{S}}(x_i) - \bar{x}_{\mathcal{U}}\Big)}{N\sqrt{\textrm{Var}(\bar{x}_{\mathcal{S}})}}
=  \dfrac{\hat{t} - t}{\sqrt{\textrm{Var}(\hat{t})}}  \mathrel{\dot\sim} \textrm{Normal}(0,1)
\]</span>
De donde se sigue que si se desea tener un intervalo de tamali <span class="math inline">\((1 - \alpha) \times 100 \%\)</span> lo que hay que hacer es buscar <span class="math inline">\(L(\mathcal{S})\)</span> y <span class="math inline">\(U(\mathcal{S})\)</span> tales que:</p>
<p><span class="math display">\[
\mathbb{P}\Bigg( L(\mathcal{S}) \leq \dfrac{\hat{t} - t}{\sqrt{\textrm{Var}(\hat{t})}} \leq U(\mathcal{S}) \Bigg) = 1 - \alpha
\]</span>
En este caso las probabilidades (por aproximación asintótica) se modelan bajo la hipótesis de normalidad. Y tomamos ventaja de que la normal es simétrica respecto a la media para proponer que <span class="math inline">\(L(\mathcal{S}) = -U(\mathcal{S})\)</span> y ambas correspondan a <span class="math inline">\(\pm \Phi^{-1}(\alpha/2)\)</span> (la función de distirbución acumulada inversa de la normal). Es decir, ambos deben corresponder a los cuantiles con probabilidad <span class="math inline">\(\alpha/2\)</span> y <span class="math inline">\(1 - \alpha/2\)</span>, denotados <span class="math inline">\(z_{\alpha/2}\)</span> y <span class="math inline">\(z_{1 - \alpha/2}\)</span>. Por simetría de la normal tenemos que: <span class="math inline">\(z_{\alpha/2} = - z_{1 - \alpha/2}\)</span> y por tanto:
<span class="math display">\[
\mathbb{P}\Bigg( z_{\alpha/2} \leq \dfrac{\hat{t} - t}{\sqrt{\textrm{Var}(\hat{t})}} \leq  z_{1 -\alpha/2} \Bigg) = 1 - \alpha
\]</span>
de donde despejamos:
<span class="math display">\[
\mathbb{P}\Bigg( z_{\alpha/2}\sqrt{\textrm{Var}(\hat{t})} \leq \hat{t} - t \leq z_{1- \alpha/2}\sqrt{\textrm{Var}(\hat{t})} \Bigg) = \mathbb{P}\Bigg( \hat{t} - z_{1-\alpha/2}\sqrt{\textrm{Var}(\hat{t})} \leq t \leq \hat{t} +  z_{ \alpha/2}\sqrt{\textrm{Var}(\hat{t})} \Bigg)  =  1 - \alpha
\]</span></p>
<p>Notamos que como no conocemos <span class="math inline">\(\textrm{Var}(\hat{t})\)</span> la podemos aproximar mediante <span class="math inline">\(\widehat{\textrm{Var}}(\hat{t})\)</span> (hay mejores aproximaciones mediante una <span class="math inline">\(t\)</span> de Student asintótica pero no lo usaremos ahora) y tener intervalos aproximados de la forma:
<span class="math display">\[\begin{equation}
\begin{aligned}
L(\mathcal{S}) &amp; =  \hat{t} - z_{1-\alpha/2}\sqrt{\widehat{\textrm{Var}}(\hat{t})} \\
U(\mathcal{S}) &amp; =  \hat{t} + z_{1-\alpha/2}\sqrt{\widehat{\textrm{Var}}(\hat{t})}
\end{aligned}
\end{equation}\]</span>
de manera concisa muchas veces los escribimos como:
<span class="math display">\[
 \hat{t} \pm z_{1-\alpha/2}\sqrt{\widehat{\textrm{Var}}(\hat{t})}
\]</span></p>
</div>
<div id="ejemplo-con-simulación" class="section level3" number="4.5.2">
<h3><span class="header-section-number">4.5.2</span> Ejemplo con simulación:</h3>
<p>Veamos cómo se ven múltiples intervalos simulados con confianza del <span class="math inline">\(90\%\)</span> y suponiendo la varianza es conocida</p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="muestreo-aleatorio-simple.html#cb171-1" aria-hidden="true" tabindex="-1"></a>nsim <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb171-2"><a href="muestreo-aleatorio-simple.html#cb171-2" aria-hidden="true" tabindex="-1"></a>n    <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb171-3"><a href="muestreo-aleatorio-simple.html#cb171-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-4"><a href="muestreo-aleatorio-simple.html#cb171-4" aria-hidden="true" tabindex="-1"></a>total.muestra  <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, nsim)</span>
<span id="cb171-5"><a href="muestreo-aleatorio-simple.html#cb171-5" aria-hidden="true" tabindex="-1"></a>confianza.bajo <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, nsim)</span>
<span id="cb171-6"><a href="muestreo-aleatorio-simple.html#cb171-6" aria-hidden="true" tabindex="-1"></a>confianza.alto <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, nsim)</span>
<span id="cb171-7"><a href="muestreo-aleatorio-simple.html#cb171-7" aria-hidden="true" tabindex="-1"></a>f <span class="ot">&lt;-</span> n<span class="sc">/</span>N</span>
<span id="cb171-8"><a href="muestreo-aleatorio-simple.html#cb171-8" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="fl">0.1</span><span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb171-9"><a href="muestreo-aleatorio-simple.html#cb171-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-10"><a href="muestreo-aleatorio-simple.html#cb171-10" aria-hidden="true" tabindex="-1"></a>var.total      <span class="ot">&lt;-</span> N<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span> <span class="sc">-</span> f)<span class="sc">/</span>n<span class="sc">*</span><span class="fu">var</span>(base.completa<span class="sc">$</span>x)</span>
<span id="cb171-11"><a href="muestreo-aleatorio-simple.html#cb171-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-12"><a href="muestreo-aleatorio-simple.html#cb171-12" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb171-13"><a href="muestreo-aleatorio-simple.html#cb171-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nsim){</span>
<span id="cb171-14"><a href="muestreo-aleatorio-simple.html#cb171-14" aria-hidden="true" tabindex="-1"></a>  muestra           <span class="ot">&lt;-</span> <span class="fu">sample</span>(base.completa<span class="sc">$</span>x, n, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb171-15"><a href="muestreo-aleatorio-simple.html#cb171-15" aria-hidden="true" tabindex="-1"></a>  total.muestra[i]  <span class="ot">&lt;-</span> N<span class="sc">*</span><span class="fu">mean</span>(muestra)</span>
<span id="cb171-16"><a href="muestreo-aleatorio-simple.html#cb171-16" aria-hidden="true" tabindex="-1"></a>  <span class="co">#var.total[i]    &lt;- N^2*(1 - f)/n*var(muestra)</span></span>
<span id="cb171-17"><a href="muestreo-aleatorio-simple.html#cb171-17" aria-hidden="true" tabindex="-1"></a>  confianza.bajo[i] <span class="ot">&lt;-</span> total.muestra[i] <span class="sc">-</span> z<span class="sc">*</span><span class="fu">sqrt</span>(var.total)</span>
<span id="cb171-18"><a href="muestreo-aleatorio-simple.html#cb171-18" aria-hidden="true" tabindex="-1"></a>  confianza.alto[i] <span class="ot">&lt;-</span> total.muestra[i] <span class="sc">+</span> z<span class="sc">*</span><span class="fu">sqrt</span>(var.total)</span>
<span id="cb171-19"><a href="muestreo-aleatorio-simple.html#cb171-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb171-20"><a href="muestreo-aleatorio-simple.html#cb171-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-21"><a href="muestreo-aleatorio-simple.html#cb171-21" aria-hidden="true" tabindex="-1"></a>intervalos.simulados <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb171-22"><a href="muestreo-aleatorio-simple.html#cb171-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">Simulacion =</span> <span class="dv">1</span><span class="sc">:</span>nsim,</span>
<span id="cb171-23"><a href="muestreo-aleatorio-simple.html#cb171-23" aria-hidden="true" tabindex="-1"></a>  <span class="at">Intervalo.Bajo =</span> confianza.bajo,</span>
<span id="cb171-24"><a href="muestreo-aleatorio-simple.html#cb171-24" aria-hidden="true" tabindex="-1"></a>  <span class="at">Total.Estimado =</span> total.muestra,</span>
<span id="cb171-25"><a href="muestreo-aleatorio-simple.html#cb171-25" aria-hidden="true" tabindex="-1"></a>  <span class="at">Intervalo.Alto =</span> confianza.alto</span>
<span id="cb171-26"><a href="muestreo-aleatorio-simple.html#cb171-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb171-27"><a href="muestreo-aleatorio-simple.html#cb171-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-28"><a href="muestreo-aleatorio-simple.html#cb171-28" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(intervalos.simulados) <span class="sc">+</span></span>
<span id="cb171-29"><a href="muestreo-aleatorio-simple.html#cb171-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> Simulacion, <span class="at">y =</span> Total.Estimado)) <span class="sc">+</span></span>
<span id="cb171-30"><a href="muestreo-aleatorio-simple.html#cb171-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_errorbar</span>(<span class="fu">aes</span>(<span class="at">x =</span> Simulacion, <span class="at">ymin =</span> Intervalo.Bajo, </span>
<span id="cb171-31"><a href="muestreo-aleatorio-simple.html#cb171-31" aria-hidden="true" tabindex="-1"></a>                    <span class="at">ymax =</span> Intervalo.Alto)) <span class="sc">+</span></span>
<span id="cb171-32"><a href="muestreo-aleatorio-simple.html#cb171-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="fu">aes</span>(<span class="at">yintercept =</span> <span class="fu">sum</span>(base.completa<span class="sc">$</span>x)), </span>
<span id="cb171-33"><a href="muestreo-aleatorio-simple.html#cb171-33" aria-hidden="true" tabindex="-1"></a>             <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>,</span>
<span id="cb171-34"><a href="muestreo-aleatorio-simple.html#cb171-34" aria-hidden="true" tabindex="-1"></a>             <span class="at">size =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>) <span class="sc">+</span> </span>
<span id="cb171-35"><a href="muestreo-aleatorio-simple.html#cb171-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb171-36"><a href="muestreo-aleatorio-simple.html#cb171-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Simulación de intervalos de confianza&quot;</span>)</span></code></pre></div>
<p><img src="Introduccion_a_Muestreo_files/figure-html/unnamed-chunk-154-1.png" width="672" /></p>
<p>Nota que estos intervalos son aproximados y no siempre van a funcionar. (¿Puedes hallar un ejemplo donde no sirvan a pesar de que <span class="math inline">\(n\)</span> y <span class="math inline">\(N\)</span> sean grandes?) Luego veremos correcciones a esto; por ahora, supondremos que la aproximación es buena.</p>
</div>
</div>
<div id="ejemplo-resumen-estimación-de-una-proporción-bajo-muestreo-aleatorio-simple-sin-reemplazo" class="section level2" number="4.6">
<h2><span class="header-section-number">4.6</span> Ejemplo Resumen: Estimación de una proporción bajo muestreo aleatorio simple sin reemplazo</h2>
<p>Se realiza una encuesta mediante muestreo aleatorio simple sin reemplazo a la población del ITAM <span class="math inline">\(N = 5000\)</span> donde interesa conocer la proporción de gente que apoya al gobierno en turno <span class="math inline">\(p\)</span>. Implícitamente, se supone que alguien apoya (proporción <span class="math inline">\(p\)</span> de toda la población) o no lo apoya (proporción <span class="math inline">\(1-p\)</span>), que dichos conjuntos son disjuntos y que no hay una tercera opción (como <code>NO RESPONDE / DESCONOCE QUIÉN GOBIERNA</code>). La pregunta es: ¿a cuántas personas hay que encuestar si interesa estimar <span class="math inline">\(p\)</span> con un error máximo de tamaño <span class="math inline">\(\epsilon = 0.05\)</span> al <span class="math inline">\(99\%\)</span> de confianza (es decir, que el estimador <span class="math inline">\(\hat{p}\)</span> de la proporción esté, a lo más, a <span class="math inline">\(\pm 0.05\)</span> de distancia del valor verdadero <span class="math inline">\(p\)</span> con un intervalo de confianza al <span class="math inline">\(99\%\)</span>)?</p>
<p>Supongamos tomamos una muestra de tamaño <span class="math inline">\(n\)</span> dada por <span class="math inline">\(\mathcal{S} = (x_1, x_2, \dots, x_n)^T\)</span> de una población <span class="math inline">\(\mathcal{U} = (x_1, x_2, \dots, x_N)^T\)</span> de tamaño <span class="math inline">\(N\)</span>. Pensemos, además, existen <span class="math inline">\(N_1\)</span> personas que aprueban al gobierno actual y <span class="math inline">\(N- N_1\)</span> que desaprueban del mismo y por tanto la proporción que nos interesa estimar es:
<span class="math display">\[
p = \dfrac{N_1}{N}
\]</span>
Por otro lado, la proporción muestral de personas que aprueban está dada por:
<span class="math display">\[
\hat{p} = \dfrac{\sum_{i = 1}^n \mathbb{I}_{\text{Aprueba}}(x_i)}{n}
\]</span>
donde si definimos <span class="math inline">\(H = \dfrac{\sum_{i = 1}^n \mathbb{I}_{\text{Aprueba}}(x_i)}{n}\)</span> notamos que la distribución de <span class="math inline">\(H\)</span> está dada por una variable <a href="https://en.wikipedia.org/wiki/Hypergeometric_distribution">Hipergeométrica</a> (pues de una población de <span class="math inline">\(N\)</span> se seleccionan <span class="math inline">\(n\)</span> donde <span class="math inline">\(N_1\)</span> cumplen la categoría deseada). Su media y varianza están dadas respectivamente por:
<span class="math display">\[
\mathbb{E}\big[ H \big] = n \dfrac{N_1}{N} = np
\]</span>
así como por:
<span class="math display">\[
\textrm{Var}\big[ H\big] = n \dfrac{N_1}{N} \Big( 1 - \dfrac{N_1}{N}\Big) \Big( \dfrac{N-n}{N-1}\Big) = np (1-p)\Big( \dfrac{N-n}{N-1}\Big)
\]</span>
Se sigue entonces que <span class="math inline">\(\mathbb{E}\big[\hat{p}\big] = p\)</span> y por tanto <span class="math inline">\(\hat{p}\)</span> es un estimador insesgado. La varianza por otro lado es:
<span class="math display">\[
\textrm{Var}\big( \hat{p} \big) = \dfrac{p(1-p)}{n}\Big( \dfrac{N-n}{N-1}\Big)
\]</span>
Finalmente, el estimador de la varianza es:
<span class="math display">\[
\widehat{\textrm{Var}}\big( \hat{p} \big) = \dfrac{\hat{p}(1-\hat{p})}{n}\Big( \dfrac{N-n}{N-1}\Big)
\]</span>
el cual también cumple que es insesgado (demuéstralo).</p>
<p>Podemos aplicar el Teorema Central del Límite para la proporción<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> notando que la definición de <span class="math inline">\(\hat{p}\)</span> coincide con una media (de las indicadoras):
<span class="math display">\[
\underbrace{\dfrac{\hat{p} - p}{\sqrt{\frac{\hat{p}(1-\hat{p})}{n} \Big( \frac{N-n}{N-1} \Big)}}}_{\widehat{\text{Var}}(\hat{p})}\mathrel{\dot\sim} \textrm{Normal}(0,1)
\]</span>
De donde se tiene que:
<span class="math display">\[\begin{equation}
\begin{aligned}
&amp; \mathbb{P}\Bigg(- z_{\alpha/2} \leq \dfrac{\hat{p} - p}{\sqrt{\frac{\hat{p}(1-\hat{p})}{n} \Big( \frac{N-n}{N-1} \Big)}}\leq z_{\alpha/2}\Bigg) \approx 1 - \alpha \\
\Rightarrow &amp; \mathbb{P}\Bigg( \hat{p} - z_{\alpha/2} \sqrt{\frac{\hat{p}(1-\hat{p})}{n} \Big( \frac{N-n}{N-1} \Big)} \leq  p \leq \hat{p} +  z_{\alpha/2} \sqrt{\frac{\hat{p}(1-\hat{p})}{n} \Big( \frac{N-n}{N-1} \Big)}\Bigg) \approx 1 - \alpha 
\end{aligned}
\end{equation}\]</span></p>
<blockquote>
<p><strong>Nota</strong> <a href="https://opentextbc.ca/introbusinessstatopenstax/chapter/a-confidence-interval-for-a-population-proportion/">Es común encontrar en Internet</a> que para los intervalos de confianza la gente supone una población muy grande <span class="math inline">\(N\)</span> respecto a la muestra <span class="math inline">\(n\)</span> y entonces eliminan el término <span class="math inline">\(\frac{N-n}{N-1}\)</span> argumentando que <span class="math inline">\(\frac{N-n}{N-1} \approx 1\)</span> y obtienen la siguiente fórmula:
<span class="math display">\[
\hat{p} \pm z_{\alpha/2} \sqrt{\frac{\hat{p}(1-\hat{p})}{n}} 
\]</span>
esto simplifica algunos cálculos (a mano) pero nosotros tenemos <code>R</code> y podemos hacer cálculos más exactos sin tener que suponer semejantes atrocidades.</p>
</blockquote>
<p>Como el error deseado es de tamaño <span class="math inline">\(\epsilon\)</span> queremos <span class="math inline">\(|p - \hat{p} | \leq \epsilon\)</span> esto se traduce en:
<span class="math display">\[\begin{equation}\nonumber
|p -  \hat{p}| \leq \underbrace{z_{\alpha/2} \sqrt{\frac{\hat{p}(1-\hat{p})}{n} \Big( \frac{N-n}{N-1} \Big) }}_{\epsilon}
\end{equation}\]</span>
de donde igualamos para despejar la <span class="math inline">\(n\)</span>:
<span class="math display">\[\begin{equation}\nonumber
\begin{aligned}
 \epsilon &amp; = z_{\alpha/2} \sqrt{\frac{\hat{p}(1-\hat{p})}{n}\Big( \frac{N-n}{N-1} \Big)} \\
 &amp; = \dfrac{\epsilon^2 }{z_{\alpha/2}^2} = \frac{\hat{p}(1-\hat{p})}{n}\Big( \frac{N-n}{N-1} \Big) \\
 &amp; = \frac{N-1}{\hat{p}(1-\hat{p})}\dfrac{\epsilon^2 }{z_{\alpha/2}^2} =  \frac{N-n}{n} = \frac{N}{n} - 1 \\
 &amp; = \frac{N-1}{\hat{p}(1-\hat{p})}\dfrac{\epsilon^2 }{z_{\alpha/2}^2} + 1 =  \frac{N}{n} \\
 &amp; \Rightarrow n = \dfrac{N}{\frac{N-1}{\hat{p}(1-\hat{p})}\frac{\epsilon^2 }{z_{\alpha/2}^2} + 1} = \dfrac{\frac{z^2_{\alpha/2}}{\epsilon^2}\hat{p}(1-\hat{p})}{\frac{N-1}{N} + \frac{1}{N}\frac{z^2_{\alpha/2}}{\epsilon^2}\hat{p}(1-\hat{p})} = \dfrac{m}{1 + \frac{m-1}{N}}
\end{aligned}
\end{equation}\]</span>
donde
<span class="math display">\[
m = \frac{z^2_{\alpha/2}}{\epsilon^2}\hat{p}(1-\hat{p})
\]</span>
Ahora el problema es que el tamaño de muestra <span class="math inline">\(n\)</span> depende de la muestra a través de <span class="math inline">\(\hat{p}\)</span> ¡y no hemos tomado la muestra! Para ello entonces analizamos el peor caso que puede ocurrir de <span class="math inline">\(\hat{p}\)</span> de tal forma que obtengamos la <span class="math inline">\(n\)</span> que puede salir con la peor proporción <span class="math inline">\(\hat{p}\)</span> posible. Para ello maximizamos con derivadas:
<span class="math display">\[\begin{equation}\nonumber
    \begin{aligned}
    \dfrac{\partial n}{\partial \hat{p}} &amp; = \dfrac{\partial}{\partial \hat{p}} \Bigg(  \dfrac{N}{\frac{N-1}{\hat{p}(1-\hat{p})}\frac{\epsilon^2 }{z_{\alpha/2}^2} + 1} \Bigg) 
    \\ &amp; = N  \Bigg(  \dfrac{1}{\frac{N-1}{\hat{p}(1-\hat{p})}\frac{\epsilon^2 }{z_{\alpha/2}^2} + 1} \Bigg)^2 \cdot \dfrac{\partial}{\partial \hat{p}} \Bigg( \frac{N-1}{\hat{p}(1-\hat{p})}\frac{\epsilon^2 }{z_{\alpha/2}^2} + 1\Bigg)  
    \\ &amp; = \underbrace{N (N-1)\frac{\epsilon^2 }{z_{\alpha/2}^2}}_{C}  \Bigg(  \dfrac{1}{\frac{N-1}{\hat{p}(1-\hat{p})}\frac{\epsilon^2 }{z_{\alpha/2}^2} + 1} \Bigg)^2 \cdot \dfrac{\partial}{\partial \hat{p}} \Bigg( \frac{1}{\hat{p}(1-\hat{p})}\Bigg)  
    \\ &amp; = C \Bigg(  \dfrac{1}{\frac{N-1}{\hat{p}(1-\hat{p})}\frac{\epsilon^2 }{z_{\alpha/2}^2} + 1} \Bigg)^2  \Bigg( \frac{1}{\hat{p}(1-\hat{p})}\Bigg)^2 \dfrac{\partial}{\partial \hat{p}} \hat{p}(1-\hat{p}) 
    \\ &amp; = C \Bigg(  \dfrac{1}{\frac{N-1}{\hat{p}(1-\hat{p})}\frac{\epsilon^2 }{z_{\alpha/2}^2} + 1} \Bigg)^2  \Bigg( \frac{1}{\hat{p}(1-\hat{p})}\Bigg)^2  (1-2\hat{p}) = 0
    \end{aligned}
\end{equation}\]</span>
de donde se sigue que <span class="math inline">\(\hat{p} = \frac{1}{2}\)</span> es un punto crítico. De hecho puede verificarse que es el máximo (por ejemplo a través de la segunda derivada). Luego, podemos estimar la <span class="math inline">\(n\)</span> de la muestra mediante:
<span class="math display">\[
n = \left\lceil \dfrac{m}{1 + \frac{m-1}{N}} \right\rceil
\]</span>
donde <span class="math inline">\(m = \dfrac{1}{4}\frac{z^2_{\alpha/2}}{\epsilon^2}\)</span>. En el caso particular de este ejercicio, <span class="math inline">\(N = 5000\)</span>, <span class="math inline">\(\epsilon = 0.05\)</span>, <span class="math inline">\(\alpha = 0.01\)</span> y <span class="math inline">\(z^2_{\alpha/2} \approx\)</span> <code>qnorm(0.9)</code>. Luego podemos calcular:</p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="muestreo-aleatorio-simple.html#cb172-1" aria-hidden="true" tabindex="-1"></a>alpha   <span class="ot">&lt;-</span> <span class="fl">0.01</span></span>
<span id="cb172-2"><a href="muestreo-aleatorio-simple.html#cb172-2" aria-hidden="true" tabindex="-1"></a>z       <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(<span class="dv">1</span> <span class="sc">-</span> alpha<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb172-3"><a href="muestreo-aleatorio-simple.html#cb172-3" aria-hidden="true" tabindex="-1"></a>epsilon <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb172-4"><a href="muestreo-aleatorio-simple.html#cb172-4" aria-hidden="true" tabindex="-1"></a>m       <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">/</span><span class="dv">4</span>)<span class="sc">*</span>(z<span class="sc">/</span>epsilon)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb172-5"><a href="muestreo-aleatorio-simple.html#cb172-5" aria-hidden="true" tabindex="-1"></a>N       <span class="ot">&lt;-</span> <span class="dv">5000</span></span>
<span id="cb172-6"><a href="muestreo-aleatorio-simple.html#cb172-6" aria-hidden="true" tabindex="-1"></a>n       <span class="ot">&lt;-</span> <span class="fu">ceiling</span>(m<span class="sc">/</span>(<span class="dv">1</span> <span class="sc">+</span> (m<span class="dv">-1</span>)<span class="sc">/</span>N))</span>
<span id="cb172-7"><a href="muestreo-aleatorio-simple.html#cb172-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb172-8"><a href="muestreo-aleatorio-simple.html#cb172-8" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;El tamaño de muestra es &quot;</span>, n))</span></code></pre></div>
<pre><code>## [1] &quot;El tamaño de muestra es 586&quot;</code></pre>
</div>
<div id="ejemplo-resumen-estimación-del-total-de-individuos-en-una-fotografía" class="section level2" number="4.7">
<h2><span class="header-section-number">4.7</span> Ejemplo Resumen: Estimación del total de individuos en una fotografía</h2>
<p>En este ejercicio vamos a determinar cuánta gente aparece en la siguiente foto:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-156"></span>
<img src="images/concierto.jpg" alt="Imagen de un concierto extraída de https://www.youtube.com/watch?v=pJ1YKwyH5bk" width="75%" />
<p class="caption">
Figure 4.1: Imagen de un concierto extraída de <a href="https://www.youtube.com/watch?v=pJ1YKwyH5bk" class="uri">https://www.youtube.com/watch?v=pJ1YKwyH5bk</a>
</p>
</div>
<p>Hay varias opciones para determinar la cantidad de gente que está en dicha foto. Una sería contar todas las cabecitas que aparecen; otra, diseñar un modelo de redes neuronales (o de <a href="https://yangliang.github.io/pdf/sp055u.pdf">convolusión</a> porque a la gente le encanta eso) que identifique una cabeza y la cuente. Nosotros lo que haremos (por ser un curso de estadística) será muestrear. Como investigador me interesa responder la siguiente pregunta:</p>
<blockquote>
<p>¿Cuánta gente está en la fotografía con un intervalo de error de <span class="math inline">\(\pm 50\)</span> casos al 95%?</p>
</blockquote>
<p>Para ello dividiremos la fotografía en <span class="math inline">\(N\)</span> pedazos (a determinar), muestrearemos <span class="math inline">\(n\)</span> de ellos y contaremos la cantidad de personas que aparecen en cada pedazo. Finalmente, generamos intervalos de confianza y de muestreo. Para ello repetimos el ejercicio anterior de despejar la <span class="math inline">\(n\)</span> del intervalo de confianza; por el teorema del límite central tenemos:</p>
<p><span class="math display">\[
\dfrac{\hat{t} - t}{\sqrt{\textrm{Var}(\hat{t})}} ~\sim \textrm{Normal}(0,1)
\]</span>
de donde obtenemos intervalos (¡verifícalo!) de la forma:
<span class="math display">\[
 \hat{t} \pm z_{1-\alpha/2}\cdot\sqrt{\textrm{Var}(\hat{t})} 
\]</span>
Donde podemos aproximar la varianza mediante <span class="math inline">\(\widehat{\text{Var}}(\hat{t}) = N^2\dfrac{1-f}{n} s^2_{x,\mathcal{S}}\)</span> donde recordamos que <span class="math inline">\(f = n/N\)</span> y <span class="math inline">\(s^2_{x,\mathcal{S}}\)</span> es la varianza muestral. Tomamos <span class="math inline">\(\epsilon = 50\)</span> y despejamos:</p>
<p><span class="math display">\[\begin{equation}\nonumber
\begin{aligned}
 \epsilon &amp; =  z_{1-\alpha/2}\cdot\sqrt{\textrm{Var}(\hat{t})} \\
 \Rightarrow \dfrac{\epsilon^2}{z_{1-\alpha/2}^2} &amp; = N^2\dfrac{1-f}{n} s^2_{x,\mathcal{S}}   \\
 \Rightarrow \dfrac{\epsilon^2}{z_{1-\alpha/2}^2 s^2_{x,\mathcal{S}} N^2} &amp; = \dfrac{1-\frac{n}{N}}{n} \\
 \Rightarrow \dfrac{\epsilon^2}{z_{1-\alpha/2}^2 s^2_{x,\mathcal{S}} N^2} &amp; = \dfrac{1}{n} - \dfrac{1}{N} \\
 \Rightarrow \dfrac{\epsilon^2}{z_{1-\alpha/2}^2 s^2_{x,\mathcal{S}} N^2} + \dfrac{1}{N} &amp; = \dfrac{1}{n}  \\
 \Rightarrow \dfrac{1}{N} \Bigg( \dfrac{\epsilon^2}{z_{1-\alpha/2}^2 s^2_{x,\mathcal{S}} N} + 1 \Bigg) &amp; = \dfrac{1}{n}  \\
  \Rightarrow \dfrac{1}{N} \Bigg( \dfrac{\epsilon^2 + z_{1-\alpha/2}^2 s^2_{x,\mathcal{S}} N}{z_{1-\alpha/2}^2 s^2_{x,\mathcal{S}} N} \Bigg) &amp; = \dfrac{1}{n}  \\
  \Rightarrow  \Bigg( \dfrac{(z_{1-\alpha/2} s_{x,\mathcal{S}} N)^2} {\epsilon^2 + z_{1-\alpha/2}^2 s^2_{x,\mathcal{S}} N}\Bigg) &amp; = n  \\
\end{aligned}
\end{equation}\]</span></p>
<p>El problema aquí es que la <span class="math inline">\(n\)</span> depende de la varianza muestral <span class="math inline">\(s^2_{x,\mathcal{S}}\)</span> (actualmente desconocida) así como de la cantidad de cuadritos originales <span class="math inline">\(N\)</span> en los que dividimos la foto. Hay en la literatura varias técnicas que se pueden utilizar para estimar el <span class="math inline">\(s^2_{x,\mathcal{S}}\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>Realizar un estudio piloto (es decir un pequeño ejemplo de lo que vas a hacer en una población chica y de ahí tener la varianza). Esta es la mejor opción.</p></li>
<li><p>Buscar otros estudios similares donde se analicen objetos similares de estudio y ver sus varianzas; suponer que la de este estudio es similar. Esta es la segunda mejor opción.</p></li>
<li><p>Inventártela (sí, es una opción pero no la mejor). Vamos, ¿cuál es la probabilidad de que nadie en todo el mundo haya hecho un análisis similar al tuyo? Si realmente estás haciendo algo completamente nuevo <em>sin estudio piloto</em> pues… podrías inventarla. ¿Lo recomiendo? No; pero pasa.</p></li>
</ol>
<p>En nuestro caso utilizaremos la varianza estimada <a href="https://arxiv.org/pdf/1903.07427.pdf">de este artículo</a> reportada en <span class="math inline">\(1.02\)</span>; luego <span class="math inline">\(s^2_{x,\mathcal{S}} \approx 1.02\)</span> para nuestro análisis.</p>
<p>Finalmente, como éste es sólo un ejercicio de clase tomaremos <span class="math inline">\(N = 100\)</span> (dividir la foto en <span class="math inline">\(100\)</span> cuadritos). De manera profesional, de nuevo habría que ver diferencias en los resultados de las estimaciones en función de los cuadritos, o bien asignar un costo a la cantidad de cuadros. Concluimos entonces que para nuestro estudio:</p>
<p><span class="math display">\[
n = \left\lceil \dfrac{(z_{1-\alpha/2} s_{x,\mathcal{S}} N)^2} {\epsilon^2 + z_{1-\alpha/2}^2 s^2_{x,\mathcal{S}} N}\right\rceil  = \left\lceil \dfrac{(1.95\cdot \sqrt{1.02} \cdot 100)^2} {50^2 + 1.95^2\cdot 1.02 \cdot 100}\right\rceil
\]</span>
Podemos calcular en <code>R</code>:</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="muestreo-aleatorio-simple.html#cb174-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">ceiling</span>((<span class="fu">qnorm</span>(<span class="fl">0.975</span>)<span class="sc">*</span><span class="fu">sqrt</span>(<span class="fl">1.02</span>)<span class="sc">*</span><span class="dv">100</span>)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>(<span class="dv">50</span><span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> (<span class="fu">qnorm</span>(<span class="fl">0.975</span>)<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span><span class="fl">1.02</span><span class="sc">*</span><span class="dv">100</span>)))</span>
<span id="cb174-2"><a href="muestreo-aleatorio-simple.html#cb174-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;El tamaño de muestra es &quot;</span>, n))</span></code></pre></div>
<pre><code>## [1] &quot;El tamaño de muestra es 14&quot;</code></pre>
<p>Podemos proceder a dividir la foto en los <span class="math inline">\(N = 100\)</span> pedazos:</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="muestreo-aleatorio-simple.html#cb176-1" aria-hidden="true" tabindex="-1"></a><span class="co">#División con base en el siguiente link:</span></span>
<span id="cb176-2"><a href="muestreo-aleatorio-simple.html#cb176-2" aria-hidden="true" tabindex="-1"></a><span class="co">#https://rpubs.com/issactoast/cutimage</span></span>
<span id="cb176-3"><a href="muestreo-aleatorio-simple.html#cb176-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(imager)</span>
<span id="cb176-4"><a href="muestreo-aleatorio-simple.html#cb176-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb176-5"><a href="muestreo-aleatorio-simple.html#cb176-5" aria-hidden="true" tabindex="-1"></a><span class="co">#Cargamos la imagen</span></span>
<span id="cb176-6"><a href="muestreo-aleatorio-simple.html#cb176-6" aria-hidden="true" tabindex="-1"></a>img <span class="ot">&lt;-</span> <span class="fu">load.image</span>(<span class="st">&quot;images/concierto.jpg&quot;</span>)</span>
<span id="cb176-7"><a href="muestreo-aleatorio-simple.html#cb176-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb176-8"><a href="muestreo-aleatorio-simple.html#cb176-8" aria-hidden="true" tabindex="-1"></a><span class="co">#Función auxiliar del link superior</span></span>
<span id="cb176-9"><a href="muestreo-aleatorio-simple.html#cb176-9" aria-hidden="true" tabindex="-1"></a>make.vr <span class="ot">&lt;-</span> <span class="cf">function</span>( x, name ){</span>
<span id="cb176-10"><a href="muestreo-aleatorio-simple.html#cb176-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">assign</span>( name, x, <span class="at">envir =</span> .GlobalEnv)</span>
<span id="cb176-11"><a href="muestreo-aleatorio-simple.html#cb176-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb176-12"><a href="muestreo-aleatorio-simple.html#cb176-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb176-13"><a href="muestreo-aleatorio-simple.html#cb176-13" aria-hidden="true" tabindex="-1"></a><span class="co">#División en N</span></span>
<span id="cb176-14"><a href="muestreo-aleatorio-simple.html#cb176-14" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb176-15"><a href="muestreo-aleatorio-simple.html#cb176-15" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="fu">sqrt</span>(N),<span class="fu">sqrt</span>(N)), <span class="at">mar =</span> <span class="fu">c</span>(<span class="fl">0.1</span>,<span class="fl">0.1</span>,<span class="fl">0.1</span>,<span class="fl">0.1</span>))</span>
<span id="cb176-16"><a href="muestreo-aleatorio-simple.html#cb176-16" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb176-17"><a href="muestreo-aleatorio-simple.html#cb176-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">sqrt</span>(N)){</span>
<span id="cb176-18"><a href="muestreo-aleatorio-simple.html#cb176-18" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">sqrt</span>(N)){</span>
<span id="cb176-19"><a href="muestreo-aleatorio-simple.html#cb176-19" aria-hidden="true" tabindex="-1"></a>    vr.name <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;sub&quot;</span>, k)</span>
<span id="cb176-20"><a href="muestreo-aleatorio-simple.html#cb176-20" aria-hidden="true" tabindex="-1"></a>    k       <span class="ot">&lt;-</span> k <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb176-21"><a href="muestreo-aleatorio-simple.html#cb176-21" aria-hidden="true" tabindex="-1"></a>    <span class="fu">imsub</span>(img, (width<span class="sc">/</span><span class="fu">sqrt</span>(N))<span class="sc">*</span>(i<span class="dv">-1</span>) <span class="sc">&lt;</span> x <span class="sc">&amp;</span> x <span class="sc">&lt;</span>  i <span class="sc">*</span> (width<span class="sc">/</span><span class="fu">sqrt</span>(N)),</span>
<span id="cb176-22"><a href="muestreo-aleatorio-simple.html#cb176-22" aria-hidden="true" tabindex="-1"></a>          (height<span class="sc">/</span><span class="fu">sqrt</span>(N))<span class="sc">*</span>(j<span class="dv">-1</span>) <span class="sc">&lt;</span> y <span class="sc">&amp;</span> y <span class="sc">&lt;</span>  j <span class="sc">*</span> (height<span class="sc">/</span><span class="fu">sqrt</span>(N))) <span class="sc">%&gt;%</span></span>
<span id="cb176-23"><a href="muestreo-aleatorio-simple.html#cb176-23" aria-hidden="true" tabindex="-1"></a>      <span class="fu">make.vr</span>(<span class="at">name =</span> vr.name) <span class="sc">%&gt;%</span></span>
<span id="cb176-24"><a href="muestreo-aleatorio-simple.html#cb176-24" aria-hidden="true" tabindex="-1"></a>      <span class="co"># save.image( file = paste0(vr.name,&quot;.jpg&quot;)) %&gt;%</span></span>
<span id="cb176-25"><a href="muestreo-aleatorio-simple.html#cb176-25" aria-hidden="true" tabindex="-1"></a>      <span class="fu">plot</span>(<span class="at">axes =</span> <span class="cn">FALSE</span>,</span>
<span id="cb176-26"><a href="muestreo-aleatorio-simple.html#cb176-26" aria-hidden="true" tabindex="-1"></a>           <span class="at">xaxt=</span><span class="st">&quot;n&quot;</span>, <span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>, </span>
<span id="cb176-27"><a href="muestreo-aleatorio-simple.html#cb176-27" aria-hidden="true" tabindex="-1"></a>           <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ann =</span> <span class="cn">FALSE</span> )    </span>
<span id="cb176-28"><a href="muestreo-aleatorio-simple.html#cb176-28" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb176-29"><a href="muestreo-aleatorio-simple.html#cb176-29" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="Introduccion_a_Muestreo_files/figure-html/unnamed-chunk-158-1.png" width="672" /></p>
<p>Podemos acceder a cada una de las imágenes que se tienen a través de su nombre (<code>sub</code> seguido de un número entre <span class="math inline">\(0\)</span> y <span class="math inline">\(100\)</span>). Muestreamos entonces los nombres de las 15 imágenes:</p>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="muestreo-aleatorio-simple.html#cb177-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Obtenemos los dígitos a muestrear</span></span>
<span id="cb177-2"><a href="muestreo-aleatorio-simple.html#cb177-2" aria-hidden="true" tabindex="-1"></a>imagenes.muestreadas <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>, n, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb177-3"><a href="muestreo-aleatorio-simple.html#cb177-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb177-4"><a href="muestreo-aleatorio-simple.html#cb177-4" aria-hidden="true" tabindex="-1"></a><span class="co">#Agregamos el prefijo sub</span></span>
<span id="cb177-5"><a href="muestreo-aleatorio-simple.html#cb177-5" aria-hidden="true" tabindex="-1"></a>imagenes.muestreadas <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;sub&quot;</span>, imagenes.muestreadas)</span></code></pre></div>
<p>Y graficamos cada una de ellas:</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="muestreo-aleatorio-simple.html#cb178-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb178-2"><a href="muestreo-aleatorio-simple.html#cb178-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb178-3"><a href="muestreo-aleatorio-simple.html#cb178-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (imagen <span class="cf">in</span> imagenes.muestreadas){</span>
<span id="cb178-4"><a href="muestreo-aleatorio-simple.html#cb178-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="fu">get</span>(imagen),  <span class="at">main =</span> imagen, <span class="at">axes =</span> <span class="cn">FALSE</span>)</span>
<span id="cb178-5"><a href="muestreo-aleatorio-simple.html#cb178-5" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="Introduccion_a_Muestreo_files/figure-html/unnamed-chunk-160-1.png" width="50%" style="display: block; margin: auto;" /><img src="Introduccion_a_Muestreo_files/figure-html/unnamed-chunk-160-2.png" width="50%" style="display: block; margin: auto;" /><img src="Introduccion_a_Muestreo_files/figure-html/unnamed-chunk-160-3.png" width="50%" style="display: block; margin: auto;" /><img src="Introduccion_a_Muestreo_files/figure-html/unnamed-chunk-160-4.png" width="50%" style="display: block; margin: auto;" /><img src="Introduccion_a_Muestreo_files/figure-html/unnamed-chunk-160-5.png" width="50%" style="display: block; margin: auto;" /><img src="Introduccion_a_Muestreo_files/figure-html/unnamed-chunk-160-6.png" width="50%" style="display: block; margin: auto;" /><img src="Introduccion_a_Muestreo_files/figure-html/unnamed-chunk-160-7.png" width="50%" style="display: block; margin: auto;" /><img src="Introduccion_a_Muestreo_files/figure-html/unnamed-chunk-160-8.png" width="50%" style="display: block; margin: auto;" /><img src="Introduccion_a_Muestreo_files/figure-html/unnamed-chunk-160-9.png" width="50%" style="display: block; margin: auto;" /><img src="Introduccion_a_Muestreo_files/figure-html/unnamed-chunk-160-10.png" width="50%" style="display: block; margin: auto;" /><img src="Introduccion_a_Muestreo_files/figure-html/unnamed-chunk-160-11.png" width="50%" style="display: block; margin: auto;" /><img src="Introduccion_a_Muestreo_files/figure-html/unnamed-chunk-160-12.png" width="50%" style="display: block; margin: auto;" /><img src="Introduccion_a_Muestreo_files/figure-html/unnamed-chunk-160-13.png" width="50%" style="display: block; margin: auto;" /><img src="Introduccion_a_Muestreo_files/figure-html/unnamed-chunk-160-14.png" width="50%" style="display: block; margin: auto;" />
Para cada una de las imágenes contamos las cabecitas que aparecen:</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="muestreo-aleatorio-simple.html#cb179-1" aria-hidden="true" tabindex="-1"></a>datos <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb179-2"><a href="muestreo-aleatorio-simple.html#cb179-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">Imagen =</span> imagenes.muestreadas,</span>
<span id="cb179-3"><a href="muestreo-aleatorio-simple.html#cb179-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">Conteo =</span> <span class="fu">c</span>(<span class="dv">13</span>, <span class="dv">11</span>, <span class="dv">9</span>, <span class="dv">14</span>, <span class="dv">9</span>, <span class="dv">15</span>, <span class="dv">14</span>, <span class="dv">10</span>, <span class="dv">1</span>, <span class="dv">22</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">17</span>, <span class="dv">16</span>)</span>
<span id="cb179-4"><a href="muestreo-aleatorio-simple.html#cb179-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb179-5"><a href="muestreo-aleatorio-simple.html#cb179-5" aria-hidden="true" tabindex="-1"></a><span class="fu">kable</span>(datos) <span class="sc">%&gt;%</span> <span class="fu">kable_styling</span>(<span class="at">latex_options =</span> <span class="st">&quot;striped&quot;</span>)</span></code></pre></div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Imagen
</th>
<th style="text-align:right;">
Conteo
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
sub52
</td>
<td style="text-align:right;">
13
</td>
</tr>
<tr>
<td style="text-align:left;">
sub43
</td>
<td style="text-align:right;">
11
</td>
</tr>
<tr>
<td style="text-align:left;">
sub34
</td>
<td style="text-align:right;">
9
</td>
</tr>
<tr>
<td style="text-align:left;">
sub35
</td>
<td style="text-align:right;">
14
</td>
</tr>
<tr>
<td style="text-align:left;">
sub54
</td>
<td style="text-align:right;">
9
</td>
</tr>
<tr>
<td style="text-align:left;">
sub23
</td>
<td style="text-align:right;">
15
</td>
</tr>
<tr>
<td style="text-align:left;">
sub83
</td>
<td style="text-align:right;">
14
</td>
</tr>
<tr>
<td style="text-align:left;">
sub14
</td>
<td style="text-align:right;">
10
</td>
</tr>
<tr>
<td style="text-align:left;">
sub79
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
sub5
</td>
<td style="text-align:right;">
22
</td>
</tr>
<tr>
<td style="text-align:left;">
sub30
</td>
<td style="text-align:right;">
8
</td>
</tr>
<tr>
<td style="text-align:left;">
sub36
</td>
<td style="text-align:right;">
9
</td>
</tr>
<tr>
<td style="text-align:left;">
sub39
</td>
<td style="text-align:right;">
17
</td>
</tr>
<tr>
<td style="text-align:left;">
sub16
</td>
<td style="text-align:right;">
16
</td>
</tr>
</tbody>
</table>
<p>Tenemos entonces que la estimación del total <span class="math inline">\(\hat{t}\)</span> es: 1200, por otro lado la varianza muestral es <span class="math inline">\(s_{x,\mathcal{S}}\)</span> está dada por: 25.2307692. Podemos entonces establecer un intervalo de confianza para el total:</p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="muestreo-aleatorio-simple.html#cb180-1" aria-hidden="true" tabindex="-1"></a>x  <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">13</span>, <span class="dv">11</span>, <span class="dv">9</span>, <span class="dv">14</span>, <span class="dv">9</span>, <span class="dv">15</span>, <span class="dv">14</span>, <span class="dv">10</span>, <span class="dv">1</span>, <span class="dv">22</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">17</span>, <span class="dv">16</span>, <span class="dv">10</span>)</span>
<span id="cb180-2"><a href="muestreo-aleatorio-simple.html#cb180-2" aria-hidden="true" tabindex="-1"></a>s2            <span class="ot">&lt;-</span> <span class="fu">var</span>(x)</span>
<span id="cb180-3"><a href="muestreo-aleatorio-simple.html#cb180-3" aria-hidden="true" tabindex="-1"></a>N             <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb180-4"><a href="muestreo-aleatorio-simple.html#cb180-4" aria-hidden="true" tabindex="-1"></a>n             <span class="ot">&lt;-</span> <span class="dv">15</span></span>
<span id="cb180-5"><a href="muestreo-aleatorio-simple.html#cb180-5" aria-hidden="true" tabindex="-1"></a>total.muestra <span class="ot">&lt;-</span> N<span class="sc">*</span><span class="fu">mean</span>(x)</span>
<span id="cb180-6"><a href="muestreo-aleatorio-simple.html#cb180-6" aria-hidden="true" tabindex="-1"></a>ci            <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(<span class="fl">0.975</span>)<span class="sc">*</span><span class="fu">sqrt</span>(N<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span> <span class="sc">-</span> n<span class="sc">/</span>N)<span class="sc">/</span>n<span class="sc">*</span>s2)</span>
<span id="cb180-7"><a href="muestreo-aleatorio-simple.html#cb180-7" aria-hidden="true" tabindex="-1"></a>ci_low        <span class="ot">&lt;-</span> <span class="fu">round</span>(total.muestra <span class="sc">-</span> ci,<span class="dv">2</span>)</span>
<span id="cb180-8"><a href="muestreo-aleatorio-simple.html#cb180-8" aria-hidden="true" tabindex="-1"></a>ci_up         <span class="ot">&lt;-</span> <span class="fu">round</span>(total.muestra <span class="sc">+</span> ci,<span class="dv">2</span>)</span>
<span id="cb180-9"><a href="muestreo-aleatorio-simple.html#cb180-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb180-10"><a href="muestreo-aleatorio-simple.html#cb180-10" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Se estiman &quot;</span>, <span class="fu">round</span>(total.muestra,<span class="dv">2</span>), <span class="st">&quot; personas con intervalo de &quot;</span>,</span>
<span id="cb180-11"><a href="muestreo-aleatorio-simple.html#cb180-11" aria-hidden="true" tabindex="-1"></a>             <span class="st">&quot;confianza al 95% de [&quot;</span>, ci_low, <span class="st">&quot; ,&quot;</span>, ci_up,<span class="st">&quot;]&quot;</span>))</span></code></pre></div>
<pre><code>## [1] &quot;Se estiman 1186.67 personas con intervalo de confianza al 95% de [959.55 ,1413.78]&quot;</code></pre>
</div>
<div id="ejercicio-7" class="section level2" number="4.8">
<h2><span class="header-section-number">4.8</span> Ejercicio:</h2>
<p>Cuando se resgistra un paquete de <code>R</code> en <a href="https://cran.r-project.org">CRAN</a> estos se registran junto con sus autores como muestra la imagen:</p>
<p><img src="images/CRAN.png" width="75%" /></p>
<p>La información de un paquete puede encontrarse en la página de <code>CRAN</code> dando clic en <code>Packages</code> y luego en <code>Table of available packages, sorted by name</code> y buscando el paquete deseado.</p>
<p>Se desea conocer el número promedio de autores por paquete registrado en <code>CRAN</code> con un intervalo de confianza al 80% y un error de <span class="math inline">\(\pm 1\)</span>. Obtén la <span class="math inline">\(n\)</span> necesaria para muestrear, calcula un estimador de la media y obtén intervalos de confianza. Justifica tu elección de la varianza para la <span class="math inline">\(n\)</span> mediante un estudio piloto (muestreando de manera inicial <span class="math inline">\(10\)</span> y calculando la varianza de ellos).</p>
<p><strong>Hint</strong> Para obtener una lista (censo) de todos los paquetes de <code>R</code> puedes utilizar la función <code>available.packages()</code> la cual devuelve una matriz con todos los paquetes e incluye la <code>url</code> de donde se encuentra.</p>
</div>
<div id="ejemplo-resumen-estimación-de-una-región-crítica" class="section level2" number="4.9">
<h2><span class="header-section-number">4.9</span> Ejemplo Resumen: Estimación de una región crítica</h2>
<p>En una elección existen dos candidatas <span class="math inline">\(A\)</span> y <span class="math inline">\(B\)</span>. Se realiza una encuesta de opinión mediante muestreo aleatorio simple sin reemplazo donde se les pregunta a una cantidad suficiente de votantes por quién votarían de las dos. En este análisis no hay <code>NO SABE / NO RESPONDE</code> sino que todos los individuos indican su preferencia. Se desea determinar la cantidad de puntos porcentuales que debe haber de diferencia entre la proporción de individuos que reportan apoyan al candidato <span class="math inline">\(A\)</span> y los que reportan que apoyan al <span class="math inline">\(B\)</span> de tal forma que el <span class="math inline">\(95\%\)</span> de las veces podamos declarar de manera adecuada al ganador.</p>
<blockquote>
<p><strong>Nota</strong> Si <span class="math inline">\(A\)</span> no es el ganador entonces <span class="math inline">\(p_A &lt; 50\%\)</span> (la proporción de votantes que van a elegir a <span class="math inline">\(A\)</span> es menor a la mitad) ¿cierto?</p>
</blockquote>
<p>Para ello el análisis es como sigue: sea <span class="math inline">\(\hat{p}_A\)</span> un estimador de la proporción de individuos que van a elegir a <span class="math inline">\(A\)</span> y <span class="math inline">\(p_A\)</span> la verdadera proporción. Sin pérdida de generalidad supondremos que <span class="math inline">\(B\)</span> es el ganador; es decir que <span class="math inline">\(p_A &lt; 0.5\)</span>.
El problema puede traducirse en determinar una <span class="math inline">\(c\)</span> tal que:
<span class="math display">\[
\mathbb{P}\big( \hat{p}_A &gt; c | p_A &lt; 0.5 \big) \leq 0.05
\]</span>
Notamos que el evento <span class="math inline">\(\{ p_A &lt; 50\%\}\)</span> es por definición conocido (con probabilidad <span class="math inline">\(0\)</span> ó <span class="math inline">\(1\)</span>) pues está dado por la población (constante). Notamos que por el teorema del límite central podemos escribir:
<span class="math display">\[
\dfrac{\hat{p}_A - p_A}{\sqrt{\text{Var}(\hat{p}_A )}}\sim \text{Normal}\big(0, 1\big)
\]</span>
donde <span class="math inline">\(\hat{p}_A = \frac{1}{N} \sum_{i = 1}^N x_i \mathbb{I}_{\mathcal{S}}(x_i)\)</span> como anteriormente hicimos para proporciones y su varianza está dada por:
<span class="math display">\[
\text{Var}(\hat{p}_A ) = \frac{p_A(1-p_A)}{n}\Big( \frac{N-1}{N-n}\Big)
\]</span>
donde el cálculo se hizo en el primer ejemplo de esta sección. Podemos transformar el problema entonces en hallar <span class="math inline">\(c\)</span> tal que:
<span class="math display">\[
\mathbb{P}\bigg( \underbrace{\frac{\hat{p}_A - p_A}{\sqrt{\text{Var}(\hat{p}_A )}}}_{Z \sim \text{Normal}(0,1)} &gt; \frac{c - p_A}{\sqrt{\text{Var}(\hat{p}_A)}} \bigg| p_A &lt; 0.5 \bigg) \leq 0.05
\]</span>
Notamos que el lado izquierdo tiene una aproximación normal y entonces podemos reescribir el problema como hallar <span class="math inline">\(c\)</span> tal que:
<span class="math display">\[
\mathbb{P}\bigg( Z &gt; \frac{c - p_A}{\sqrt{\text{Var}(\hat{p}_A)}} \bigg| p_A &lt; 0.5 \bigg) \leq 0.05 \qquad \text{ donde } Z \sim \text{Normal}(0,1).
\]</span></p>
<p>Recordando la expresión para la varianza sustituyo:
<span class="math display">\[
\mathbb{P}\left( Z &gt; \dfrac{c - p_A}{\sqrt{\frac{p_A(1-p_A)}{n}\Big( \frac{N-1}{N-n}\Big)}} \Bigg| p_A &lt; 0.5 \right) \leq 0.05 \qquad \text{ donde } Z \sim \text{Normal}(0,1).
\]</span>
En función del análisis pasado, observamos que <span class="math inline">\(\dfrac{c - p_A}{\sqrt{\frac{p_A(1-p_A)}{n}\Big( \frac{N-1}{N-n}\Big)}}\)</span> es una función decreciente en términos de <span class="math inline">\(p_A\)</span> (¡compruébalo!) y que el mínimo valor se alcanza en el máximo de la <span class="math inline">\(p_A\)</span> en el intervalo; es decir cuando <span class="math inline">\(p_A = \frac{1}{2}\)</span>. Luego el problema se transforma en hallar <span class="math inline">\(c\)</span> tal que:</p>
<p><span class="math display">\[
\mathbb{P}\left( Z &gt; \dfrac{c - \frac{1}{2}}{\sqrt{\frac{\frac{1}{2}(1-\frac{1}{2})}{n}\Big( \frac{N-1}{N-n}\Big)}} \right) \leq 0.05 \qquad \text{ donde } Z \sim \text{Normal}(0,1).
\]</span>
donde eliminamos el evento <span class="math inline">\(p_A &lt; 0.5\)</span> por ser un evento seguro. Reescribimos el evento:
<span class="math display">\[
\underbrace{\mathbb{P}\left( Z &lt; \dfrac{c - \frac{1}{2}}{\sqrt{\frac{\frac{1}{2}(1-\frac{1}{2})}{n}\Big( \frac{N-1}{N-n}\Big)}} \right)}_{\Phi(x)} \geq 0.95 \qquad \text{ donde } x = \dfrac{c - \frac{1}{2}}{\sqrt{\frac{\frac{1}{2}(1-\frac{1}{2})}{n}\Big( \frac{N-1}{N-n}\Big)}}
\]</span>
de tal forma que descubrimos la acumulada de la normal; terminamos de escribir todo:
<span class="math display">\[
\Phi(x) \geq 0.95
\]</span>
donde aplicamos la función inversa de la acumulada de la normal para descubrir:
<span class="math display">\[
\dfrac{c - \frac{1}{2}}{\sqrt{\frac{\frac{1}{2}(1-\frac{1}{2})}{n}\Big( \frac{N-1}{N-n}\Big)}} \geq \phi^{-1}(0.95) \Rightarrow c = \frac{1}{2} + \phi^{-1}(0.95)\sqrt{\frac{\frac{1}{2}(1-\frac{1}{2})}{n}\Big( \frac{N-1}{N-n}\Big)}
\]</span>
de donde se sigue que:
<span class="math display">\[
\hat{p}_{A} &gt; \frac{1}{2}\Bigg(1 + \phi^{-1}(0.95)\sqrt{\frac{N-1}{n(N-n)}} \Bigg) \Rightarrow 2\hat{p}_A = 1 + \phi^{-1}(0.95)\sqrt{\frac{N-1}{n(N-n)}} 
\]</span>
Notando que los puntos porcentuales de <span class="math inline">\(B\)</span> estimados mediante <span class="math inline">\(\hat{p}_B\)</span> tienen la forma:
<span class="math display">\[
\hat{p}_B = 1 - \hat{p}_A
\]</span>
se tiene entonces que la diferencia entre puntos para determinar quien gana es:
<span class="math display">\[
\hat{p}_A - \hat{p}_B = 2\hat{p}_A - 1 \geq \phi^{-1}(0.95)\sqrt{\frac{N-1}{n(N-n)}} 
\]</span>
El mismo análisis se seguiría bajo la hipótesis de que el perdedor es <span class="math inline">\(B\)</span>; por tanto se tiene que cumplir que:
<span class="math display">\[
| \hat{p}_A - \hat{p}_B | \geq \phi^{-1}(0.95)\sqrt{\frac{N-1}{n(N-n)}} 
\]</span>
para poder declarar como ganador a aquél con más puntos porcentuales de manera correcta con una confianza del <span class="math inline">\(95\%\)</span>.</p>
</div>
<div id="ejemplo-resumen-estimación-del-total-de-una-población" class="section level2" number="4.10">
<h2><span class="header-section-number">4.10</span> Ejemplo Resumen: Estimación del total de una población</h2>
<p>Consideremos una población de tiburones donde se desconoce el tamaño total de la población <span class="math inline">\(N\)</span>. Algunas veces para determinar el tamaño poblacional se utiliza un modelo de <em>captura y recaptura</em>. En él se capturan <span class="math inline">\(\ell\)</span> individuos los cuales se identifican (<a href="http://www.fao.org/tempref/docrep/fao/008/a0212e/a0212E04.pdf">mediante etiquetas</a>, por ejemplo) y se devuelven a convivir entre la población de <span class="math inline">\(N\)</span> para mezclarse de vuelta. Una vez mezclados, seleccionamos <span class="math inline">\(n\)</span> nuevos individuos por muestreo aleatorio simple sin reemplazo donde descubrimos que <span class="math inline">\(K\)</span> están marcados. Suponiendo que <span class="math inline">\(K \neq 0\)</span>, determinaremos un estimador <span class="math inline">\(\hat{N}\)</span> del total poblacional (en el caso <span class="math inline">\(K = 0\)</span> tuvimos muy mala suerte y seguimos recapturando tiburones hasta encontrar alguno).</p>
<p>En primer lugar notamos que los <span class="math inline">\(K\)</span> marcados que surgen en la segunda muestra siguen una distribución hipergeométrica:
<span class="math display">\[
\mathbb{P}\big( K = x) = \dfrac{\binom{\ell}{x} \binom{N-\ell}{n-x}}{\binom{N}{n}}
\]</span>
donde <span class="math inline">\(x \in \big[ \max\{ 0, \ell-N+n\}, \min\{n,\ell\}\big]\cap\mathbb{N}\)</span>. Para construir el estimador notamos que:
<span class="math display">\[
\mathbb{E}(K) = n \frac{\ell}{N}
\]</span>
de donde podemos despejar <span class="math inline">\(N\)</span>:
<span class="math display">\[
N= n \frac{\ell}{\mathbb{E}(K) }
\]</span>
Ahora bien, dada una muestra donde se obtuvieron <span class="math inline">\(K\)</span> (de <span class="math inline">\(n\)</span>) marcados se propone un estimador de <span class="math inline">\(N\)</span> dado por:
<span class="math display">\[
\hat{N} = \ell  \cdot \frac{n}{K}
\]</span>
donde <span class="math inline">\(K = \sum_{i = 1}^n x_i\)</span> donde las <span class="math inline">\(x_i = 1\)</span> si estaba marcado y <span class="math inline">\(x_i = 0\)</span> si no lo estaba. La <span class="math inline">\(K\)</span> de hecho depende de la muestra y se puede escribir como:
<span class="math display">\[
K = \sum_{i = 1}^N x_i\mathbb{I}_{\mathcal{S}}(x_i)
\]</span>
Para estimar si <span class="math inline">\(\hat{N}\)</span> es insesgado, habría que calcular su valor esperado condicional en que <span class="math inline">\(K &gt; 0\)</span>. Para ello notamos que:
<span class="math display">\[
\mathbb{E}\big[ \hat{N} | K &gt; 0\big] =(\ell n) \cdot \mathbb{E}\big[ \frac{1}{K} \big| K &gt; 0 \big]
\]</span>
Sabemos (por la desigualdad de Jensen) que <span class="math inline">\(\mathbb{E}\big[ \frac{1}{K} \big] \neq \dfrac{1}{\mathbb{E}[K]}\)</span> por lo cual aproximamos el valor esperado mediante una expansión de Taylor; es decir para una función <span class="math inline">\(f \in \mathcal{C}^2\)</span>:
<span class="math display">\[
\mathbb{E}\big[ f(X) \big] \approx \mathbb{E}\big[ f(\mu) + (X - \mu) f&#39;(\mu) +  (X - \mu)^2 f&#39;&#39;(\mu)\big] = f(\mu) + \text{Var}\big[X\big] f&#39;&#39;(\mu)
\]</span>
donde <span class="math inline">\(\mu = \mathbb{E}\big[X\big]\)</span>. En nuestro caso <span class="math inline">\(f(k) = \frac{1}{k}\)</span> y por tanto:
<span class="math display">\[
\mathbb{E}\big[ \frac{1}{K} \big| K &gt; 0 \big]\approx \dfrac{1}{\mathbb{E}\big[ K | K &gt; 0]} + 2 \cdot \dfrac{\text{Var}\big[K | K &gt; 0\big] }{\big(\mathbb{E}\big[ K | K &gt; 0]\big)^3} = \dfrac{1}{\mu} + 2 \dfrac{\sigma^2}{\mu^3}
\]</span>
Calculamos los valores esperados:
<span class="math display">\[
\mathbb{E}\big[K\big] = \underbrace{\mathbb{E}\big[K | K = 0\big]\mathbb{P}(K = 0)}_{=0} + \mathbb{E}\big[K | K &gt; 0\big]\mathbb{P}(K &gt; 0) \Rightarrow \mathbb{E}\big[K | K &gt; 0\big] = \frac{\ell n}{N} \dfrac{1}{\mathbb{P}(K &gt; 0)}
\]</span>
de donde se sigue que:
<span class="math display">\[
\mathbb{E}\big[K | K &gt; 0\big] = \frac{\ell n}{N} \dfrac{1}{1 - \mathbb{P}(K = 0)} = \dfrac{\ell n}{N} \dfrac{1}{1 - \frac{\binom{N-\ell}{n}}{\binom{N}{n}} } = \dfrac{\ell n}{N} \cdot \dfrac{\binom{N}{n}}{\binom{N}{n} - \binom{N-\ell}{n}} = \mu
\]</span></p>
<p>Por otro lado el cálculo de la varianza:
<span class="math display">\[\begin{equation}\nonumber
\begin{aligned}
 \text{Var}\big[K | K &gt; 0\big] &amp; =\mathbb{E}\big[K^2 | K &gt; 0] - \mathbb{E}\big[K | K &gt; 0]^2\\
 &amp; =  \dfrac{\mathbb{E}\big[K^2]}{\mathbb{P}(K &gt; 0)} - \mu^2 \\
 &amp; =  \dfrac{\text{Var}[K] + \mathbb{E}[K]^2}{1 - \mathbb{P}(K = 0)} - \mu^2 \\
 &amp; = \dfrac{\text{Var}[K] + \Big(n\frac{\ell}{N}\Big)^2}{1 - \mathbb{P}(K = 0)} - \mu^2\\
 &amp; = \dfrac{\frac{n\ell}{N} \cdot \frac{(N-\ell)}{N} \cdot \Big( \frac{N-n}{N-1} \Big) + \Big(n\frac{\ell}{N}\Big)^2}{1 - \mathbb{P}(K = 0)} - \mu^2 \\
 &amp; = \dfrac{\frac{n\ell}{N} \cdot \frac{(N-\ell)}{N} \cdot \Big( \frac{N-n}{N-1} \Big) + \Big(n\frac{\ell}{N}\Big)^2}{1 -  \frac{\binom{N-\ell}{n}}{\binom{N}{n}}} - \mu^2 \\
 &amp; = \binom{N}{n} \dfrac{\frac{n\ell}{N} \cdot \frac{(N-\ell)}{N} \cdot \Big( \frac{N-n}{N-1} \Big) + \Big(n\frac{\ell}{N}\Big)^2}{\binom{N}{n} -  \binom{N-\ell}{n}}- \mu^2 &amp; = \sigma^2\\
\end{aligned}
\end{equation}\]</span></p>
<p>Donde se tiene entonces que:
<span class="math display">\[
\mathbb{E}\big[ \hat{N} | K &gt; 0\big] \approx (\ell n) \Bigg[ \cdot \dfrac{1}{\mathbb{E}\big[ K | K &gt; 0]} + 2 \cdot \dfrac{\text{Var}\big[K | K &gt; 0\big] }{\big(\mathbb{E}\big[ K | K &gt; 0]\big)^3} \Bigg]
\]</span>
con los valores estimados en los renglones anteriores. En particular, <span class="math inline">\(\hat{N}\)</span> no es insesgado pero puede demostrarse que en el límite <span class="math inline">\(\lim_{\substack{n \to \infty \\ N-n\to\infty}}\)</span> lo es.</p>
<p>De manera similar puede obtenerse (ver Lohr capítulo 13):
<span class="math display">\[
\text{Var}\big[ \hat{N} | K &gt; 0\big]\approx \Big(\dfrac{n \ell}{K}\Big)^2 \dfrac{\ell - K}{K(\ell - 1)}
\]</span>
Misma que puede utilizarse para los intervalos de confianza.</p>
</div>
<div id="demostración-del-teorema-del-límite-central-para-muestras-finitas" class="section level2" number="4.11">
<h2><span class="header-section-number">4.11</span> Demostración del Teorema del Límite Central para Muestras Finitas</h2>
<p>PRONTO
<!--
**OJO ESTA SECCIÓN TIENE UN ERROR: CUANDO TERMINE DE CORREGIRLO LES AVISO CONTINÚA HASTA LA PARTE DE BERNOULLI**
Éste no es el teorema más general pero es una adaptación suficiente para nuestros propósitos. Para ello el esquema de demostraciones es como sigue:

  1. Primero probamos el Teorema del Límite Central bajo [la condición de Lindberg](https://en.wikipedia.org/wiki/Lindeberg%27s_condition)
  
  2. Usamos dicha versión del TLC (más general que el de Proba 2) para demostrar el teorema de límite central para poblaciones finitas la cual es una adaptación del de Hajek basada en la que presenta [Lehmann](https://www.springer.com/gp/book/9780387985954#:~:text=Elements%20of%20Large-Sample%20Theory%20provides%20a%20unified%20treatment%20of,the%20asymptotics%20of%20survey%20methodology.) 
  
## Teorema de Límite Central bajo condición de Lindberg

Eventualmente, si encuentro una forma de ponerla en términos de Proba 2 aparecerá por ahora te recomiendo checar [este artículo](https://dornsife.usc.edu/assets/sites/1193/docs/lin.pdf)

## Teorema de Límite Central para poblaciones finitas

Introducimos la notación: $\{ \Pi_N \}_{N \in \mathbb{N}}$ es una sucesión de poblaciones de tamaño creciente donde $\Pi_1 = \{ \nu_{1,1}\}$, $\Pi_2 = \{ \nu_{2,1}, \nu_{2,2}\}$ y en general $\Pi_N = \{ \nu_{1,N}, \nu_{2,N}, \dots, \nu_{N,N}\}$. Supongamos además para cada $\Pi_N$ se toma una muestra aleatoria de tamaño $n_N = n(N)$ que depende, de alguna forma, de $N$. Los $n$ valores que se incluyen en la suma son $x_{N,1}, x_{N,2}, \dots, x_{N,n}$ y su suma es:
$$
S_N = \sum\limits_{i = 1}^n x_{N,i}
$$
Por otro lado el promedio de los $N$ valores de $\Pi_N$ es:
$$
\mu_N = \dfrac{1}{N}\sum\limits_{j = 1}^N \nu_{n,j}
$$
Bajo esta notación se tiene el siguiente teorema:

**TEOREMA DEL LÍMITE CENTRAL PARA POBLACIONES FINITAS** Sea 
$$
S_N^* = \dfrac{S_N - \mathbb{E}[S_N]}{\sqrt{\textrm{Var}(S_N)}}
$$
entonces $S^*_N$ converge en distribución a una variable aleatoria $\text{Normal}(0,1)$ si $n, (N-n) \to \infty$ y se cumple la siguiente condición de Lindberg:

$$
\lim _{N\to \infty }{\frac {1}{s_{N}^{2}}}\sum _{k=1}^{N}\mathbb {E} \left[(X_{k}-\mu _{k})^{2}\cdot \mathbf {1} _{\{|X_{k}-\mu _{k}|>\varepsilon s_{N}\}}\right]=0
$$

con 

$$
s_N^2 = \sum\limits_{i = 1}^N (\nu_{N,i} - \nu_N)^2 \frac{n}{N}\Big( 1 - \frac{n}{N}\Big)
$$

y los $\mu_k$ dados por:
$$
\mu_k = (\nu_{N,k} - \nu_N) \frac{n}{N}
$$


**Demostración** Las $S_N$ no son independientes entre sí pero construiremos una nueva sucesión de varianles aleatorias independientes que tenga el mismo límite y a la que podamos apliar el Teorema del Límite Central. Para ello tomamos una colección $\{ U_i \}_{i = 1}^N$ de $N$ variables aleatorias independientes con distribución $\text{Uniforme}(0,1)$. Asignamos a cada una de estas $U_i$ su rango; es decir:
$$
R\big(U_i\big) = j \Leftrightarrow U_{(j)} = U_i
$$
(con palabras, $U_i$ es la $j$-ésima en el momento en el que ordenamos todas las $U$s). Una muestra de tamaño $n$ la podemos construir como sigue: un elemento $\nu_{i,N}$ se incluye en la muestra si y sólo si $R(U_i) \leq n$. Como las $U_i$ son uniformes e independientes se puede demostrar usando estadísticos de orden que cada una de las $\binom{N}{n}$ muestras es igualmente probable. Podemos entonces escribir cada $S_N$ como:
$$
S_N = \sum\limits_{i = 1}^N \nu_{N,i} \mathbb{I}_{[0,n]}\big(R(U_i) \big)
$$
Esta suma, sin embargo, está construida con variables _dependientes_ pues los rangos $R(U_i)$ son dependientes entre sí. Podemos construir otra suma distinta dada por:
$$
T_N = \sum\limits_{i = 1}^N (\nu_{N,i} - \mu_N) \mathbb{I}_{[0,\frac{n}{N}]}(U_i) + n \mu_N
$$
Donde demostraremos que en el límite $T_N$ toma los mismos valores que $S_N$. Por ahora pensemos que eso pasa; entonces basta con probar que $T_N \to \text{Normal}(0,1)$ cuando $N \to \infty$. Podemos aplicar el teorema central del límite a las variables aleatorias dadas por:
$$
X_{N,i} = (\nu_{N,i} - \nu_N) \mathbb{I}_{[0,\frac{n}{N}]}(U_i)
$$
Éstas sí son independientes pues las $U_i$ son independientes y por tanto que las $U_i$ caigan o no en el intervalo $[0,\frac{n}{N}]$ es un evento independiente para cada $i$. Podemos calcular la media y varianza de las $X_{N,i}$:
$$
\mu_{N,i} = (\nu_{N,i} - \mu_N) \frac{n}{N} \qquad \text{ y }\qquad  \sigma^2_{N,i} = (\nu_{N,i} - \nu_N)^2 \dfrac{n}{N}\Big( 1 - \dfrac{n}{N} \Big)
$$
Pues 
$$
\mathbb{E}\big[ X_{N,i}\big] = (\nu_{N,i} - \nu_N) \mathbb{E}\big[\mathbb{I}_{[0,\frac{n}{N}]}(U_i)\big] = (\nu_{N,i} - \nu_N) \mathbb{P}\big(U_i \leq \frac{n}{N}\big) =  (\nu_{N,i} - \nu_N) \frac{n}{N}
$$
y por otro lado:
$$
\text{Var}\big[ X_{N,i}\big] = (\nu_{N,i} - \nu_N)^2 \text{Var}\big[\mathbb{I}_{[0,\frac{n}{N}]}(U_i)\big] = (\nu_{N,i} - \nu_N)^2 p(1-p) =  (\nu_{N,i} - \nu_N)^2 \frac{n}{N}\Big( 1 - \frac{n}{N}\Big)
$$
En este caso podemos traducir la condición de Lindberg dada por:
$$
\lim _{N\to \infty }{\frac {1}{s_{N}^{2}}}\sum _{k=1}^{N}\mathbb {E} \left[(X_{k}-\mu _{k})^{2}\cdot \mathbf {1} _{\{|X_{k}-\mu _{k}|>\varepsilon s_{N}\}}\right]=0
$$

$\forall \varepsilon >0$ con 
$$
s_N^2 = \sum\limits_{i = 1}^N (\nu_{N,i} - \nu_N)^2 \frac{n}{N}\Big( 1 - \frac{n}{N}\Big)
$$
y los $\mu_k$ de arriba.
Lo único que falta por ver es que en el límite $T_N$ toma los mismos valores que $S_N$. Para ello demostraremos que:
$$
\lim_{N \to \infty} \dfrac{\mathbb{E}\big[ (T_N - S_N)^2 \big]}{\text{Var}(T_N)} = 0
$$

Para ello introducimos la notación:
$$
J_i = \mathbb{I}_{[0,\frac{n}{N}]}\bigg(\frac{R_i}{N}\bigg) \qquad K_i= \mathbb{I}_{[0,\frac{n}{N}]}(U_i) = \mathbb{I}_{[0,\frac{n}{N}]}(U_{(R_i)})
$$
donde $R_i = R(U_i)$. Tenemos entonces (demuestra) que:
$$
T_N - S_N = \sum\limits_{i = 1}^N (\nu_{N,i} - \mu_N)\Big[ K_i - J_i\Big]
$$
Calcularemos la esperanza condicional de $\mathbb{E}\Big[ (T_N - S_N)^2 | U_{(1)}, U_{(2)}, \dots, U_{(N)}\Big]$ y después usaremos la propiedad de torre para obtener $\mathbb{E}\Big[ (T_N - S_N)^2\Big]$. Para ello notamos que:
$$
\mathbb{E}\Big[ K_i - J_i | U_{(1)}, U_{(2)}, \dots, U_{(N)}\Big] = 0
$$
pues es independiente de $i$; por otro lado, $\sum_{i = 1}^N (\nu_{N,i} - \mu_n) = 0$ Luego $\mathbb{E}\Big[ (T_N - S_N) | U_{(1)}, U_{(2)}, \dots, U_{(N)}\Big] = 0$ y por tanto:
$$
\mathbb{E}\Big[ (T_N - S_N)^2| U_{(1)}, U_{(2)}, \dots, U_{(N)}\Big] = \textrm{Var}\Big[ (T_N - S_N)^2| U_{(1)}, U_{(2)}, \dots, U_{(N)}\Big]
$$
Una cota para la varianza está dada por [Lehmann](https://www.springer.com/gp/book/9780387985954#:~:text=Elements%20of%20Large-Sample%20Theory%20provides%20a%20unified%20treatment%20of,the%20asymptotics%20of%20survey%20methodology.) (ecuación A.49) donde:
$$
 \textrm{Var}\Big[ (T_N - S_N)^2| U_{(1)}, U_{(2)}, \dots, U_{(N)}\Big]\leq
 \dfrac{1}{N-1}\sum_{i = 1}^N (\nu_{N,i} - \mu_n)^2\sum_{i = 1}^N (K_i - J_i)
$$
donde $\sum_{i = 1}^N (K_i - J_i) = |D - n|$ donde $D$ es la cantidad de $U_i$ de valor menor o igual a $n/N$. Si usamos la propiedad de torre:

$$
 \mathbb{E}\Big[ (T_N - S_N)^2\Big]\leq
 \dfrac{1}{N-1}\sum_{i = 1}^N (\nu_{N,i} - \mu_n)^2\mathbb{E}\Big[ | D - n| \Big]
$$
Recordando la desigualdad de Jensen:
$$
\mathbb{E}\Big[ | D - n| \Big]^2 = \mathbb{E}\Big[ Y^2\Big] = \text{Var}(Y)
$$
Aplicamos dicha desigualdad a $Y = D - n$ donde $D \sim\text{Binomial}(n/N, N)$ (demuestra). Tenemos entonces:
$$
\mathbb{E}\Big[ | D - n| \Big] \leq\sqrt{ N \frac{n}{N}\Big( 1 - \frac{n}{N}\Big)}
$$
y por tanto:

$$
 \mathbb{E}\Big[ (T_N - S_N)^2\Big]\leq
 \dfrac{1}{N-1}\sum_{i = 1}^N (\nu_{N,i} - \mu_n)^2\sqrt{ N \frac{n}{N}\Big( 1 - \frac{n}{N}\Big)}
$$
Finalmente, notamos que:
$$
\text{Var}(T_N) = \sum\limits_{i = 1}^N (\nu_{N,i} - \mu_n)^2\frac{n}{N}\Big( 1 - \frac{n}{N}\Big)
$$
de donde se sigue que:

$$
\dfrac{\mathbb{E}\Big[ (T_N - S_N)^2\Big]}{\text{Var}(T_N) }\leq \frac{N}{N-1} \sqrt{\frac{N}{n(N-n)}}
$$
Este último límite es una cuestión de cálculo comprobar que tiende a $0$ cuando $N - n$ tiende a infinito pues si tomamos $\alpha \in (0,1)$ y suponemos sin pérdida de generalidad que $n \geq \alpha N$ entonces:
$$
\sqrt{\frac{N}{n(N-n)}} \leq \sqrt{\frac{1/\alpha}{N-n}}\to 0 \qquad \textrm{cuando} \qquad (N-n) \to \infty
$$

--></p>
</div>
<div id="muestreo-aleatorio-simple-bernoulli-be" class="section level2" number="4.12">
<h2><span class="header-section-number">4.12</span> Muestreo Aleatorio Simple Bernoulli (BE)</h2>
<p>En un esquema de muestreo Bernoulli (BE) se tiene una población de tamaño <span class="math inline">\(N\in\mathbb{N}\)</span> (constante) la cual se enlista de manera ordenada <span class="math inline">\(U = (x_1,x_2,\dots,x_N)^T\)</span>. Se recorre la lista de <span class="math inline">\(1\)</span> hasta <span class="math inline">\(N\)</span>. Cada elemento de la población, se selecciona y se mide con probabilidad <span class="math inline">\(\pi \in (0,1)\)</span> para generar una muestra <span class="math inline">\(\mathcal{S} = (x_1, x_2, \dots, x_n)^T\)</span> de tamaño <span class="math inline">\(n = n(\mathcal{S})\)</span> aleatorio (con <span class="math inline">\(0 \leq n(\mathcal{S}) \leq N\)</span>).</p>
<blockquote>
<p>Un ejemplo de muestreo Bernoulli ocurre en las aduanas del Sistema de Administración Tributaria (SAT) donde con probabilidad <span class="math inline">\(\pi\)</span> se revisa la mercancía de un viajero (de un total predefinido de <span class="math inline">\(N\)</span> viajeros) para verificar no haya contrabando y con probabilidad <span class="math inline">\(1-\pi\)</span> se le deja entrar al país sin revisar su mercancía.</p>
</blockquote>
<p>Un muestreo Bernoulli no necesariamente tiene muestras del mismo tamaño: como el que cada elemento esté en la muestra depende de <span class="math inline">\(\pi\)</span> entonces <span class="math inline">\(n(\mathcal{S})\)</span> es una variable aleatoria con distribución Binomial:
<span class="math display">\[
n(\mathcal{S})\sim \textrm{Binomial}(N, \pi)
\]</span>
con media y varianza dadas por:
<span class="math display">\[
\mathbb{E}\Big[ n(\mathcal{S})\Big] = N\pi \quad \text{ y } \quad \text{Var}\Big[ n(\mathcal{S})\Big] = N\pi(1 - \pi)
\]</span>
Una forma de muestrear de un muestreo Bernoulli es recorrer uno a uno los elementos de la muestra y generar una variable aleatoria <span class="math inline">\(B_i \sim \textrm{Bernoulli}(\pi)\)</span> de tal forma que si <span class="math inline">\(B_i = 1\)</span> se incluye el elemento en la muestra. Este esquema está programado en <code>R</code> como sigue:</p>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="muestreo-aleatorio-simple.html#cb182-1" aria-hidden="true" tabindex="-1"></a>datos <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Edad =</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">12</span>, <span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">14</span>),</span>
<span id="cb182-2"><a href="muestreo-aleatorio-simple.html#cb182-2" aria-hidden="true" tabindex="-1"></a>                    <span class="at">Raza =</span> <span class="fu">c</span>(<span class="st">&quot;Labrador&quot;</span>, <span class="st">&quot;Pomeranio&quot;</span>,<span class="st">&quot;Labrador&quot;</span>,</span>
<span id="cb182-3"><a href="muestreo-aleatorio-simple.html#cb182-3" aria-hidden="true" tabindex="-1"></a>                             <span class="st">&quot;Pastor Alemán&quot;</span>, <span class="st">&quot;Bulldog&quot;</span>,<span class="st">&quot;Bulldog&quot;</span>, <span class="st">&quot;Chihuahua&quot;</span>))</span>
<span id="cb182-4"><a href="muestreo-aleatorio-simple.html#cb182-4" aria-hidden="true" tabindex="-1"></a>datos<span class="sc">$</span>en_muestra <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb182-5"><a href="muestreo-aleatorio-simple.html#cb182-5" aria-hidden="true" tabindex="-1"></a>proba   <span class="ot">&lt;-</span> <span class="dv">3</span><span class="sc">/</span><span class="dv">4</span></span>
<span id="cb182-6"><a href="muestreo-aleatorio-simple.html#cb182-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(datos)){</span>
<span id="cb182-7"><a href="muestreo-aleatorio-simple.html#cb182-7" aria-hidden="true" tabindex="-1"></a>  Bi <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dv">1</span>, <span class="at">prob =</span> <span class="fu">c</span>(<span class="dv">1</span> <span class="sc">-</span> proba, proba))</span>
<span id="cb182-8"><a href="muestreo-aleatorio-simple.html#cb182-8" aria-hidden="true" tabindex="-1"></a>  datos<span class="sc">$</span>en_muestra[i] <span class="ot">&lt;-</span> Bi</span>
<span id="cb182-9"><a href="muestreo-aleatorio-simple.html#cb182-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb182-10"><a href="muestreo-aleatorio-simple.html#cb182-10" aria-hidden="true" tabindex="-1"></a>muestra <span class="ot">&lt;-</span> datos <span class="sc">%&gt;%</span> <span class="fu">filter</span>(en_muestra <span class="sc">==</span> <span class="dv">1</span>)</span></code></pre></div>
<p>Bajo este esquema se tiene que:
<span class="math display">\[
\pi_k = \mathbb{P}(x_k \in \mathcal{S}) = \pi \qquad \forall k
\]</span>
Además en este caso las <span class="math inline">\(\{ \mathbb{I}_{\mathcal{S}}(x_k) \}_k\)</span> son independientes y por tanto:
<span class="math display">\[
\pi_{k,l} = \pi^2
\]</span>
En caso de muestreo aleatorio Bernoulli tenemos que un estimador del total es de la misma forma que en el caso de muestreo aleatorio simple:
<span class="math display">\[
\hat{t}_{\pi} = \frac{1}{\pi} \sum\limits_{i = 1}^{n(\mathcal{S})} x_i
\]</span>
El cual es insesgado pues usando indicadoras reescribimos <span class="math inline">\(\hat{t}_{\pi} = \frac{1}{\pi} \sum\limits_{i = 1}^N x_i \mathbb{I}_{\mathcal{S}}(x_i)\)</span> y tomamos valor esperado:
<span class="math display">\[
\mathbb{E}\Big[ \hat{t}_{\pi} \Big] = \frac{1}{\pi} \sum\limits_{i = 1}^N x_i \mathbb{E}\Big[\mathbb{I}_{\mathcal{S}}(x_i)\Big] =\frac{1}{\pi}\sum\limits_{i = 1}^N x_i\pi = \sum\limits_{i = 1}^N x_i = t
\]</span>
por otro lado su varianza está dada por:
<span class="math display">\[
\textrm{Var}_{\text{BE}}(\hat{t}_{\pi}) = \Big( \frac{1}{\pi} - 1\Big)\sum\limits_{i = 1}^N x_i^2
\]</span>
la cual puede estimarse de manera insesgada mediante:
<span class="math display">\[
\widehat{\textrm{Var}}_{\text{BE}}(\hat{t}_{\pi}) = \frac{1}{\pi}\Big( \frac{1}{\pi} - 1\Big)\sum\limits_{i = 1}^{n(\mathcal{S})} x_i^2
\]</span></p>
<div id="ejercicio-8" class="section level3" number="4.12.1">
<h3><span class="header-section-number">4.12.1</span> Ejercicio</h3>
<ol style="list-style-type: decimal">
<li><p>Demuestra la expresión para <span class="math inline">\(\textrm{Var}_{\text{BE}}(\hat{t}_{\pi})\)</span></p></li>
<li><p>Demuestra que <span class="math inline">\(\widehat{\textrm{Var}}_{\text{BE}}(\hat{t}_{\pi})\)</span> es un estimador insesgado de <span class="math inline">\(\textrm{Var}_{\text{BE}}(\hat{t}_{\pi})\)</span>.</p></li>
</ol>
</div>
<div id="ejemplo-1" class="section level3" number="4.12.2">
<h3><span class="header-section-number">4.12.2</span> Ejemplo</h3>
<p>Consideraremos un ejemplo presentado por <em>Särndal et al</em>. Un profesor corrige 600 exámenes. Quiere tener un estimado de la calificación de sus alumnos y para ello cada que aparece un examen tira un dado justo de <span class="math inline">\(6\)</span> caras y si sale un <span class="math inline">\(6\)</span> corrige dicho examen; en caso contrario lo deja pasar. Al final del análisis el profe obtiene una muestra de <span class="math inline">\(90\)</span> estudiantes de los cuales <span class="math inline">\(60\)</span> pasaron. Asignamos <span class="math inline">\(x_i = 0\)</span> si un alumno no pasó y <span class="math inline">\(x_i = 1\)</span> si pasó; de esta forma la estimación de la cantidad de alumnos que pasaron es un total dado por:
<span class="math display">\[
\hat{t} = \frac{1}{\pi} \sum\limits_{i = 1}^{90} x_i = \dfrac{1}{\frac{1}{6}} 60 = 360
\]</span>
El profe, después de pensarlo un rato se le ocurre otra manera de estimar la proporción de los alumnos que pasaron. Si pasaron <span class="math inline">\(60/90\)</span> se tiene entonces que <span class="math inline">\(2/3\)</span> de los alumnos pasan; aplicando el <span class="math inline">\(2/3\)</span> a los <span class="math inline">\(600\)</span> alumnos que tiene un estimador alternativo del total sería:
<span class="math display">\[
\hat{t}_{\text{Alt}} = \dfrac{2}{3}\cdot 600 = 400
\]</span>
El cual escrito en términos de las variables utilizadas es:
<span class="math display">\[
\hat{t}_{\text{Alt}} = \begin{cases}
\frac{N}{n(\mathcal{S})} \cdot \sum\limits_{i = 1}^{n(\mathcal{S})} x_i &amp; \text{ si } n(\mathcal{S}) &gt; 0 \\
0 &amp; \text{ si } n(\mathcal{S}) = 0
\end{cases}
\]</span>
La pregunta obligada es ¿cuál es un mejor estimador si <span class="math inline">\(\hat{t}\)</span> o bien <span class="math inline">\(\hat{t}_{\text{Alt}}\)</span>?</p>
</div>
<div id="un-mejor-estimador-el-proporcional-al-tamaño" class="section level3" number="4.12.3">
<h3><span class="header-section-number">4.12.3</span> Un mejor estimador: el proporcional al tamaño</h3>
<p>Para decidir si <span class="math inline">\(\hat{t}_{\text{Alt}}\)</span> es un mejor estimador que <span class="math inline">\(\hat{t}\)</span> calculemos su valor esperado y su varianza. En ambos casos tenemos dos cosas aleatorias: los elementos que sí quedaron en la muestra (las <span class="math inline">\(x_i\)</span>) y el tamaño de muestra (la <span class="math inline">\(n\)</span>). Para ello utilizamos <a href="https://en.wikipedia.org/wiki/Law_of_total_expectation">la propiedad de torre de la esperanza condicional</a>:</p>
<p><span class="math display">\[\begin{equation}\nonumber
\begin{aligned}
 \mathbb{E}\big[ \hat{t}_{\text{Alt}} \big] &amp; = \mathbb{E}\Big[ \mathbb{E}\big[ \hat{t}_{\text{Alt}}  \big| n(\mathcal{S}) = k \big] \Big]   \\
 &amp; = \sum\limits_{k = 0}^N  \mathbb{E}\Big[ \hat{t}_{\text{Alt}} \Big| n(\mathcal{S}) = k \Big]  \cdot \mathbb{P}\big(n(\mathcal{S}) = k\big)  
 \\ &amp; =  \sum\limits_{k = 1}^N \mathbb{E}\Big[ \frac{N}{n(\mathcal{S})} \cdot \sum\limits_{i = 1}^{N} x_i \mathbb{I}_{\mathcal{S}}(x_i)  \Big| n(\mathcal{S}) = k \Big]  \cdot \mathbb{P}\big(n(\mathcal{S}) = k\big) 
 \\ &amp; = 
\sum\limits_{k = 1}^N \mathbb{E}\Big[ \frac{N}{k} \cdot \sum\limits_{i = 1}^{N} x_i \mathbb{I}_{\mathcal{S}}(x_i) \Big| n(\mathcal{S}) = k \Big]  \cdot \mathbb{P}\big(n(\mathcal{S}) = k\big) 
\\ &amp; = \sum\limits_{k = 1}^N \frac{N}{k} \mathbb{E}\Big[ \sum\limits_{i = 1}^N x_i\mathbb{I}_{\mathcal{S}}(x_i) \Big| n(\mathcal{S}) = k \Big] \binom{N}{k} \pi^k (1 - \pi)^{N - k}
\\ &amp; = \sum\limits_{k = 1}^N  \Bigg( \frac{N}{k}\sum\limits_{i = 1}^N x_i \mathbb{E}\Big[\mathbb{I}_{\mathcal{S}}(x_i) \Big| n(\mathcal{S}) = k \Big]\Bigg) \binom{N}{k} \pi^k (1 - \pi)^{N - k}
\\ &amp; = \sum\limits_{k = 1}^N  \Bigg( \frac{N}{k}\sum\limits_{i = 1}^N x_i \frac{k}{N} \Bigg) \binom{N}{k} \pi^k (1 - \pi)^{N - k}
\\ &amp; = \Bigg( \sum\limits_{i = 1}^N x_i \Bigg) \cdot  \sum\limits_{k = 1}^N \Bigg( \binom{N}{k} \pi^k (1 - \pi)^{N - k}\Bigg) 
\\ &amp; = t \cdot \big( 1 - (1 - \pi)^N\big)
\end{aligned}
\end{equation}\]</span></p>
<p>en este caso el estimador <em>no</em> es insesgado y su sesgo es <span class="math inline">\((1 - \pi)^N\)</span>. Este sesgo es prácticamente ignorable pues para aplicaciones con <span class="math inline">\(N\)</span> grande <span class="math inline">\((1 - \pi)^N \approx 0\)</span> y no habrá mucha variación en el resultado.</p>
<p><strong>Definición</strong>
Dado <span class="math inline">\(\hat{\theta}\)</span> estimador de <span class="math inline">\(\theta\)</span> definimos el <strong>sesgo</strong> de <span class="math inline">\(\hat{\theta}\)</span> como:
<span class="math display">\[
\text{Sesgo}(\hat{\theta})= \mathbb{E}\Big[\hat{\theta} - \theta \Big]
\]</span></p>
<p>Podemos calcular la varianza de nuestro estimador; para ello denotamos
<span class="math display">\[
H(\pi,N) = \sum\limits_{k = 1}^N \dfrac{1}{k}\binom{N}{k} \pi^N (1 - \pi)^{N - k} - \dfrac{\big( 1 - (1 - \pi)^N\big)}{N}
\]</span></p>
<p>Luego:</p>
<p><span class="math display">\[\begin{equation}\nonumber
\begin{aligned}
 \text{Var}\big[ \hat{t}_{\text{Alt}} \big] &amp; = \mathbb{E}\big[ \hat{t}_{\text{Alt}}^2 \big] - \mathbb{E}\big[ \hat{t}_{\text{Alt}} \big]^2
 \\ &amp; = \mathbb{E}\big[ \hat{t}_{\text{Alt}}^2 \big] - \Big( t \cdot \big( 1 - (1 - \pi)^N\big)\Big)^2
 \\ &amp; = \mathbb{E}\Big[ \mathbb{E}\big[ \hat{t}_{\text{Alt}}^2 \big| n(\mathcal{S}) = k\big]\Big] - \Big( t \cdot \big( 1 - (1 - \pi)^N\big)\Big)^2
 \\ &amp; = \sum\limits_{k = 1}^N \mathbb{E}\big[ \hat{t}_{\text{Alt}}^2 \big| n(\mathcal{S}) = k\big]\cdot \mathbb{P}\big(n(\mathcal{S}) = k\big) - \Big( t \cdot \big( 1 - (1 - \pi)^N\big)\Big)^2
 \\ &amp; = \sum\limits_{k = 1}^N \dfrac{N^2}{k^2} \mathbb{E}\bigg[ \Big( \sum\limits_{i = 1}^N x_i \mathbb{I}_{\mathcal{S}}(x_i)\Big)^2 \bigg| n(\mathcal{S}) = k\bigg]\cdot \mathbb{P}\big(n(\mathcal{S}) = k\big) - \Big( t \cdot \big( 1 - (1 - \pi)^N\big)\Big)^2
\end{aligned}
\end{equation}\]</span>
Notamos que:
<span class="math display">\[\begin{align*}
\mathbb{E}\bigg[ \Big( \sum\limits_{i = 1}^N x_i &amp;  \mathbb{I}_{\mathcal{S}}(x_i)\Big)^2 \bigg| n(\mathcal{S})  = k\bigg] \\ &amp; 
= \textrm{Var}\bigg[ \Big( \sum\limits_{i = 1}^N x_i \mathbb{I}_{\mathcal{S}}(x_i)\Big) \bigg| n(\mathcal{S})  = k\bigg] + \mathbb{E}\bigg[ \Big( \sum\limits_{i = 1}^N x_i \mathbb{I}_{\mathcal{S}}(x_i)\Big) \bigg| n(\mathcal{S})  = k\bigg]^2
\\ &amp; =  \sum\limits_{i = 1}^N x_i^2 \textrm{Var}\Big[ \mathbb{I}_{\mathcal{S}}(x_i) \Big| n(\mathcal{S})  = k\Big] +  \sum\limits_{i = 1}^N\sum\limits_{\substack{j = 1 \\ j \neq i}}^N x_i x_j \textrm{Cov}\Big[ \mathbb{I}_{\mathcal{S}}(x_i), \mathbb{I}_{\mathcal{S}}(x_j) \Big| n(\mathcal{S})  = k\Big] 
\\ &amp; \qquad +  \bigg( \sum\limits_{i = 1}^N x_i \mathbb{E}\Big[  \mathbb{I}_{\mathcal{S}}(x_i) \Big| n(\mathcal{S})  = k\Big]\bigg)^2
\\ &amp; = \sum\limits_{i = 1}^N x_i^2 \dfrac{k}{N}\Big( 1 - \frac{k}{N}\Big) +  \sum\limits_{i = 1}^N\sum\limits_{\substack{j = 1 \\ j \neq i}}^N x_i x_j \Big( \dfrac{k(k-1)}{N(N-1)} - \dfrac{k^2}{N^2}\Big) + \Bigg( \dfrac{k}{N} \sum\limits_{i = 1}^N x_i \Bigg)^2
\\ &amp; =  \dfrac{k}{N}\Bigg[ \sum\limits_{i = 1}^N x_i^2\Big( 1 - \frac{k}{N}\Big) +  \sum\limits_{i = 1}^N\sum\limits_{\substack{j = 1 \\ j \neq i}}^N x_i x_j \bigg( \dfrac{k-1}{N-1} - \dfrac{k}{N}\bigg)\Bigg] + k^2 \bar{x}_{\mathcal{U}}^2
\\ &amp; =  \dfrac{k}{N}\Bigg[ \sum\limits_{i = 1}^N x_i^2\Big(\frac{N-k}{N}\Big) -  \sum\limits_{i = 1}^N\sum\limits_{\substack{j = 1 \\ j \neq i}}^N x_i x_j \bigg(  \dfrac{N-k}{N(N-1)}\bigg)\Bigg] + k^2 \bar{x}_{\mathcal{U}}^2
\\ &amp; =  \dfrac{k}{N}(N-k) \Bigg[ \frac{1}{N}\sum\limits_{i = 1}^N x_i^2 -  \frac{1}{N}\frac{1}{N-1}\sum\limits_{i = 1}^N\sum\limits_{\substack{j = 1 \\ j \neq i}}^N x_i x_j \Bigg] + k^2 \bar{x}_{\mathcal{U}}^2
\\ &amp; =  \dfrac{k}{N}(N-k)  \frac{1}{N-1}\sum\limits_{i = 1}^N \Bigg[ \frac{N-1}{N} x_i^2 -  \frac{1}{N} x_i \sum\limits_{\substack{j = 1 \\ j \neq i}}^N  x_j \Bigg] + k^2 \bar{x}_{\mathcal{U}}^2
\\ &amp; =  \dfrac{k}{N}(N-k)  \frac{1}{N-1}\sum\limits_{i = 1}^N \Bigg[  x_i^2 -  \frac{1}{N} x_i \sum\limits_{j = 1}^N  x_j \Bigg] + k^2 \bar{x}_{\mathcal{U}}^2
\\ &amp; =  \dfrac{k}{N}(N-k)  \frac{1}{N-1} \Bigg[  \sum\limits_{i = 1}^N x_i^2 -  \frac{1}{N}\bigg( \sum\limits_{i = 1}^N x_i \bigg) \bigg( \sum\limits_{j = 1}^N  x_j\bigg) \Bigg] + k^2 \bar{x}_{\mathcal{U}}^2
\\ &amp; =  \dfrac{k}{N}(N-k)  \frac{1}{N-1} \Bigg[  \sum\limits_{i = 1}^N x_i^2 -  \frac{1}{N}\bigg( \sum\limits_{i = 1}^N x_i \bigg)^2  \Bigg] + k^2 \bar{x}_{\mathcal{U}}^2
\\ &amp; =    \dfrac{k}{N} (N-k) \dfrac{1}{N-1}\sum\limits_{i = 1}^N \Big( x_i - \dfrac{1}{N}\sum_{j = 1}^N x_j\Big)^2  + k^2 \bar{x}_{\mathcal{U}}^2
\\ &amp; =  k  \dfrac{(N- k)}{N} \dfrac{1}{N-1}\sum\limits_{i = 1}^N \Big( x_i - \bar{x}_{\mathcal{U}}\Big)^2  + k^2 \bar{x}_{\mathcal{U}}^2
\\ &amp; = k \dfrac{(N- k)}{N}  s^2_{\mathcal{U}} + k^2 \bar{x}_{\mathcal{U}}^2
\end{align*}\]</span></p>
<p>por lo cual si sustituimos en la ecuación anterior:
%<span class="math display">\[\begin{equation}\nonumber
\begin{aligned}
 \text{Var}\big[ \hat{t}_{\text{Alt}} \big] &amp;  = \sum\limits_{k = 1}^N \dfrac{N^2}{k^2} \mathbb{E}\bigg[ \Big( \sum\limits_{i = 1}^N x_i \mathbb{I}_{\mathcal{S}}(x_i)\Big)^2 \bigg| n(\mathcal{S}) = k\bigg]\cdot \mathbb{P}\big(n(\mathcal{S}) = k\big) - \Big( t \cdot \big( 1 - (1 - \pi)^N\big)\Big)^2
 \\ &amp; = \sum\limits_{k = 1}^N \dfrac{N^2}{k^2}  \Big[ k \dfrac{(N- k)}{N}  s^2_{\mathcal{U}} + k^2 \bar{x}_{\mathcal{U}}^2 \Big] \cdot \binom{N}{k} \pi^k (1 - \pi)^{N-k} - \Big( t \cdot \big( 1 - (1 - \pi)^N\big)\Big)^2
 \\ &amp; = N^2 \sum\limits_{k = 1}^N   \Big[ \dfrac{(N- k)}{k N}  s^2_{\mathcal{U}} + \bar{x}_{\mathcal{U}}^2 \Big] \cdot \binom{N}{k} \pi^k (1 - \pi)^{N-k} - t^2 \cdot \big( 1 - (1 - \pi)^{2N}\big)
 \\ &amp; = N^2 s^2_{\mathcal{U}} \sum\limits_{k = 1}^N   \Big( \frac{1}{k} - \frac{1}{N} \Big)  \binom{N}{k} \pi^k (1 - \pi)^{N-k} + N^2  \bar{x}_{\mathcal{U}}^2 \sum\limits_{k = 1}^N  \binom{N}{k} \pi^k (1 - \pi)^{N-k}
 \\ &amp; \qquad -  N^2 \bar{x}_{\mathcal{U}}^2\big( 1 - (1 - \pi)^{2N}\big)
 \\ &amp; =  N^2 s^2_{\mathcal{U}} \sum\limits_{k = 1}^N  \frac{1}{k}  \binom{N}{k} \pi^k (1 - \pi)^{N-k}  - \frac{1}{N} \big( 1 - (1 - p)^N \big) + 
 \\ &amp; \qquad \big( 1 - (1 - p)^N \big) N^2\bar{x}_{\mathcal{U}}^2\Big( 1 - \big( 1 - (1 - p)^N \big) \Big) 
 \\ &amp; = N^2[ H(N,\pi) s^2_{\mathcal{U}} + (1 - p)^N\big( 1 - (1 - p)^N \big) \bar{x}_{\mathcal{U}}^2 ]
\end{aligned}
\end{equation}\]</span></p>
<p>En nuestro caso para elegir el mejor estimador entre <span class="math inline">\(\hat{t}\)</span> y <span class="math inline">\(\hat{t}_{\text{alt}}\)</span> se calculan las varianzas de ambos. Una posible elección es aquél que tiene menos varianza (podría estar más cercano al valor dado que el sesgo de <span class="math inline">\(\hat{t}_{\text{alt}}\)</span> es pequeñísimo<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>). Se puede demostrar (ver Särndal) que en general
<span class="math display">\[
\text{Var}\big[ \hat{t}_{\text{Alt}} \big] \ll \text{Var}\big[ \hat{t} \big]
\]</span>
Y usualmente se prefiere el estimador <span class="math inline">\(\hat{t}_{\text{Alt}}\)</span>.</p>
</div>
</div>
<div id="ejemplo-resumen-aduana" class="section level2" number="4.13">
<h2><span class="header-section-number">4.13</span> Ejemplo Resumen: Aduana</h2>
<p>Se sabe que de manera diaria fluyen por un punto de la aduana 1000 cargamentos. Cada cargamento que entra debe ser analizado para buscar contrabando con probabilidad <span class="math inline">\(p\)</span> (y con probabilidad <span class="math inline">\((1 - p)\)</span> se deja pasar sin mayor análisis). Determina la probabilidad <span class="math inline">\(p\)</span> si se desea estimar el total de cargamentos con contrabando que pasan por la aduana y, a la vez, se busca que el <span class="math inline">\(75\%\)</span> de las ocasiones no se analicen más de <span class="math inline">\(200\)</span> cargamentos.</p>
<p>Para encontrar la probabilidad <span class="math inline">\(p\)</span> (correspondiente al <span class="math inline">\(\pi\)</span>) recordamos que el tamaño de la muestra <span class="math inline">\(n\)</span> tiene una distribución Binomial:
<span class="math display">\[
n(\mathcal{S}) \sim \text{Binomial}(1000,p)
\]</span>
Buscamos entonces una <span class="math inline">\(p\)</span> tal que
<span class="math display">\[
\mathbb{P}\big( B \leq 200) = 0.75 \quad \text{ donde } B \sim \text{Binomial}(1000,p)
\]</span>
En particular, notamos que del lado izquierdo tenemos a la función de distribución acumulada <span class="math inline">\(F_B(200) = \mathbb{P}(B\leq 200)\)</span> la cual depende (de manera implícita) de <span class="math inline">\(p\)</span>. ¡Hagamos explícita la dependencia de los parámetros <span class="math inline">\(p\)</span> y <span class="math inline">\(N\)</span>:
<span class="math display">\[
F_B(200; 1000, p) = 0.75 \quad \text{ donde } B \sim \text{Binomial}(1000,p)
\]</span>
Podemos graficar la función de distribución acumulada como función de <span class="math inline">\(p\)</span>:</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="muestreo-aleatorio-simple.html#cb183-1" aria-hidden="true" tabindex="-1"></a>p.val <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="fl">0.5</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb183-2"><a href="muestreo-aleatorio-simple.html#cb183-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb183-3"><a href="muestreo-aleatorio-simple.html#cb183-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> p.val, <span class="at">y =</span> <span class="fu">pbinom</span>(<span class="dv">200</span>, <span class="dv">1000</span>, p.val))) <span class="sc">+</span></span>
<span id="cb183-4"><a href="muestreo-aleatorio-simple.html#cb183-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb183-5"><a href="muestreo-aleatorio-simple.html#cb183-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb183-6"><a href="muestreo-aleatorio-simple.html#cb183-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;p&quot;</span>,</span>
<span id="cb183-7"><a href="muestreo-aleatorio-simple.html#cb183-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;F(200;1000,p)&quot;</span>,</span>
<span id="cb183-8"><a href="muestreo-aleatorio-simple.html#cb183-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Función de distribución acumulada como función de p&quot;</span>,</span>
<span id="cb183-9"><a href="muestreo-aleatorio-simple.html#cb183-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">subtitle =</span> <span class="st">&quot;Modelo Binomial(1000,p) evaluado en x = 200&quot;</span></span>
<span id="cb183-10"><a href="muestreo-aleatorio-simple.html#cb183-10" aria-hidden="true" tabindex="-1"></a>  ) </span></code></pre></div>
<p><img src="Introduccion_a_Muestreo_files/figure-html/unnamed-chunk-165-1.png" width="672" />
Notamos entonces que lo que necesitamos es hallar la <span class="math inline">\(p\)</span> donde la función de distribución acumulada (como función de <span class="math inline">\(p\)</span>) toca al <span class="math inline">\(0.75\)</span>. Para ello, como no podemos despejar, utilizamos un método numérico a través de <code>uniroot</code> para encontrar el <span class="math inline">\(0\)</span> de la función <span class="math inline">\(g(p) = F_B(200; 1000, p) - 0.75\)</span> (pues la <span class="math inline">\(p^*\)</span> tal que <span class="math inline">\(g(p^*)=0\)</span> es la respuesta):</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="muestreo-aleatorio-simple.html#cb184-1" aria-hidden="true" tabindex="-1"></a>g.fun <span class="ot">&lt;-</span> <span class="cf">function</span>(p){<span class="fu">pbinom</span>(<span class="dv">200</span>, <span class="dv">1000</span>, p) <span class="sc">-</span> <span class="fl">0.75</span>}</span>
<span id="cb184-2"><a href="muestreo-aleatorio-simple.html#cb184-2" aria-hidden="true" tabindex="-1"></a>raiz  <span class="ot">&lt;-</span> <span class="fu">uniroot</span>(g.fun, <span class="at">lower =</span> <span class="dv">0</span>, <span class="at">upper =</span> <span class="fl">0.5</span>)</span>
<span id="cb184-3"><a href="muestreo-aleatorio-simple.html#cb184-3" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;El valor de p es &quot;</span>, raiz<span class="sc">$</span>root))</span></code></pre></div>
<pre><code>## [1] &quot;El valor de p es 0.192159774829166&quot;</code></pre>
<p>De donde obtenemos el <span class="math inline">\(p\)</span> necesario.</p>
</div>
<div id="muestreo-aleatorio-simple-con-reemplazo-mascr" class="section level2" number="4.14">
<h2><span class="header-section-number">4.14</span> Muestreo Aleatorio Simple con Reemplazo (MAS/cR)</h2>
<p>El muestreo aleatorio simple con reemplazo es idéntico al muestreo aleatorio sin reemplazo <em>pero</em> en este caso no se extrae un elemento de la muestra sino que se permite que se seleccione múltiples veces. En cada selección hay una probabilidad <span class="math inline">\(1/N\)</span> de que un individuo de la población sea seleccionado. Cada selección es independiente de la pasada. Aquí consideraremos un universo de tamaño constante <span class="math inline">\(N\in\mathbb{R}\)</span> dado por <span class="math inline">\(U =(x_1, x_2, \dots, x_N)^T\)</span> y las variables aleatorias <span class="math inline">\(N_k\)</span> que denotan la cantidad de veces que <span class="math inline">\(x_k\)</span> fue seleccionado para incluirse en la muestra<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>. El orden en el que fueron seleccionados los elementos no importa.</p>
<p>En el caso de muestreo aleatorio simple con reemplazo se fija un tamaño de muestra <span class="math inline">\(m\)</span> y hay por tanto <span class="math inline">\(N^m\)</span> muestras posibles. Cada una de las muestras sigue la siguiente función de probabilidad uniforme:
<span class="math display">\[
\mathbb{P}(\mathcal{S} = S) = \begin{cases}
\frac{1}{N^m} &amp; \text{ si } \#S = m\\
0 &amp; \text{ en otro caso}
\end{cases}
\]</span>
Dado un elemento <span class="math inline">\(x_k\)</span> la probabilidad de que dicho <span class="math inline">\(x_k\)</span> aparezca <span class="math inline">\(r\)</span> veces en la muestra de tamaño <span class="math inline">\(m\)</span> está dada por:
<span class="math display">\[
\binom{m}{r}\Big(\frac{1}{N}\Big)^r \Big( 1 - \frac{1}{N}\Big)^{m - r}
\]</span>
En particular se tiene que la probabilidad de que <span class="math inline">\(x_k\)</span> no esté en la muestra es:
<span class="math display">\[
\Big( 1 - \frac{1}{N}\Big)^m
\]</span>
o bien de que esté en la muestra:
<span class="math display">\[
\pi_k = 1 - \Big( 1 - \frac{1}{N}\Big)^m
\]</span>
lo cual se calcula por el complemento. Por otro lado, la probabilidad conjunta <span class="math inline">\(\pi_{k,l}\)</span> de que <span class="math inline">\(x_k\)</span> y <span class="math inline">\(x_l\)</span> estén en la muestra se puede computar usando inclusión exclusión:</p>
<p><span class="math display">\[
\pi_{k,l} = 1 - \underbrace{\Big( 1 - \frac{1}{N}\Big)^m}_{\text{No está }x_k} - \underbrace{\Big( 1 - \frac{1}{N}\Big)^m}_{\text{No está }x_l} + \underbrace{\Big( 1 - \frac{2}{N}\Big)^m}_{\text{No está ni }x_k\text{ ni }x_l}
\]</span></p>
<p>En <code>R</code> puedes obtener un muestreo aleatorio simple con reemplazo cambiando en <code>sample</code> el <code>replace</code>:</p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="muestreo-aleatorio-simple.html#cb186-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sample</span>(<span class="fu">c</span>(<span class="st">&quot;A&quot;</span>,<span class="st">&quot;B&quot;</span>,<span class="st">&quot;C&quot;</span>), <span class="dv">10</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>##  [1] &quot;B&quot; &quot;A&quot; &quot;C&quot; &quot;A&quot; &quot;C&quot; &quot;B&quot; &quot;B&quot; &quot;A&quot; &quot;A&quot; &quot;C&quot;</code></pre>
<p>Una observación es bastante relevante aquí:
<span class="math display">\[\begin{align*}
\pi_k &amp; = 1 - (1 - \frac{1}{N})^m = 1 - \sum\limits_{j = 0}^m\binom{m}{j}\big( - \frac{1}{N})^{m-j}
\\ &amp; = 1 - \bigg[ \sum\limits_{j = 0}^{m-2}\binom{m}{j}\Big(-\frac{1}{N}\Big)^{m-j} - \frac{m}{N} + 1\bigg] 
\\ &amp; = \frac{m}{N} - \sum\limits_{j = 0}^{m-2}\binom{m}{j}\Big(-\frac{1}{N}\Big)^{m-j} \\ &amp; = \frac{m}{N} + \mathcal{O}\Bigg( \frac{m^2}{N^2}\Bigg)
\end{align*}\]</span></p>
<p>donde <span class="math inline">\(\mathcal{O}\Bigg( \frac{m^2}{N^2}\Bigg)\)</span> es notación que implica que una función <span class="math inline">\(f(n)\)</span> es de orden <span class="math inline">\(g(n)\)</span> (denotado <span class="math inline">\(f(n) = \mathcal{O}\big(g(n)\big)\)</span>) si y sólo si existe <span class="math inline">\(M\)</span> tal que para cualquier <span class="math inline">\(n \in\mathbb{N}\)</span>tal que <span class="math inline">\(|f(n)|/g(n)\leq M\)</span>. Escrito con palabras en este caso esto significa que si <span class="math inline">\(m/N\)</span> es pequeño entonces <span class="math inline">\(\frac{m^2}{N^2}\)</span> es caso <span class="math inline">\(0\)</span> y entonces muestrear con reemplazo es casi lo mismo que muestrear sin reemplazo (lo cual tiene sentido: si tu población es muy grande <span class="math inline">\(N \gg 0\)</span> entonces está bien difícil que vuelvas a capturar a uno en tu encuesta y por tanto es casi lo mismo muestrear <strong>con</strong> que <strong>sin</strong> reemplazo en términos prácticos).</p>
<p>Para el análisis del muestreo aleatorio simple con reemplazo podemos generalizar la idea de variables indicadoras. Como en este tipo de muestreo pueden aparecer <strong>varias veces</strong> los mismos valores <span class="math inline">\(x_i\)</span> utilizaremos unas variables <span class="math inline">\(\mathbb{A}_{\mathcal{S}}(x_i)\)</span> para denotar cuántas veces aparece el valor <span class="math inline">\(x_i\)</span> en la muestra aleatoria <span class="math inline">\(\mathcal{S}\)</span>; es decir:
<span class="math display">\[
\mathbb{A}_{\mathcal{S}}(x_i) = \begin{cases}
0 &amp; \text{ si } x_i \not\in \mathcal{S} \\
k &amp; \text{ si } x_i  \in \mathcal{S} \quad k \textrm{ veces}
\end{cases}
\]</span>
Observamos que la distribución de las <span class="math inline">\(a_i\)</span> es multinomial:
<span class="math display">\[\begin{equation}
\begin{aligned}
\mathbb{P}\Big(\mathbb{A}_{\mathcal{S}}(x_1) = a_1,\mathbb{A}_{\mathcal{S}}(x_2) = a_2, \dots,\mathbb{A}_{\mathcal{S}}(x_N) =  a_N\Big) &amp; = 
\\ &amp; = \binom{m}{a_1}\Big(\frac{1}{N}\Big)^{a_1} \cdot \binom{m - a_1}{a_2}\Big(\frac{1}{N}\Big)^{a_2}\cdot\binom{m - a_1 - a_2}{a_3}\Big(\frac{1}{N}\Big)^{a_3} \cdots \binom{m - \sum_{l = 1}^{N-1} a_l}{a_N}\Big(\frac{1}{N}\Big)^{a_N} \\
&amp; = \dfrac{m!}{a_1!a_2!\cdots a_N!}\dfrac{1}{(m - \sum_{l= 1}^N a_l)!} \Big(\frac{1}{N}\Big)^{\sum_{l= 1}^N a_l} \\
&amp; = \dfrac{m!}{a_1!a_2!\cdots a_N!} \Big(\frac{1}{N}\Big)^{m} \\
\end{aligned}
\end{equation}\]</span></p>
<p>donde tomamos que <span class="math inline">\(\sum\limits_{l = 1}^N a_l = m\)</span>. Al ser una distribución multinomial se tienen las marginales:
<span class="math display">\[
\mathbb{E}\Big[\mathbb{A}_{\mathcal{S}}(x_i) \Big] = \dfrac{m}{N}
\]</span>
y por otro lado:</p>
<p><span class="math display">\[
\textrm{Var}\Big[\mathbb{A}_{\mathcal{S}}(x_i) \Big] = \dfrac{m(N-1)}{N^2}
\]</span>
con:
<span class="math display">\[
\textrm{Cov}\Big[\mathbb{A}_{\mathcal{S}}(x_i) ,\mathbb{A}_{\mathcal{S}}(x_j) \Big] = -\dfrac{m}{N^2}
\]</span></p>
<p>Un estimador de la media es la suma de los <span class="math inline">\(N\)</span> valores únicos en el universo (<em>i.e.</em> <span class="math inline">\(N = \# \mathcal{U}\)</span>) multiplicados por la cantidad de veces que aparecen en la muestra (las <span class="math inline">\(\mathbb{A}_{\mathcal{S}}(x_i)\)</span>):</p>
<p><span class="math display">\[
\bar{x}_{\mathcal{S}} = \frac{1}{m} \sum\limits_{i = 1}^m x_i = \frac{1}{m} \sum\limits_{i = 1}^N x_i\cdot \mathbb{A}_{\mathcal{S}}(x_i)
\]</span>
en este caso se tiene que el estimador es insesgado:
<span class="math display">\[
\mathbb{E}\Big[\bar{x}_{\mathcal{S}}\Big] =  \frac{1}{m} \sum\limits_{i = 1}^N x_i\cdot \mathbb{E}\Big[ \mathbb{A}_{\mathcal{S}}(x_i)\Big] = \frac{1}{m}\sum\limits_{i = 1}^N x_i \dfrac{m}{N}= \bar{x}_{\mathcal{U}} 
\]</span>
Su varianza está dada por lo siguiente:
<span class="math display">\[\begin{align*}
\textrm{Var}\Big[\bar{x}_{\mathcal{S}}\Big] &amp; = \frac{1}{m^2}\bigg( \sum\limits_{i = 1}^N x_i^2\text{Var}\Big[ \mathbb{A}_{\mathcal{S}}(x_i)\Big] + \sum\limits_{i= 1}^N \sum\limits_{\substack{j = 1 \\ j \neq i}}^N \textrm{Cov}\Big[\mathbb{A}_{\mathcal{S}}(x_i) ,\mathbb{A}_{\mathcal{S}}(x_j) \Big]  x_i x_k\bigg) 
\\ &amp; = \frac{1}{m^2}\bigg( \frac{m(N-1)}{N^2} \sum\limits_{i = 1}^N x_i^2  - \frac{m}{N^2} \sum\limits_{i= 1}^N \sum\limits_{\substack{j = 1 \\ j \neq i}}^N x_i x_k\bigg)
\\ &amp; = \frac{N-1}{mN}\bigg( \frac{1}{N} \sum\limits_{i = 1}^N x_i^2  - \frac{1}{N} \sum\limits_{i= 1}^N \sum\limits_{\substack{j = 1 \\ j \neq i}}^N x_i x_k\bigg)
\\ &amp; = \frac{N-1}{mN}\Bigg( \frac{1}{N-1} \sum\limits_{i = 1}^N \bigg[ \frac{N-1}{N} x_i^2  - \frac{1}{N} x_i \sum\limits_{\substack{j = 1 \\ j \neq i}}^N x_k\bigg]\Bigg)
\\ &amp; = \frac{N-1}{mN}\Bigg( \frac{1}{N-1} \sum\limits_{i = 1}^N \bigg[  x_i^2  - \frac{1}{N} x_i \sum\limits_{j = 1}^N x_k\bigg]\Bigg)
\\ &amp; = \frac{N-1}{mN}\Bigg( \frac{1}{N-1} \bigg[  \sum\limits_{i = 1}^N  x_i^2  - \frac{1}{N}\sum\limits_{i = 1}^N  x_i \sum\limits_{j = 1}^N x_k\bigg]\Bigg)
\\ &amp; = \frac{N-1}{mN}\Bigg( \frac{1}{N-1} \bigg[  \sum\limits_{i = 1}^N  x_i^2  - \Big(\sum\limits_{i = 1}^N  x_i\Big)^2\bigg]\Bigg)
\\ &amp; = \frac{N-1}{mN} s^2_{\mathcal{U}}
\end{align*}\]</span>
Donde la última igualdad se sigue de la misma que se hizo con Bernoulli.
Un estimador de la varianza es:
<span class="math display">\[
\widehat{\textrm{Var}}\Big[\bar{x}_{\mathcal{S}}\Big] = \frac{N-1}{mN} s^2_{\mathcal{S}}
\]</span></p>
</div>
<div id="ejemplo-resumen-proporción-de-trabajadores-enfermos-con-o-sin-reemplazo" class="section level2" number="4.15">
<h2><span class="header-section-number">4.15</span> Ejemplo Resumen: Proporción de trabajadores enfermos con o sin reemplazo</h2>
<p>Nos interesa estimar la proporción de trabajadores <span class="math inline">\(P\)</span> afectados por una enfermedad en su trabajo en un negocio que emplea a 1500 personas. Además sabemos que en población general <span class="math inline">\(3\)</span> de cada <span class="math inline">\(10\)</span> personas enferman. Para ello obtenemos una muestra aleatoria con reemplazo donde además buscamos un intervalo de confianza al <span class="math inline">\(0.95\)</span> con un error a lo más de <span class="math inline">\(0.01\)</span>.</p>
<p>Proponemos el estimador de la proporción dado por:
<span class="math display">\[
\bar{x}_m = \frac{1}{m}\sum\limits_{i = 1}^m x_i
\]</span>
donde <span class="math inline">\(x_i = 1\)</span> si el trabajador tiene la enfermedad (<span class="math inline">\(0\)</span> en otro caso). En este caso tenemos que la varianza está dada por:</p>
<p><span class="math display">\[
\widehat{\text{Var}}(\bar{x}_m) =\frac{N-1}{N\cdot m} s^2_{\mathcal{U}}
\]</span>
Tenemos entonces que el error es:
<span class="math display">\[
0.01 \geq \epsilon = Z_{1 - \alpha/2} \sqrt{\widehat{\text{Var}}(\bar{x}_m)}
\]</span>
de donde se tiene:
<span class="math display">\[
\dfrac{0.01^2}{Z_{1 - \alpha/2}^2} \geq  \frac{N-1}{N\cdot m} s^2_{\mathcal{U}}
\]</span>
o de manera equivalente:
<span class="math display">\[
\dfrac{0.01^2}{Z_{1 - \alpha/2}^2\cdot s^2_{\mathcal{U}}}\dfrac{N}{N-1} \geq \frac{1}{m} 
\]</span></p>
<p>de donde se sigue que:</p>
<p><span class="math display">\[
\Big(\dfrac{Z_{1 - \alpha/2}s_{\mathcal{U}}}{0.01}\Big)^2\cdot \dfrac{N-1}{N} \leq  m
\]</span>
donde finalmente como sabemos que <span class="math inline">\(3\)</span> de cada <span class="math inline">\(10\)</span> personas lo padecen notamos que la probabilidad en el mundo real de obtener a una persona que lo tenga es <span class="math inline">\(p = \frac{1}{3}\)</span> y podemos modelar la variable <code>estar enfermo</code> mediante una Bernoulli y por tanto una buena aproximación a <span class="math inline">\(s^2_{\mathcal{U}}\)</span> es:
<span class="math display">\[
s^2_{\mathcal{U}} \approx \underbrace{\frac{1}{3}\Big( 1 - \frac{1}{3}\Big)}_{p\cdot(1-p)}
\]</span>
donde sustituimos:
<span class="math display">\[
\frac{1}{3}(1 - \frac{1}{3})\cdot \Bigg(\dfrac{Z_{1 - \alpha/2}}{0.01}\Bigg)^2\cdot \dfrac{N-1}{N} \leq  m
\]</span></p>
<p>Concluimos que:</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="muestreo-aleatorio-simple.html#cb188-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">ceiling</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">3</span><span class="sc">*</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">3</span>)<span class="sc">*</span>(<span class="fu">qnorm</span>(<span class="fl">0.975</span>)<span class="sc">/</span><span class="fl">0.01</span>)<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>(<span class="dv">1500</span> <span class="sc">-</span> <span class="dv">1</span>)<span class="sc">/</span><span class="dv">1500</span>)</span>
<span id="cb188-2"><a href="muestreo-aleatorio-simple.html#cb188-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;m = &quot;</span>, m))</span></code></pre></div>
<pre><code>## [1] &quot;m = 8531&quot;</code></pre>
<p>Lo cual es un sinsentido estadístico: ¿para qué muestrear más de lo que se tiene en población total? En este ejercicio lo mejor sería hacer un censo.</p>
<blockquote>
<p><strong>Nota</strong> Si se repite el ejercicio con muestreo aleatorio simple sin reemplazo acabamos con <span class="math inline">\(n \approx 1300\)</span> lo cual sí tiene sentido como muestreo. En general la <span class="math inline">\(m\)</span> de muestreo con reemplazo es mayor que la <span class="math inline">\(n\)</span> de sin reemplazo (las varianzas son mayores). Intuitivamente esto tiene sentido pues si estás muestrando con la posibilidad de repetidos a fuerza necesitas muestrear más para obtener la misma cantidad de datos únicos.</p>
</blockquote>
</div>
<div id="ejemplo-resumen-captura-recaptura-con-reemplazo" class="section level2" number="4.16">
<h2><span class="header-section-number">4.16</span> Ejemplo Resumen: Captura-Recaptura con reemplazo</h2>
<p>Se realiza un estudio para determinar la cantidad de ratas en la CDMX. Para ello se pone una trampa en algún lugar aleatorio de la ciudad. Si se atrapa una rata se le marca y se le deja ir. Si para <span class="math inline">\(50\)</span> ratas capturadas, contamos <span class="math inline">\(42\)</span> marcadas determina el número de ratas en la isla suponiendo que las <span class="math inline">\(50\)</span> fueron con reemplazo.</p>
<p>Para ello denotamos <span class="math inline">\(p_N(r)\)</span> a la probabilidad de tener <span class="math inline">\(r\)</span> ratas distintas en <span class="math inline">\(m\)</span> intentos con reemplazos (<span class="math inline">\(m = 50\)</span> es determinado por nosotros y no es aleatorio) en una población de tamaño desconocido <span class="math inline">\(N\)</span>. Una vez que se fijan las <span class="math inline">\(r\)</span> ratas que van a salir hay <span class="math inline">\(\binom{N}{r}\)</span> formas de elegirlas. Entonces:
<span class="math display">\[
p_N(r) = \dfrac{N!}{r!(N-r)!} q_N(r)
\]</span>
donde <span class="math inline">\(q_N(r)\)</span> es la probabilidad de obtener <span class="math inline">\(r\)</span> distintas ratas en <span class="math inline">\(m\)</span> intentos con reemplazo. Fijado el número de ratas, el universo <span class="math inline">\(\Omega\)</span> de posibilidades se forma por el grupo de mapeos de <span class="math inline">\(\{1,2,\dots, m\}\to\{1,2,\dots, N\}\)</span> (todas las formas de haber acomodado las ratas). Tenemos <span class="math inline">\(m \geq r\)</span> y de hecho:
<span class="math display">\[
q_N(r) = \sum_{\omega\in\text{Fav}} p(\omega)
\]</span></p>
<p>dpmde <span class="math inline">\(p(\omega)\)</span> es la probabilidad de obtener un mapeo favorable (<span class="math inline">\(w\in\text{Fav}\)</span>). Tenemos que <span class="math inline">\(p(\omega) = N^{-m}\)</span> para toda <span class="math inline">\(\omega\)</span>. La cantidad de casos favrables es lo mismo que preguntarse por la cantidad de mapeos suprayectivos del conjunto <span class="math inline">\(\{1,2,\dots, m\}\)</span> en <span class="math inline">\(\{1,2,\dots, r\}\)</span> lo cual está dado por <span class="math inline">\(r!\)</span> multiplicado por el número de Stirling de segundo tipo <span class="math inline">\(\mathfrak{s}_m^{(r)}\)</span>:
<span class="math display">\[
\mathfrak{s}_m^{(r)} = \frac{1}{r!} \sum\limits_{i = 1}^r \binom{r}{i} i^m (-1)^{r-i}
\]</span>
donde <span class="math inline">\(\mathfrak{s}_m^{(r)}\)</span> es la forma de encontrar de un grupo de <span class="math inline">\(m\)</span> elementos <span class="math inline">\(r\)</span> partes no vacías. Tenemos entonces:
<span class="math display">\[
p_N(r) = \dfrac{N!}{(N-r)!N^m} \mathfrak{s}_m^{(r)} \qquad \text{para} r = 1,2,\dots,\min\{m,N\}.
\]</span>
Lo que vamos a hacer es pensar que la <span class="math inline">\(N\)</span> que generó los datos es la <span class="math inline">\(N\)</span> máxima (<em>criterio de máxima verosimilitud</em>) y entonces lo que hay que maximizar es:
<span class="math display">\[
 \dfrac{N!}{(N-r)!N^m} = \dfrac{\prod_{i = 0}^{r -1} (N-i)}{N^m}
\]</span></p>
<p>Hay dos formas de hacer esta maximización: enlistando todas las <span class="math inline">\(N\)</span> y <span class="math inline">\(r\)</span> para mi población o bien derivando el logaritmo:
<span class="math display">\[
\dfrac{d}{dN}  \ln \Bigg( \dfrac{\prod_{i = 0}^{r -1} (N-i)}{N^m}\Bigg) = \dfrac{d}{dN}\Bigg[ \sum\limits_{i = 0}^{r-1} \ln(N-i)  - m\ln (N) \Bigg] = \sum\limits_{i = 0}^{r-1} \dfrac{1}{N-i} - \dfrac{m}{N} = 0
\]</span>
de donde se sigue que:
<span class="math display">\[
\sum\limits_{i = 0}^{r-1} \dfrac{N}{N-i} = m
\]</span>
La cual es una ecuación no lineal que se puede resolver mediante <code>uniroot</code> como la pasada.</p>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="muestreo-aleatorio-simple.html#cb190-1" aria-hidden="true" tabindex="-1"></a>m.val <span class="ot">&lt;-</span> <span class="cf">function</span>(N){</span>
<span id="cb190-2"><a href="muestreo-aleatorio-simple.html#cb190-2" aria-hidden="true" tabindex="-1"></a>  r <span class="ot">&lt;-</span> <span class="dv">42</span></span>
<span id="cb190-3"><a href="muestreo-aleatorio-simple.html#cb190-3" aria-hidden="true" tabindex="-1"></a>  m <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb190-4"><a href="muestreo-aleatorio-simple.html#cb190-4" aria-hidden="true" tabindex="-1"></a>  suma <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb190-5"><a href="muestreo-aleatorio-simple.html#cb190-5" aria-hidden="true" tabindex="-1"></a>  N    <span class="ot">&lt;-</span> <span class="fu">floor</span>(N) <span class="co">#Esto es para garantizar sale un entero; no es la mejor opción de optimizar</span></span>
<span id="cb190-6"><a href="muestreo-aleatorio-simple.html#cb190-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>r){</span>
<span id="cb190-7"><a href="muestreo-aleatorio-simple.html#cb190-7" aria-hidden="true" tabindex="-1"></a>    suma <span class="ot">&lt;-</span> suma <span class="sc">+</span> N<span class="sc">/</span>(N <span class="sc">-</span> (i <span class="sc">-</span> <span class="dv">1</span>))</span>
<span id="cb190-8"><a href="muestreo-aleatorio-simple.html#cb190-8" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb190-9"><a href="muestreo-aleatorio-simple.html#cb190-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(suma <span class="sc">-</span> m)</span>
<span id="cb190-10"><a href="muestreo-aleatorio-simple.html#cb190-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb190-11"><a href="muestreo-aleatorio-simple.html#cb190-11" aria-hidden="true" tabindex="-1"></a><span class="fu">floor</span>(<span class="fu">uniroot</span>(m.val, <span class="at">lower =</span> <span class="dv">100</span>, <span class="at">upper =</span> <span class="dv">200</span>)<span class="sc">$</span>root)</span></code></pre></div>
<pre><code>## [1] 136</code></pre>
</div>
<div id="muestreo-aleatorio-simple-ponderado-masp" class="section level2" number="4.17">
<h2><span class="header-section-number">4.17</span> Muestreo Aleatorio Simple Ponderado (MAS/P)</h2>
<p>En el caso más general posible cada uno de los elementos <span class="math inline">\(x_k\)</span> tiene una probabilidad <span class="math inline">\(\pi_k\)</span> de aparecer en la muestra. Análogamente se tienen probabilidades conjuntas de la forma <span class="math inline">\(\pi_{k,l}\)</span> donde no necesariamente hay independencia. El estimador del total está dado por el estimador Horvitz Thompson:
<span class="math display">\[
\hat{t} = \sum\limits_{k = 1}^n \dfrac{x_k}{\pi_k}
\]</span>
donde su varianza y sus estimadores fueron ya calculados desde muestreo aleatorio simple. En el siguiente capítulo comenzaremos a variar mucho más las <span class="math inline">\(\pi_k\)</span> cuando entremos a muestreo estratificado. ¡Nos vemos pronto!</p>
<!--# Muestreo Aleatorio Simple Poisson (PO)
En un esquema de muestreo Poisson (PO) se tiene una población de tamaño $N\in\mathbb{N}$ (constante) la cual se enlista de manera ordenada $U = (x_1,x_2,\dots,x_N)^T$. Se recorre la lista de $1$ hasta $N$. Cada elemento de la población, se selecciona y se mide con probabilidad $\pi_k \in (0,1)$ para $k = 1,2,\dots, N$ a fin de generar una muestra $\mathcal{S} = (x_1, x_2, \dots, x_n)^T$ de tamaño aleatorio $n = n(\mathcal{S})$ (con $0 \leq n(\mathcal{S}) \leq N$). En el esquema Poisson se asume que $\mathbb{I}(x_k)$ es independiente de $\mathbb{I}(x_j)$ para $k\neq j$ pues corresponden a llegadas bajo un modelo _Poisson_. -->
</div>
<div id="ejercicios-1" class="section level2" number="4.18">
<h2><span class="header-section-number">4.18</span> Ejercicios</h2>
<ol style="list-style-type: decimal">
<li>Bajo muestreo aleatorio Bernoulli se propone el siguiente estimador para el total:
<span class="math display">\[
\hat{t}_{\text{BE}} = \dfrac{N}{\mathbb{E}[n(\mathcal{S})]} \sum\limits_{i = 1}^{n(\mathcal{S})} x_i
\]</span></li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Demuestra que es insesgado</li>
<li>Obtén su varianza</li>
<li>Para el ejemplo del profesor con los exámenes ¿es <span class="math inline">\(\hat{t}_{\text{BE}}\)</span> una mejor opción que <span class="math inline">\(\hat{t}_{\text{Alt}}\)</span> ? Justifica calculando en <code>R</code> todos los posibles casos (desde que <span class="math inline">\(0\)</span> pasan hasta que los <span class="math inline">\(600\)</span> pasan el examen) y analizando cuántas de esas veces la varianza de <span class="math inline">\(\hat{t}_{\text{Alt}}\)</span> es menor que la de <span class="math inline">\(\hat{t}_{\text{BE}}\)</span>.</li>
</ol>
<ol start="2" style="list-style-type: decimal">
<li>Se tiene una muestra aleatoria simple con reemplazo y se calcula la varianza del estimador de la media como sigue:</li>
</ol>
<p><span class="math display">\[
\text{Var}(\bar{x}_{\mathcal{S}}) = \frac{1}{n}\sum\limits_{i = 1}^N x_i^2 \cdot \text{Var}(\mathbb{I}_{\mathcal{S}}(x_i)) =  \frac{1}{n}\sum\limits_{i = 1}^N x_i^2 \cdot \frac{1}{N}\Big(1 - \frac{1}{N}\Big) =  \frac{1}{N\cdot n}\sum\limits_{i = 1}^N x_i^2 \cdot \underbrace{\Big(1 - \frac{1}{N}\Big)}_{(N-1)/N}  = \frac{N-1}{N\cdot n} s^2_{\mathcal{U}}
\]</span></p>
<p>El resultado está bien pero el razonamiento tiene un error. Identifica los errores.</p>
<ol start="3" style="list-style-type: decimal">
<li><p>Sea <span class="math inline">\(\mathcal{S}\)</span> una muestra bajo diseño Bernoulli con parámetro <span class="math inline">\(\pi\)</span>. Sea <span class="math inline">\(n(\mathcal{S}) = \#\mathcal{S}\)</span> la variable aleatoria del tamaño de <span class="math inline">\(\mathcal{S}\)</span>. Demuestra que condicional en que <span class="math inline">\(n(\mathcal{S}) = n\)</span> la probabilidad de que <span class="math inline">\(\mathcal{S} = S\)</span> es la misma que bajo muestreo aleatorio simple.</p></li>
<li><p>Encuentra un estimador insesgado de la varianza poblacional bajo muestreo aleatorio simple sin reemplazo. <em>Hint</em> Demuestra que:
<span class="math display">\[
\dfrac{1}{N} \sum\limits_{k = 1}^N (x_k - \bar{x}_{\mathcal{U}})^2 =\dfrac{1}{2N^2} \sum\limits_{k = 1}^N\sum\limits_{\substack{l = 1 \\ l \neq k}}^N (x_k - \bar{x}_{l})^2  
\]</span></p></li>
<li><p>Considera la población de la tabla siguiente:</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
<p>Edad</p>
</th>
<th style="text-align:right;">
<p>Peso</p>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
<p>15</p>
</td>
<td style="text-align:right;">
<p>50</p>
</td>
</tr>
<tr>
<td style="text-align:right;">
<p>30</p>
</td>
<td style="text-align:right;">
<p>70</p>
</td>
</tr>
<tr>
<td style="text-align:right;">
<p>5</p>
</td>
<td style="text-align:right;">
<p>20</p>
</td>
</tr>
<tr>
<td style="text-align:right;">
<p>25</p>
</td>
<td style="text-align:right;">
<p>90</p>
</td>
</tr>
<tr>
<td style="text-align:right;">
<p>10</p>
</td>
<td style="text-align:right;">
<p>35</p>
</td>
</tr>
<tr>
<td style="text-align:right;">
<p>45</p>
</td>
<td style="text-align:right;">
<p>85</p>
</td>
</tr>
</tbody>
</table></li>
</ol>
<p>Se sabe que en una muestra aleatoria sin reemplazo que se obtenga de dicha población es dos veces más factible seleccionar a alguien de menos de <span class="math inline">\(20\)</span> años que a alguien de más de <span class="math inline">\(20\)</span> años.
a. Determina un estimador insesgado de la media de peso para dicha población.
b. Se obtiene la muestra aleatoria simple de peso dada por <span class="math inline">\(S = \{50, 35, 85 \}\)</span>. Determina la probabilidad de haber obtenido dicha muestra.
c. A partir de la muestra <span class="math inline">\(S\)</span> del inciso anterior, estima el peso total poblacional (la suma de los pesos) y da un intervalo asintótico de confianza de 75% para dicho total.</p>
<ol start="6" style="list-style-type: decimal">
<li><p>De una población de <span class="math inline">\(N = 1000\)</span> personas se entrevistaron a <span class="math inline">\(n = 100\)</span>. Se les preguntó el tipo de música que preferían; <span class="math inline">\(30\)</span> personas respondieron <em>reguetón</em>.</p>
<ol style="list-style-type: lower-alpha">
<li>Estima el total de personas de la población que prefieren .</li>
<li>Genera un intervalo de confianza asintótico de 50% para dicha estimación.</li>
</ol></li>
<li><p>Un estimador <span class="math inline">\(\hat{\theta}\)</span> de <span class="math inline">\(\theta\)</span> es Fisher consistente si cuando <span class="math inline">\(n = N\)</span> el estimador <span class="math inline">\(\hat{\theta} = \theta\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li>Demuestra que <span class="math inline">\(\bar{x}_{\mathcal{S}}\)</span> es Fisher consistente.</li>
<li>Da un ejemplo de un estimador que no sea Fisher consistente.</li>
</ol></li>
<li><p>¿Cuál de los siguientes diseñois de muestreo simple sin reemplazo tiene la mayor precisión para estimar la media poblacional? Suponiendo que todas las poblaciones tienen la misma varianza <span class="math inline">\(\sigma^2\)</span>:</p>
<ol style="list-style-type: lower-alpha">
<li>Tomar <span class="math inline">\(n = 400\)</span> de <span class="math inline">\(N = 4000\)</span></li>
<li>Tomar <span class="math inline">\(n = 300\)</span> de <span class="math inline">\(N = 4000\)</span></li>
<li>Tomar <span class="math inline">\(n = 3000\)</span> de <span class="math inline">\(N = 300,000,000\)</span></li>
</ol></li>
<li><p>En una ciudad hay <span class="math inline">\(10\)</span> millones de habitantes. Interesa saber el porcentaje de personas que utilizan bicicleta en dicha ciudad con un margen de error de <span class="math inline">\(4\)</span> puntos porcentuales y una confianza del <span class="math inline">\(75\%\)</span>. Determina el tamaño de muestra mínimo para hacer una encuesta de usuarios de bicicleta.</p></li>
<li><p>Bajo <strong>muestreo Bernoulli</strong>, define el total de la población como <span class="math inline">\(t = \sum_{i=1}^{N} x_i\)</span>. Un estimador del total es:
<span class="math display">\[
\hat{t}_1 = \dfrac{1}{p} \sum_{k=1}^{N} x_k \mathbb{I}_{\mathcal{S}}(x_k)
\]</span></p></li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Obtén <span class="math inline">\(\mathbb{E}[\hat{t}_1]\)</span></li>
<li>Demuestra que <span class="math inline">\(\textrm{Var}[\hat{t}_1] = \big(\frac{1}{p}-1 \big) \sum_{k=1}^{N} x_k^2\)</span>.</li>
<li>Obtén un estimador de <span class="math inline">\(\textrm{Var}[\hat{t}_1]\)</span>. ¿Es insesgado?</li>
</ol>
<ol start="10" style="list-style-type: decimal">
<li>Un estimador distinto del total de la población (bajo Bernoulli) está dado por:
<span class="math display">\[
\hat{t}_2 = N \bar{x} 
\]</span>
donde <span class="math inline">\(\bar{x} = \frac{1}{\# S} \sum\limits_{k=1}^{N} x_k \mathbb{I}_k\)</span>.</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Demuestra que <span class="math inline">\(\mathbb{E}[\hat{t}_2] = \big(1 - (1-p)^N\big) \cdot t\)</span> <em>Hint</em> Utiliza la propiedad de torre de proba 2: <span class="math inline">\(\mathbb{E}[X] = \mathbb{E}\big[\mathbb{E}[X| Y ]\big]\)</span> condicionando en el tamaño de la muestra.</li>
<li>Demuestra que <span class="math inline">\(\textrm{Var}[\hat{t}_2] = \frac{N^3}{N-1} \Big[ H \sigma^2 + (1-p)^N \Big( 1 - (1 - p)^N \Big) \bar{x}^2 \Big]\)</span> donde <span class="math inline">\(H = \sum_{k=1}^{N} \frac{1}{k} \binom{N}{k} p^k (1-p)^{N-k} - \big(1 - (1-p)^N\big)/N\)</span>.</li>
</ol>
<ol start="11" style="list-style-type: decimal">
<li><p>En muestreo aleatorio simple con reemplazo sea <span class="math inline">\(n_{\text{únicos}}\)</span> la cardinalidad del conjunto <span class="math inline">\(\{ x_k | x_k \in \mathcal{S}\}\)</span>; es decir, el número de valores únicos que se obtuvieron en dicho muestreo. Determina <span class="math inline">\(\mathbb{E}\big[n_{\text{únicos}}\big]\)</span></p></li>
<li><p>En un esquema de muestreo Poisson (PO) se tiene una población de tamaño <span class="math inline">\(N\in\mathbb{N}\)</span> (constante) la cual se enlista de manera ordenada <span class="math inline">\(U = \{x_1,x_2,\dots,x_N\}\)</span>. Se recorre la lista de <span class="math inline">\(1\)</span> hasta <span class="math inline">\(N\)</span>. Cada elemento de la población, se selecciona y se mide con probabilidad <span class="math inline">\(\pi_k \in (0,1)\)</span> para <span class="math inline">\(k = 1,2,\dots, N\)</span> a fin de generar una muestra <span class="math inline">\(S = \{x_1, x_2, \dots, x_n\}\)</span> de tamaño <span class="math inline">\(\# S\)</span> (con <span class="math inline">\(0 \leq \# S \leq N\)</span>) donde en este caso la cardinalidad de <span class="math inline">\(S\)</span>, <span class="math inline">\(\# S\)</span>, es una variable aleatoria. En este caso se asume que <span class="math inline">\(\mathbb{I}_k\)</span> es independiente de <span class="math inline">\(\mathbb{I}_j\)</span> para <span class="math inline">\(k\neq j\)</span>. Responde los siguientes incisos considerando que la muestra <span class="math inline">\(S\)</span> fue generada por un esquema Poisson.</p>
<ol style="list-style-type: lower-alpha">
<li>Demuestra que <span class="math inline">\(\mathbb{E}[\mathbb{I}_k] = \pi_k\)</span>. ¿Cuánto vale es <span class="math inline">\(\textrm{Cov}(\mathbb{I}_k, \mathbb{I}_j)\)</span>?</li>
<li>Demuestra <span class="math inline">\(\mathbb{E}[\# S ] = \sum_{k=1}^N \pi_k\)</span> y <span class="math inline">\(\textrm{Var}[\# S ] = \sum_{k=1}^N \pi_k (1 - \pi_k)\)</span>.</li>
<li>Define la media de la población como <span class="math inline">\(\mu_X = \frac{1}{N} \sum_{i=1}^{N} x_i\)</span>. Se propone un estimador de la media como:
<span class="math display">\[
\hat{\mu} =  \frac{1}{n} \sum_{k=1}^{n} \dfrac{x_k}{\pi_k} 
\]</span></li>
<li>Determina el sesgo de <span class="math inline">\(\hat{\mu}\)</span></li>
<li>¿Es <span class="math inline">\(\hat{\mu}\)</span> Fisher-consistente?</li>
<li>Obtén el error cuadrático medio de <span class="math inline">\(\hat{\mu}\)</span></li>
<li>Demuestra que <span class="math inline">\(\widehat{\textrm{Var}}[\hat{\mu}] = \frac{1}{N^2} \sum_{i \in S} (1 - \pi_i) \pi_i^{-2} x_i^2\)</span> es estimador insesgado de la varianza <span class="math inline">\(\textrm{Var}[\hat{\mu}]\)</span>.</li>
<li>Una aplicación de el muestreo Poisson es en estimación de cantidad de madera en un bosque. Los investigadores van al bosque y estiman a ojo de buen cubero el tamaño de un árbol en una de las categorías: , , . Una vez que a ojo se estimaron los tamaños se seleccionan los árboles pequeños con probabilidad <span class="math inline">\(p_1\)</span> para cada árbol, los medianos con probabilidad <span class="math inline">\(p_2\)</span> y los grandes con probabilidad <span class="math inline">\(p_3\)</span> (<span class="math inline">\(0 &lt; p_1 &lt; p_2 &lt; p_3 &lt; 1\)</span>). La selección de cada árbol es independiente de que otro haya sido seleccionado. Se busca estimar el total de madera midiendo los árboles (área de la base por altura). Supongamos que un bosque tiene <span class="math inline">\(N\)</span> árboles de los cuales <span class="math inline">\(N_1\)</span> son pequeños, <span class="math inline">\(N_2\)</span> son medianos y <span class="math inline">\(N_3\)</span> son grandes (<span class="math inline">\(\sum N_i = N\)</span>). Supongamos además que medir un árbol pequeño cuesta <span class="math inline">\(C_1\)</span>, un árbol mediano cuesta <span class="math inline">\(C_2\)</span> y un árbol grande cuesta <span class="math inline">\(C_3\)</span> (<span class="math inline">\(0 &lt; C_1 &lt; C_2 &lt; C_3\)</span>). Específicamente, supongamos que <span class="math inline">\(p_i = C_i / \sum_k C_k\)</span>. En promedio para <span class="math inline">\(N_1 = N_2 = 300\)</span> y <span class="math inline">\(N_3 = 400\)</span> y <span class="math inline">\(C_1 = 10\)</span>, <span class="math inline">\(C_2 = 20\)</span> y <span class="math inline">\(C_3 = 70\)</span> ¿es más barato un muestreo Poisson (parámetros <span class="math inline">\(p_i\)</span>) o un muestreo aleatorio simple sin reemplazo de los árboles del bosque de tamaño <span class="math inline">\(n = \lceil \sum_k n_k \pi_k \rceil\)</span>?</li>
</ol></li>
<li><p>Supongamos que se tienen dos muestras aleatorias simples sin reemplazo <span class="math inline">\(\mathcal{S}\)</span> para <span class="math inline">\(\mathcal{U}\)</span> y <span class="math inline">\(\mathcal{S}_{-}\)</span> para <span class="math inline">\(\mathcal{U}\setminus\mathcal{S}\)</span> (de tamaños <span class="math inline">\(n\)</span> y <span class="math inline">\(n_{-}\)</span>. Encuentra la covarianza entre <span class="math inline">\(\bar{x}_{\mathcal{S}}\)</span> y <span class="math inline">\(\bar{x}_{\mathcal{S}_{-}}\)</span>.</p></li>
<li><p>Queremos estimar el área cultivada por granjas en una localidad. De <span class="math inline">\(N = 2010\)</span> granjas seleccionamos <span class="math inline">\(100\)</span> mediante muestreo aleatorio simple sin reemplazo. Medimos <span class="math inline">\(x_k\)</span> el área cultivada en hectáreas y obtenemos que:
<span class="math display">\[
\sum_{i = 1}^n x_k = 2907 \qquad \text{y} \qquad \sum_{i = 1}^n x_k^2= 154593
\]</span>
Estima el promedio de hectáreas cultivadas y da un intervalo al <span class="math inline">\(90\%\)</span>.</p></li>
<li><p>Determina el tamaño de muestra bajo muestreo aleatorio simple para hallar con una precisión de al menos dos puntos porcentuales y un intervalo al <span class="math inline">\(95\%\)</span> la proporción de personas que usan lentes en una población de tamaño <span class="math inline">\(N\)</span></p></li>
<li><p>De una población de 4000 individuos nos interesan dos proporciones: <span class="math inline">\(P_1\)</span> la proporción de individuos con lavadora y <span class="math inline">\(P_2\)</span> la proporción de individuos con laptop. Se sabe además de un estudio previo que que:
<span class="math display">\[
45\% \leq P_1 \leq 65\% \quad \text{y} \quad 5\% \leq P_2 \leq 10\%
\]</span>
De qué tamaño tiene que ser la muestra si queremos conocer <em>a la vez</em> <span class="math inline">\(P_1\)</span> con una precisión <span class="math inline">\(\pm 2\%\)</span> y <span class="math inline">\(P_2\)</span> con un error de <span class="math inline">\(1\%\)</span> y una confianza de <span class="math inline">\(95\%\)</span></p></li>
<li><p>Nos interesa conocer el precio por litro en una población de <span class="math inline">\(10\)</span> gasolineras. Los precios para mayo y junio de las mismas aparecen en la tabla 2. En particular, queremos estimar la evolución del precio por litro entre dichos meses. Para ello se proponen dos métodos</p></li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Muestrear <span class="math inline">\(n\)</span> estaciones en mayo y <span class="math inline">\(n\)</span> en junio de manera independiente y calcular la diferencia en precios.</li>
<li>Muestrear <span class="math inline">\(n\)</span> estaciones en mayo e ir a verlas en junio de nuevo (a las mismas) y calcular la diferencia en precios.</li>
</ol>
<p>Determina cuál de los métodos es mejor (en términos de varianza)</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr>
<td style="text-align:left;">
Mes
</td>
<td style="text-align:left;">
Gas1
</td>
<td style="text-align:left;">
Gas2
</td>
<td style="text-align:left;">
Gas3
</td>
<td style="text-align:left;">
Gas4
</td>
<td style="text-align:left;">
Gas5
</td>
<td style="text-align:left;">
Gas6
</td>
<td style="text-align:left;">
Gas7
</td>
<td style="text-align:left;">
Gas8
</td>
<td style="text-align:left;">
Gas9
</td>
<td style="text-align:left;">
Gas10
</td>
</tr>
<tr>
<td style="text-align:left;">
Mayo
</td>
<td style="text-align:left;">
5.82
</td>
<td style="text-align:left;">
5.33
</td>
<td style="text-align:left;">
5.76
</td>
<td style="text-align:left;">
5.98
</td>
<td style="text-align:left;">
6.2
</td>
<td style="text-align:left;">
5.89
</td>
<td style="text-align:left;">
5.68
</td>
<td style="text-align:left;">
5.55
</td>
<td style="text-align:left;">
5.69
</td>
<td style="text-align:left;">
5.81
</td>
</tr>
<tr>
<td style="text-align:left;">
Junio
</td>
<td style="text-align:left;">
5.89
</td>
<td style="text-align:left;">
5.34
</td>
<td style="text-align:left;">
5.92
</td>
<td style="text-align:left;">
6.05
</td>
<td style="text-align:left;">
6.2
</td>
<td style="text-align:left;">
6
</td>
<td style="text-align:left;">
5.79
</td>
<td style="text-align:left;">
5.63
</td>
<td style="text-align:left;">
5.78
</td>
<td style="text-align:left;">
5.84
</td>
</tr>
</tbody>
</table>
<ol start="18" style="list-style-type: decimal">
<li><p>En una población de tamaño <span class="math inline">\(N\)</span> consideramos <span class="math inline">\(n\)</span> individuos mediante muestreo aleatorio simple sin reemplazo. Consideremos <span class="math inline">\(D\)</span> una subpoblación de tamaño <span class="math inline">\(N_D\)</span> de donde <span class="math inline">\(n_D\)</span> es el tamaño de los muestreados que son elementos de <span class="math inline">\(D\)</span>. La muestra la podemos dividir en dos partes: <span class="math inline">\(\mathcal{S} = (\mathcal{S}_{D}, \mathcal{S}_{\bar{D}})^T\)</span> según se esté o no en <span class="math inline">\(D\)</span>. Encuentra la distribución de <span class="math inline">\(\mathcal{S}_{D}\)</span> condicional en <span class="math inline">\(n_D\)</span>:</p></li>
<li><p>Dada una población <span class="math inline">\(\mathcal{U} = \{1,2,3\}\)</span> se tiene el diseño:
<span class="math display">\[
p(\{1,2\}) = 1/2, \quad p(\{1,3\}) = 1/4, \quad p(\{2,3\}) = 1/4
\]</span>
determina los <span class="math inline">\(\pi_k\)</span> los <span class="math inline">\(\pi_{k,l}\)</span> y los <span class="math inline">\(\Delta_{k,l}\)</span></p></li>
</ol>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="2">
<li id="fn2"><p>En la literatura muchas referencias establecen una muestra aleatoria como un conjunto de valores. Yo utilizo vectores para poder hablar de repeticiones (por ejemplo si extraes el mismo valor varias veces en la muestra).<a href="muestreo-aleatorio-simple.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Enumero las submatrices para luego poder hablar de ellas<a href="muestreo-aleatorio-simple.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Una mejor distribución sería una <span class="math inline">\(t\)</span> de Student; empero eso lo verás en Estadística Matemática.<a href="muestreo-aleatorio-simple.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>Calcúlalo.<a href="muestreo-aleatorio-simple.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>Observa que las variables aleatorias <span class="math inline">\(N_k\)</span> generalizan a las variables indicadoras.<a href="muestreo-aleatorio-simple.html#fnref6" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="análisis-exploratorio-de-datos.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="muestreo-aleatorio-estratificado.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": ["night", "white", "sepia"],
"family": "sans",
"size": [2, 4]
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Introduccion_a_Muestreo.pdf", "Introduccion_a_Muestreo.epub"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true,
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
