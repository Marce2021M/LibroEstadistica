# (APPENDIX) Apéndice {-}

```{r, echo = FALSE, message=FALSE}
library(ggplot2)
library(ggimage) 
library(grid)
library(emoGG) #devtools::install_github("dill/emoGG")
library(tidyverse)
library(beepr)
library(gridExtra)
library(latex2exp)
library(DescTools)
library(filesstrings)
filedir <- "datasets/"
subDir  <- "Mi_curso_de_R"
subcarpeta <- "~/Desktop/Mi_curso_de_R"

require(knitr)
require(kableExtra)
require(reshape2)
require(data.tree)

#Knitr table options
options(knitr.table.format = "html") 

#Chunk options
opts_chunk$set(echo = FALSE)
opts_chunk$set(results = 'asis')
opts_chunk$set(fig.width  = 4)
opts_chunk$set(fig.height = 4)
opts_chunk$set(fig.align='center')

```

# Programación en `R`

```{r, fig.cap = "`R` es un programa chido de estadística. FIN.", fig.width=1.5, fig.height=1.5, cache=TRUE, echo = FALSE,  out.width = "150px", message=FALSE}
knitr::include_graphics('images/rlogo.png')
```



Una de las primeras cosas que necesitamos saber es que `R` (por más que sus más ávidos defensores digan lo contrario) no es para todo. Si tú ya conoces otro lenguaje (sea `Stata`, `Excel`, `SAS`, `Python`, `Matlab`, `Julia`, etc) sabrás utilizar muchas de sus opciones. Estoy seguro que, de conocer uno de estos, te será muchísimo más fácil seguir sacando promedios en tu lenguaje favorito que en `R`, realizar regresiones lineales es probablemente más sencillo en `Stata` mientras que las gráficas de barras para mí son más simples en `Excel`, `Python` excede en aplicaciones de inteligencia artificial mientras que `Matlab` es más veloz que `R`, `Julia` tiene muchas cosas de ecuaciones diferenciales que nadie más. 

Lo que probablemente no sea más sencillo de hacer en otro lenguaje es realizar análisis estadístico, gráficas de todo tipo y modelos de simulación. Para eso, `R` es, indiscutiblemente, una de las mejores opciones para quienes no conocen de programación^[Modelos de simulación más avanzados suelen hacerse en `C`, `C++` o `Fortran` por su velocidad; empero, es necesario conocer más de programación.]. 

Finalmente, uno de los consejos más importantes que te puedo dar es que este curso no te va a servir si no practicas. Igual que como pasa con los idiomas uno no aprende `R` en una semana _sin practicarlo después_. Mi sugerencia es que, a la vez que sigues estas notas comiences a trabajar un proyecto _tuyo_ específico junto con [el buscador de Internet de tu preferencia](https://yandex.com) a la mano y empieces a usar `R` en él. Practica^[La práctica hace al maestro]. 

## Algunas ventajas de `R` y cosas no tan padres

### Puntos a favor de `R`

- Todo el mundo lo usa. Quizá éste es el punto más a favor. Si mucha gente lo conoce y lo utiliza, hay más opciones de ayuda. Los sitios de StackOverflow [en inglés](https://stackoverflow.com) y [en español](https://es.stackoverflow.com) son excelentes para pedir apoyo en `R`; los [grupos de usuarios de Google](https://groups.google.com/forum/#!forum/r-help-archive) son otra fuente muy buena. Entre más gente usa el programa; es más fácil obtener ayuda porque seguro alguien más tuvo hace ya tiempo el mismo problema que tú. 
- Todas las personas que trabajan en estadística publican sus métodos y su código en `R` (eso, claro, cuando publican sus métodos). Es raro encontrar _un nuevo método estadístico_ en el mundo y que no se pueda usar, de alguna forma, en `R`. 

- Dentro de los lenguajes de programación `R` es de los más sencillos. Quienes lo hicieron realmente se preocuparon por su público (de no especialistas) y en general desarrollan para él. 

- `R` es gratis. Y en esta época de austeridad, cualquier ahorro es bueno. Que sea gratis no significa que no esté respaldado: existen versiones de `R` respaldadas por grandes compañías como [Microsoft](https://mran.microsoft.com/open)

- Todo lo que se hace en `R` es público. `R` no tiene métodos secretos ni es una caja negra. Todo lo que hace cada una de las funciones de `R`, cualquiera lo puede revisar, por completo. 

- En `R` puedes hacer libros o notas ¡como este! donde guardes todo tu trabajo, reportes automatizados e incluso [documentos interactivos](https://gallery.shinyapps.io/086-bus-dashboard/) para facilitar el análisis de datos. 

- `R` puede hacer gráficas bonitas: 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
rojos  <- data.frame(x = rnorm(100, 1), y = rnorm(100,1))
verdes <- data.frame(x = rnorm(100, -1), y = rnorm(100,-2))
xmin   <- c(-4,4)
ymin   <- c(-4,4)
scatter <- ggplot(mapping=aes(x = x, y = y)) + 
  geom_point(data = verdes, color = "deepskyblue3", size = 4) + 
  geom_point(data = rojos, color = "tomato3", size = 4) +
  geom_point(data = data.frame(x = 1, y = 2), color = "black", size = 5) +
  geom_point(data = data.frame(x = 1, y = 2), color = "orange", size = 3) +
  theme_classic() +
  theme(plot.background = element_rect(fill = "white"),
        panel.background = element_rect(fill = "white")) +
  xlim(xmin[1],xmin[2]) + ylim(ymin[1],ymin[2]) + 
  xlab("Una variable") + 
  ylab("Otra variable") +
  ggtitle("Datos simulados de X y Y")

empty <- ggplot()+geom_point(aes(1,1), colour="white")+
         theme(axis.ticks=element_blank(), 
               axis.text.x=element_blank(), axis.text.y=element_blank(),                plot.background = element_rect(fill = "white"),
               panel.background = element_rect(fill = "white"),
               axis.title.x=element_blank(), axis.title.y=element_blank())

xhist <- seq(xmin[1],xmin[2], length.out = 100)
hist_top <- ggplot(mapping=aes(x=x,y=y)) + 
  geom_line(color="tomato3", data = data.frame(x = xhist, y = dnorm(xhist,1))) +
  geom_line(color="deepskyblue3", data = data.frame(x = xhist, y = dnorm(xhist,-1))) +
  theme_classic() + xlim(xmin[1],xmin[2]) + 
  theme(plot.background = element_rect(fill = "white"),
        panel.background = element_rect(fill = "white")) +
  ylab("Densidad") + xlab("") +
  ggtitle("Proyección 2")

yhist      <- seq(ymin[1],ymin[2], length.out = 100)
hist_right <- ggplot(mapping=aes(x=x,y=y)) + 
                geom_line(color="tomato3", data = data.frame(x = yhist, y = dnorm(yhist,1))) +
                geom_line(color="deepskyblue3", data = data.frame(x = yhist, y = dnorm(yhist,-2))) + 
                  ylab("Densidad") + xlab("") +
                ggtitle("Proyección 1") +
                theme_classic() + theme(plot.background = element_rect(fill = "white"),
                      panel.background = element_rect(fill = "white")) +
                xlim(ymin[1],ymin[2]) + coord_flip()

g <- arrangeGrob(
            hist_top +  geom_vline(xintercept = 0, linetype="dashed", color = "forestgreen"), 
            empty, 
            scatter +  geom_vline(xintercept = 0, linetype="dashed", color = "forestgreen") +
              geom_hline(yintercept = -0.5, linetype="dashed", color = "forestgreen") + 
              annotate("text", x = -3.5, y = 3.5, label = TeX("$\\Omega_1$"), color = "black") + 
              annotate("text", x = 3.5,  y = 3.5, label = TeX("$\\Omega_2$"), color = "tomato3") + 
              annotate("text", x = 3.5, y = -3.5, label = TeX("$\\Omega_3$"), color = "black") + 
              annotate("text", x = -3.5, y = -3.5, label = TeX("$\\Omega_4$"), color = "deepskyblue3"), 
            hist_right +  geom_vline(xintercept = -0.5, linetype="dashed", color = "forestgreen"), ncol=2, nrow=2, widths=c(5, 2), heights=c(2, 5),
            bottom = textGrob("RESULTADOS DE LA SIMULACIÓN", gp=gpar(fontface="bold", col ="black")))

cowplot::ggdraw(g) + 
  theme(plot.background = element_rect(fill="white", color = NA))

```


Por supuesto, no todo es miel sobre hojuelas con `R`. Particularmente, algunos de los problemas con el lenguaje:

```{r,  fig.cap = "La curva de aprendizaje de `R` es más empinada pero después de un rato vale la pena", fig.width=5.5, fig.height=3.5, cache=TRUE, echo = FALSE, message=FALSE}

#Get data
x     <- seq(-0.5,2, length.out=250)
pdata <- data.frame(x = x, y = x^2)

ggplot(pdata) + 
  geom_vline(xintercept = 1, linetype = "dashed", color = "gray", size = 0.5) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray", size = 0.5) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray", size = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray", size = 0.5) +
  geom_line(aes(x = x, y = y, color = "STATA, SPSS ó SAS"), data = subset(pdata, x > 0)) +
  geom_line(aes(x = x, y = 0, color = "STATA, SPSS ó SAS"), data = subset(pdata, x < 0)) + 
  geom_line(aes(x = x, y = x, color = "R")) + 
  geom_point(aes(x = x, y = y), color = "gray", data = data.frame(x = 1, y = 1)) + 
  geom_point(aes(x = x, y = y), color = "gray", data = data.frame(x = 0, y = 0)) + 
  theme_classic() + 
  ylab(toupper("Dificultad para programar")) +
  xlab(toupper("Dificultad del modelo")) + 
  coord_cartesian(ylim = c(-0.5, 4), clip = 'off') + 
  annotate("text", x = 0.5, y = -0.9, label = "No uses R") +
  annotate("text", x = 1.5, y = -0.9, label = "Usa R") +
  annotate("text", x = -0.25, y = -0.9, label = "¡R!") +
  theme(axis.text = element_blank(),
        axis.title.x = element_text(margin = margin(0.5, unit = "cm")),
        axis.ticks.y = element_blank(),
        axis.ticks.length.x = unit(.5, "cm")) + 
  scale_x_continuous(breaks = c(0,1)) +
  scale_y_continuous(breaks = c(0,1)) + 
  scale_color_manual("Programa", 
                     values = c("STATA, SPSS ó SAS" = "tomato3", 
                                "R" = "deepskyblue3"))

```

- La curva de aprendizaje es mucho más empinada que para otros programas estadísticos (como  `Stata`, `SAS` o `SPSS`) ¡particularmente si es tu primera vez programando!

- La mayor parte de las personas que trabajan en `R` no son programadores de verdad. Gran parte del código que te puedes encontrar **en el mundo real** está escrito [con prisa para salir del aprieto](https://nsaunders.wordpress.com/2014/05/14/this-is-why-code-written-by-scientists-gets-ugly/) sin mucha planeación, con pocos comentarios, falta de control de versiones y pocas herramientas de revisión. ¡Internet está lleno de [creaturas espantosas escritas en `R`](https://codegolf.stackexchange.com/a/4011)!

```{r,fig.cap = "`R` puede ser muy lento pero eso te da oportunidad de hacer otras cosas ;) .", fig.width=4.5, fig.height=4.5, cache=TRUE, echo = FALSE,  message=FALSE}
knitr::include_graphics('images/compiling.png')
```

- `R` [de ninguna manera es veloz](https://github.com/matthieugomez/benchmark-stata-r) por lo que algunos programas (lo veremos en simulación) pueden ser extremada (y dolorosamente) lentos.  

## Bienvenidx a `R`, `r R.Version()$nickname` (sí, así se llama esta versión)

`R` es un lenguaje de cómputo y un programa estadístico [libre](https://www.gnu.org/philosophy/free-sw.html), gratuito, [de programación funcional](http://adv-r.had.co.nz/Functional-programming.html) (¿qué es eso?), [orientado a objetos](https://en.wikipedia.org/wiki/Object-oriented_programming) (_what??_) que mutó a partir de otros dos lenguajes conocidos como `Scheme` y `S`^[De ahí que se llame `R` porque la `R` es una mejor letra que la `S` (todos lo sabemos) 
-Atte. Rodrigo, el autor de este documento.]. El primero de estos fue  desarrollado en el MIT por Sussman y Steele mientras que el segundo surgió en los laboratorios Bell^[Mejor conocidos ahora como AT&T, la compañía celular que nunca tiene señal.] creado por Becker, Wilks y Chambers. `R` [nació en junio de 1995](https://cran.r-project.org/doc/html/interface98-paper/paper_2.html) a partir del trabajo de 	Ross Ihaka y Robert Gentleman^[Sus nombres empiezan con la letra `R` ¿coincidencia?]. 

Desde su creación, la mayor parte del desarrollo de `R` ha sido trabajo completamente voluntario de la [Fundación R](https://www.r-project.org/foundation/), del equipo de R Core y de miles de usuarios que han creado funciones específicas para `R` conocidas como paquetes (`packages`). Actualmente el repositorio más importante de `R`, CRAN, contiene más de _16000_ paquetes con distintas funciones para hacer ¡lo que quieras!

Como todo el trabajo en `R` es voluntario hace falta: 

1. Una homologación en los métodos. Puedes encontrar varias funciones _que supuestamente hacen exactamente lo mismo_ (como es el caso de `emojifont`, `fontemoji` y `emoGG` para graficar usando emojis). 

2. Estandarizar la notación. Algunos paquetes como aquellos del `tidyverse` (veremos más adeltna) utilizan `pipes` (`%>%`); estos sólo funcionan en el `tidyverse` pero no fuera del mismo.

Sin embargo, también es una gran ventaja que sean los usuarios de `R` quienes guían su desarrollo. El lenguaje va mutando según peticiones de las personas que lo usan. Si hay algo que te gustaría `R` tuviera y aún no existe ¡lo puedes proponer! 

## Instalando cosas

### Instalación de `R`

```{r, fig.cap = "Oficialmente, la página de `R` es de las páginas más feas del mundo. ¡No te dejes llevar por las apariencias!", fig.width=3.5, fig.height=3.5, cache=TRUE, echo = FALSE,  message=FALSE}
knitr::include_graphics('images/CRAN1.png')
```

A lo largo de estas notas estaré trabajando con: `r R.Version()$version.string` _`r R.Version()$nickname`_. La más reciente versión de `R` la puedes encontrar en [CRAN](https://cran.r-project.org). Para ello ve al sitio y selecciona tu plataforma. 

> **Nota usuarios de Mac** En algunas Mac, al abir R, aparece el siguiente mensaje de advertencia:
`During startup - Warning messages: 1: Setting LC_CTYPE failed [...]`
para [solucionarlo](https://stackoverflow.com/questions/9689104/installing-r-on-mac-warning-messages-setting-lc-ctype-failed-using-c) ve a `Aplicaciones` y abre `Terminal`. Copia y pega en ella el siguiente texto:
`defaults write org.R-project.R force.LANG en_US.UTF-8`
Da enter, cierra la `Terminal` y reinicia `R`. 

- En el caso de Windows da clic en `Download R for Windows` y luego en `install R for the first time`. Finalmente, ejecuta el instalable que aparece al dar click en  ``r paste0("Download R ", R.Version()$major,".",R.Version()$minor," for Windows")`` . 

> Para este curso pudiera ser que requirieras las herramientas de desarrollador [Rtools](https://cran.r-project.org/bin/windows/Rtools/).

- En el caso de Mac selecciona `Download R for (Mac) OS X` y luego elige   ``r paste0("R-", R.Version()$major,".",R.Version()$minor,".pkg")``. En Mac puede que necesites instalar adicionalmente [XQuartz](https://www.xquartz.org) (según tu versión de Mac). Si tu Mac es una versión suficientemente antigua, sigue las instrucciones específicas de `CRAN`.  

- En el caso de Linux al elegir `Download R for Linux` tendrás la opción de buscar tu distribución específica. Al elegirla, aparecerán instrucciones para tu terminal de comandos; síguelas. En el caso de Linux, según los paquetes de `R` que elijamos instalar en la computadora requerirás instalar paquetería adicional para tu distribución de Linux. `R` te informará de la paquetería necesaria conforme la requiera.    

> Si tienes problemas para instalar puedes usar [RStudio Cloud](https://rstudio.cloud).

## Instalación de `RStudio`

```{r,  fig.cap = "RStudio es una empresa que se dedica a hacer cosas para R.", fig.width=3.5, fig.height=3.5, cache=TRUE, echo = FALSE,  message=FALSE}
knitr::include_graphics('images/rstudio.png')
```

`RStudio` es una interfaz gráfica (IDE) para `R`. Puedes pensar a `R` como el _Bloc de Notas_ y a `RStudio` como _Word_. El _Bloc_ tiene todas las capacidades que necesitas para poder escribir; empero, es muchísimo mejor trabajar tus _papers_ en _Word_. De la misma manera, `R` tiene todas las capacidades para hacer estadística _pero un formato horrible_ y `RStudio` se ha convertido en la más popular forma de usar `R`. Por supuesto que no es la única; algunas alternativas son [Atom con ide-r](https://atom.io/packages/ide-r), [Eclipse con StatET](https://marketplace.eclipse.org/content/statet-r) y [RKWard](https://rkward.kde.org). En general es posible seguir estas notas sin que tengas `RStudio` pero, si es tu primera vez programando, no lo recomiendo.

> Si ya tienes experiencia con lenguajes como Python, Javascript, Java ó alguno de los mil C que existen, no tendrás ningún problema usando el editor de tu preferencia.

Para descargar `RStudio` ve a [su página](https://www.rstudio.com) y da clic en `Download RStudio`. Baja tu pantalla hasta donde dice `Installers for Supported Platforms` y elige tu plataforma: `Windows`, `Mac OS X` ó tu sabor de `Linux` preferido. Una vez descargado el archivo, ábrelo y sigue las instrucciones que aparecen en pantalla.  


## Primeros pasos en `R` usando `RStudio`
Una vez hayas instalado `R` y `RStudio`, abre `RStudio`^[Si decidiste no instalar RStudio salta al final de esta sección.]. Te enfrentarás a una pantalla similar a esta: 

```{r fig-main, fig.cap = "La primera vez que abres RStudio", cache=TRUE, echo = FALSE}
knitr::include_graphics('images/RStudio1.png')
```

Si tu RStudio tiene sólo 3 páneles, como en mi caso, ve a la esquina superior izquierda (signo de hoja+) y elige un nuevo `R Script`

```{r, fig.cap = "Elige hoja+ para crear un nuevo archivo", cache=TRUE, echo = FALSE}
knitr::include_graphics('images/RStudio2.png')
```

Tendrás, entonces, 4 páneles como se ve a continuación: 

```{r, fig.cap = "RStudio <3", cache=TRUE, echo = FALSE}
# Import the image
library(grid)
bg <- png::readPNG("images/RStudio3.png")
ggplot() +  
  annotation_custom(rasterGrob(bg, 
                                 width = unit(1,"npc"), 
                                 height = unit(1,"npc")), 
                      -Inf, Inf, -Inf, Inf) +
  geom_label(aes(x = 2.5, y = 1.5), fill = "deepskyblue3", color = "white", alpha = 0.95,
              label = "1", size = 10) +
  geom_label(aes(x = 2.5, y = 7.5), fill = "deepskyblue3", color = "white", alpha = 0.95,
              label = "2", size = 10) +
  geom_label(aes(x = 7.5, y = 1.5), fill = "deepskyblue3", color = "white", alpha = 0.95,
              label = "4", size = 10) +
  geom_label(aes(x = 7.5, y = 7.5), fill = "deepskyblue3", color = "white", alpha = 0.95,
              label = "3", size = 10) +
  xlim(c(0,10)) + ylim(c(0,10)) + theme_void()


```



1. El primer panel (esquina inferior izquierda) es la `Consola`. Aquí es donde se ejecutan las acciones. Prueba escribir `2 + 3` en él y presiona enter. Aparece el resultado de la suma. Definitivamente, `R` es la calculadora que más trabajo cuesta instalar. 

```{r,  fig.cap = "La consola de `R` es la calculadora más difícil de instalar que existe.", cache=TRUE, echo = FALSE,  message=FALSE}
knitr::include_graphics('images/RStudio4.png')
```

2. El segundo panel (esquina superior izquierda) es el panel con el `Script`. Aquí se escribe el programa pero no _se ejecuta_. Prueba escribir `10 + 9`. ¿Ves que no pasa nada? Lo que acabas de hacer es crear un programa que, cuando se ejecute, hará la suma de `10 + 9`. ¡Qué programa más aburrido! Sin embargo, no todo está perdido: presiona `CTRL+Enter` (`Cmd+Enter` en Mac) al final de la línea o bien da clic en `Run` y verás que, en la consola, aparece la instrucción y el resultado de la misma. El `Script` es una excelente fuente para tener un historial de lo que estás haciendo. 

```{r, fig.cap = "El `Script` sirve para salvar las instrucciones en el orden en que las vas a ejecutar.", cache=TRUE, echo = FALSE,  message=FALSE}
knitr::include_graphics('images/RStudio5.png')
```

3. El tercer panel contiene el ambiente. Aquí aparecerán las variables que vayamos creando. Por ahora, para poner un ejemplo, importaremos el archivo `Example1.csv` (con valores simulados) [disponible en Github](https://github.com/RodrigoZepeda/LibroEstadistica/tree/master/datasets) dando clic en `Import Dataset` y `From Text (base)`. Selecciona el archivo y elige las opciones en la ventana de previsualización que hagan que se vea bien. Nota que una vez realizada la importación aparece en el panel derecho `Example1.` Al dar clic podrás ver la base de datos. Las bases de datos y variables que utilices durante tus análisis aparecerán en esa sección.

```{r, echo = FALSE}
set.seed(371)
Example1 <- data.frame(Alturas = rnorm(100, 1.65, 0.3), Peso = rnorm(100, 80, 1))
write.csv(Example1, paste0(filedir,"Example1.csv"), row.names=FALSE)
```

```{r, fig.cap = "El `Ambiente` muestra las variables (incluyendo bases de datos) que estás utilizando en este momento. A diferencia de otros programas estadísticos (o sea `Stata`) en `R` es posible tener múltiples bases de datos abiertas a la vez.",  cache=TRUE, echo = FALSE,  message=FALSE}
knitr::include_graphics('images/RStudio6.png')
```

4. Para entender mejor lo que ocurre en el último de los páneles, lo mejor es trabajar con nuestra base. Escribe en la consola `plot(Example1)` . En el cuarto pánel aparecerá una gráfica. El cuarto de los páneles para nosotros tendrá esa utilidad: mostrará las gráficas que hagamos así como la ayuda. Para ver la ayuda para las instrucciones de `R` puedes escribir `?`. Prueba teclear `?plot` en la consola. El signo de interrogación es un `help()` que muestra las instrucciones para usar una función.  

```{r,  fig.cap = "La gráfica que aparece de hacer un `plot` de la base de datos de ejemplo.",  cache=TRUE, echo = FALSE, message=FALSE}
plot(Example1)
```

```{r,  fig.cap = "El cuarto panel muestra respectivamente las gráficas y la ayuda.", cache=TRUE, echo = FALSE,  message=FALSE}
knitr::include_graphics('images/RStudio7.png')
```

Mi sugerencia personal es que escribas todo lo que haces en el `Script` y que sólo utilices la consola para verificar valores. De esta manera podrás almacenar todas las instrucciones ejecutadas y volver a ellas cuando se requieran. Por último te sugiero utilizar `#` gatos para comentar tu código. Así, el código anterior lo podrías ver en la consola como:

```{r, eval = FALSE}
#Aquí pruebo cómo R hace las sumas
10 + 9
```

[Comenta](https://www.freecodecamp.org/news/code-comments-the-good-the-bad-and-the-ugly-be9cc65fbf83/). [Comenta](https://www.c-sharpcorner.com/blogs/why-comments-are-important-while-writing-a-code). [Comenta, por favor](https://blog.codinghorror.com/code-tells-you-how-comments-tell-you-why/). Tu ser del futuro que regrese a sus archivos de `R` un mes después de haberlos hecho te lo agradecerá (y tu profe también). 

Finalmente y como aclaración para estas notas, el código de `R` aparece como:
```{r, eval = FALSE}
#Esto es código de R
7 - 2
```

Mientras que los resultados de evaluar en `R` se ven con `#`:
```{r, echo = FALSE}
#Esto es código de R
7 - 2
```

Así, la evaluación con su resultado se ve de la siguiente forma:
```{r}
#Esto es código de R
7 - 2
```

## Cálculos numéricos
`R` sirve como calculadora para las operaciones usuales. En él puedes hacer sumas,

```{r}
#Esto es una suma en R
12 + 31
```

```{r,  fig.cap = "Ada Lovelace (1815-1852), la primera en diseñar un algoritmo computacional ¡y sin tener computadoras!", fig.width=3.5, fig.height=3.5, cache=TRUE, echo = FALSE, message=FALSE}
knitr::include_graphics('images/ada_lovelace.jpg')
```

restas,
```{r}
#Esto es una resta en R
3 - 4
```
 
multiplicaciones,
```{r}
#Esto es una multiplicación en R
7*8
```

divisiones,
```{r}
#Esto es una división en R
4/2
```



sacar logaritmos naturales $\ln$,
```{r}
#Para sacar logaritmo usas el comando log
log(100)
```

o bien logaritmos en cualquier base,^[Recuerda que un logaritmo base $a$ te dice a qué potencia $b$ tuve que elevar $a$ para llegar a $b$. Por ejemplo $\log_{10}(100) = 2$ te dice que para llegar al $100$ tuviste que hacer $10^2$.]
```{r}
#Puedes especificar la base del logaritmo con base 
log(100, base = 10)
```

también puedes elevar a una potencia (por ejemplo hacer $6^3$),
```{r}
#Así se calculan potencias
6^3
```

calcular la exponencial $e$,
```{r}
#Para exponenciales puedes usar exp
exp(1)
```

o bien exponenciar cualquier variable $e^{-3}$,
```{r}
#O bien exponenciales específicas, e^-3
exp(-3)
```

también puedes usar el número $\pi$.
```{r}
#Cálculo de pi
pi
```

No olvides que `R` usa el orden de las operaciones de matemáticas. Siempre es de izquierda a derecha con las siguientes excepciones: 

1. Primero se evalúa lo que está entre paréntesis. 

2. En segundo lugar se calculan potencias. 

3. Lo tercero en evaluarse son multiplicaciones y divisiones. 

4. Finalmente, se realizan sumas y restas. 

Por ejemplo, en la siguiente ecuación 
$$
2 - 2 \cdot \frac{(3^4 - 9)}{(5 + 4)}
$$
se resuelven primero los paréntesis $(3^4 - 9) = 81 - 9 = 72$ y $(5 + 4) = 9$; luego se resuelve la división: $\frac{72}{9}=8$, se multiplica por el $2$: $2 \cdot 8 = 16$ y finalmente se hace la resta: $2-16 = -14$. 

### Ejercicio 
Determina, sin evaluar, los resultados de los siguientes segmentos de código:
```{r, eval = FALSE}
#Primer ejercicio 
(9 - 3)^2 * (2 - 1) - 6
```



```{r, eval = FALSE}
#Segundo ejercicio 
6 * 2 / (7 - 3) * 5
```

```{r, eval = FALSE}
#Tercer ejercicio 
2 * 3 ^ 2 * 2 / (5 - 4) * 1 / 10 
```

Evalúa para comprobar tu respuesta. 

### Ejercicio 
Calcula el área y el perímetro de un círculo de radio 5. Recuerda que la fórmula del área es $\pi \cdot r^2$ donde $r$ es el radio; mientras que la del perímetro es: $\pi \cdot d$ donde $d$ es el díametro (= dos veces el radio).

### Respuestas
```{r, echo = FALSE}
r = 5
cat(paste0("Área = ", pi*r^2,"\n"))
cat(paste0("Perímetro = ", pi*r*2))
```

## Variables

`R` es un programa orientado a objetos; esto quiere decir que `R` almacena la información en un conjunto de variables que pueden tener diferentes `clases` y opera con ellos según su clase. Por ejemplo, un conjunto de caracteres, entre comillas, es un `Character` (`R` lo piensa como texto)

```{r}
#Un conjunto de caracteres es un char
"Hola"
```

Un número (por ejemplo `2` tiene clase `numeric`)^[Puede ser `float`, `int`, `double` pero no nos preocuparemos por eso.]. Hay que tener mucho cuidado con combinar floats con `Strings`:

```{r}
#Código que sí funciona porque ambos son números
2 + 4 
```

```{r, fig.cap = "El algoritmo diseñado por Ada Lovelace.", fig.width=3.5, fig.height=3.5, cache=TRUE, echo = FALSE,, message=FALSE}
knitr::include_graphics('images/algorithm_lovelace.jpg')
```

```{r, error = TRUE}
#Código que no funciona porque uno es caracter
2 + "4" 
```

Si lo piensas, este último error ¡tiene todo el sentido! no puedes sumar un número a un texto. ¿O qué significaría `'Felices' * 4` ?

La magia de `R` comienza con que puedes almacenar valores en variables. Por ejemplo, podemos asignar un valor a una variable:
```{r}
#Asignamos x = 10
x <- 10
```



Hay dos formas de asignar valores, una es con la flecha de asignación $\leftarrow$ y otra con el signo de igual:
```{r}
#Podemos asignar valores con el signo de =
y = 6
```

Nota que, cuando realizamos operaciones, la asignación es la última  que se realiza:
```{r}
#Aquí z = 106
z <- y + x^2
```

Los valores que fueron asignados en las variables, `R` los recuerda y es posible calcular con ellos:
```{r}
#Podemos realizar una suma
x + y

#O bien podemos realizar una multiplicación
3*y - x
```

Podemos preguntarnos por el valor de las variables numéricas mediante los operadores `==` (sí, son dos iguales), `!=` (que es un $\neq$) `>`, `>=`, `<=` y `<`:
```{r}
#Podemos preguntarnos si x vale 4
x == 4
```

> El operador de asignación también se puede utilizar al revés $2 \rightarrow x$ pero no lo hagas, por favor. 

Nota que no estamos asignando el valor de `x`:
```{r}
x
```

Podemos preguntarnos por diferencia:
```{r}
x != 4 
```

Así como por mayores, menores incluyendo posibles igualdades (_i.e._ los casos $\geq$ y $\leq$)
```{r}
#Nos preguntamos si x > y
x > y

#Nos preguntamos si x >= 10
x >= 10

#Nos preguntamos si y < 6
y < 6

#O bien si y <= 6
y <= 6
```

En todos los casos los resultados han sido `TRUE` ó `FALSE`. La clase de variables que toma valores `TRUE` ó `FALSE` se conoce como booleana. Hay que tener mucho cuidado con ellas porque, puedes acabar con resultados muy extraños:

```{r}
#MALAS PRÁCTICAS, NO HAGAS ESTO
#Cuando lo usas como número TRUE vale 1
100 + TRUE

#MALAS PRÁCTICAS, NO HAGAS ESTO
#Cuando lo usas como número FALSE vale 0
6*FALSE
```

> [Aquí](https://medium.com/mindorks/common-bad-programming-practices-7fb470ed74d2) puedes encontrar una lista de malas prácticas en computación a evitar. 

Finalmente, nota que es posible reescribir una variable y cambiar su valor:
```{r}
#Aquí x vale 10, como antes
x

#Aquí cambianos el valor de x y valdrá 0.5
x <- 0.5
x
```

### Ejercicios
Determina el valor que imprime `R` en cada caso, sin que corras los siguientes pedazos de código. Después, verifica tu respuesta con `R`:

```{r, eval = FALSE}
#Primer ejercicio
x <- 100
y <- 3
x > y
```

```{r, eval = FALSE}
#Segundo ejercicio
z <- (4 - 2)^3
z <- z + z + z
z
```

```{r, eval = FALSE}
#Tercer ejercicio
x <- 3
y <- 2
z <- x * y
x <- 5
y <- 10
z
```

```{r, eval = FALSE}
#Cuarto ejercicio
variable1 <- 1000
variable2 <- 100
variable3 <- variable1/variable2 <= 10
variable3
```

```{r, eval = FALSE}
#Quinto ejercicio
"2" - 2
```

```{r, eval = FALSE}
#Sexto ejercicio
(0.1 + 0.1 + 0.1) == 0.3
```

### NIVEL 3

Determina, sin correr el programa, qué regresa la consola en este caso
```{r, eval = FALSE}
x <- 2 
x <- 5 + x -> y -> x
x <- x^2
x
```

Comprueba con la consola tus resultados; puede que encuentres respuestas poco intuitivas. 

## Observaciones sobre la aritmética de punto flotante

Si hiciste el penúltimo ejercicio (el cual, obviamente hiciste y comprobaste con la consola) podrás haber notado una trampa. Analicemos qué ocurre; quizá hicimos mal la suma
```{r}
#Veamos si este lado está mal
(0.1 + 0.1 + 0.1)

#O si éste es el que tiene la trampa
0.3
```

Aparentemente no hay nada malo ¿qué rayos le pasa a `R`? La respuesta está [en la aritmética de punto flotante](https://www.youtube.com/watch?v=PZRI1IfStY0). Podemos pedirle a `R` que nos muestre los primeros 100 dígitos de la suma `0.1 + 0.1 + 0.1`:

```{r, fig.cap = "Réplica de la Z3, la primer computadora con punto flotante (1941).", fig.width=3.5, fig.height=3.5, cache=TRUE, echo = FALSE,  message=FALSE}
knitr::include_graphics('images/Z3_Deutsches_Museum.jpeg')
```

```{r}
#Veamos qué pasa con la suma
options(digits = 22) #Cambiamos dígitos
(0.1 + 0.1 + 0.1)    #Sumamos
```

> El comando `options(digits = 22)` especifica que `R` debe imprimir en la consola `22` dígitos. No más. 

¡[Ahí está el detalle](https://www.youtube.com/watch?v=1jaCpeXg-gg)! `R` no sabe sumar. En general, ningún programa de computadora sabe hacerlo. Veamos otros ejemplos:

```{r}
4.1 - 0.1 #Debería dar 4
3/10      #Debería ser 0.3
log(10^(12345), base = 10) #Debería dar 12345
``` 

El problema está en cómo las computadoras representan los números. Ellas escriben los números en binario. Por ejemplo, 230 lo representan como `r DescTools::DecToBin(230)` mientras que el 7 es: `r DescTools::DecToBin(7)`. El problema de las computadoras radica en que éstas tienen una memoria finita por lo que números muy grandes como: $124765731467098372654176$ la computadora hace lo mejor por representarlos eligiendo el más cercano:
```{r}
#Nota la diferencia entre lo que le decimos a R
#y lo que resulta
x <- 124765731467098372654176
x
```

> Un error de punto flotante en la vida real ocasionó en los años noventa, [la explosión del cohete `Ariane 5`](https://www.esa.int/Newsroom/Press_Releases/Ariane_501_-_Presentation_of_Inquiry_Board_report). Moraleja: hay que tener cuidado y respeto al punto flotante.  

No olvides cambiar la cantidad de dígitos que deseas que imprima `R` en su consola de vuelta:
```{r}
options(digits = 6) #Cambiamos dígitos
```

El mismo problema ocurre con números decimales cuya representación binaria es periódica; por ejemplo el $\frac{1}{10}$ en binario se representa como $0.0001100110011\overline{0011}\dots$. Como es el cuento de nunca acabar con dicho número, `R` lo trunca y almacena sólo los primeros dígitos de ahí que, cada vez que escribes `0.1`, `R` en realidad almacene el <code>`r sprintf(0.1, fmt = '%#.22f')`</code> que es _casi lo mismo_ pero no es estrictamente igual. Hay que tener mucho cuidado con esta inexactitud de las computadoras (inexactitud estudiada por la rama de [Análisis Numérico](https://www.springer.com/gp/book/9781461484523)) pues puede generar varios resultados imprevistos.

### ¿Cómo checar un if?

En general lo que hacen las computadoras para comparar valores es que verifican que, en valor absoluto, el error sea pequeño. Recuerda que el valor absoluto de $x$, $|x|$, regresa siempre el positivo:
$$
|4| = 4 \qquad \textrm{y} \qquad |-8| = 8
$$

Para verificar que algo es más o menos $0.3$ suele usarse el valor absoluto^[En `R` el comando `abs` toma el valor absoluto.] de la siguiente manera:
```{r}
abs( (0.1 + 0.1 + 0.1) - 0.3 ) < 1.e-6
```

donde `1.e-6` es notación corta para `r sprintf("%f", 1.e-6)`  (también escrito como $1\times 10^{-6}$). La pregunta que nos estamos haciendo es que si el error entre sumar $0.1+0.1+0.1$ y $0.3$ es muy pequeño $< 0.000001$:
$$
| (0.1 + 0.1 + 0.1) - 0.3 | < 0.000001
$$



## Leer y almacenar variables en `R`

Para terminar esta sección, aprenderemos cómo guardar variables en `R`. Para eso, el concepto de directorio es uno de los más relevantes. En general, en computación, [el directorio](https://en.wikipedia.org/wiki/Working_directory) se refiere a la dirección en tu computadora donde estás trabajando. Por ejemplo, si estás en una carpeta en tu escritorio de nombre "Ejercicios_R" probablemente tu directorio sea '~/Desktop/Ejercicios_R/' (en Mac) o bien '~\\Desktop\\Ejercicios_R\\'  en Windows^[Windows usa backslash. Y hay [toda una historia detrás de ello](https://www.howtogeek.com/181774/why-windows-uses-backslashes-and-everything-else-uses-forward-slashes/)]. La forma de saber tu directorio (en general) es ir a la carpeta que te interesa y con clic derecho ver propiedades (o escribir `ls` en la terminal `Unix`).

`R` tiene un directorio `default` que quién sabe dónde está (depende de tu instalación, generalmente está donde tu `Usuario`). Usualmente lo mejor es elegir un directorio para cada uno de los proyectos que hagas. Para ello si estás en `RStudio` puedes utilizar `Shift+Ctrl+H` (`Shift+Cmd+H` en Mac) o bien ir a `Session > Set Working Directory > Choose Directory` y elegir el directorio donde deseas trabajar tu proyecto. Pensando que elegiste el escritorio (`Desktop` en mi computadora) notarás que en la consola aparece el comando  `setwd("~/Desktop")` (o bien con '\\' si eres Windows). Mi sugerencia es que copies ese comando en tu `Script` para que, la próxima vez que lo corras ya tengas preestablecido el directorio. 

```{r, eval = FALSE}
#Si eres Mac/Linux
setwd("~/Desktop") 

#Si eres Windows
setwd("C:\Users\Rodrigo\Desktop") #Rodrigo = Mi usuario
```


Podemos verificar el directorio elegido con `getwd()`:
```{r, eval = FALSE}
getwd()
```



> En general es buena práctica en `R` establecer, hasta arriba del `Script`, el comando de directorio. Esto con el propósito de que, cuando compartas un archivo, la persona a quien le fue compartido el archivo pueda rápidamente elegir su propio directorio en su computadora. 

Probemos guardar unas variables en un archivo dentro de nuestro directorio. Para ello utilizaremos el comando `save`. 

```{r}
#Crear las variables
x <- 200
y <- 100

#Los archivos de variables de R son rda
save(x,y, file = "MisVariables.rda")
```

Si vas a tu directorio, notarás que el archivo `MisVariables.rda` acaba de ser creado. De esta forma `R` puede almacenar objetos creados en `R` que sólo `R` puede leer (más adelante veremos cómo exportar bases de datos y gráficas). Observa que en tu ambiente (si estás en `RStudio` puedes verlas en el panel 3) deben aparecer las variables que hemos usado hasta ahora:

```{r, echo = FALSE}
rm(rojos, empty, pdata, scatter, xmin, ymin, hist_right, hist_top,
   verdes, bg, g, r, xhist, yhist)
names(.GlobalEnv)[-which(names(.GlobalEnv) %in% c(".Random.seed", "escritorio","subDir","subcarpeta", "filedir"))]
```

Podemos probar sumar nuestras variables y todo funciona súper:
```{r}
x + y #Funciona magníficamente
```

Limpiemos el ambiente. El comando equivalente al `clear all` en `R` es un poco más complicado de memorizar: 
```{r, eval = FALSE}
#EL clear all de R
rm(list = ls())
```

```{r, echo = FALSE}
rm(x,y)
```

Ahora, si vuelves a ver el ambiente, éste estará vacío: ¡hemos limpiado el historial! Nota que si intentamos operar con las variables, `R` ya no las recuerda:

```{r, error=TRUE}
x + y #Error
```

> Así como hay que lavarse las manos antes de comer, es buen hábito limpiar todas las variables del ambiente de `R` antes de usarlo. 

Podemos leer la base de datos usando `load`:
```{r}
#Leemos las variables
load("MisVariables.rda")

#Una vez leídas podemos empezar a jugar con ellas
x + y #Ya funciona
```

Por último, es necesario resaltar la importancia del directorio. Para ello crea una nueva carpeta en tu escritorio de nombre <code>`r subDir`</code>. Mueve el archivo `"MisVariables.rda"` dentro de la carpeta. Borra todo e intenta leer de nuevo el archivo:

```{r, echo = FALSE, message=FALSE}
#file.move(path.expand("MisVariables.rda"), path.expand(subcarpeta))
```

```{r, eval = FALSE}
#Borramos todo
rm(list = ls())

#Intentamos leer el archivo de nuevo
load("MisVariables.rda")
```


```{r, echo = FALSE, error = TRUE}
setwd("~")
load("MisVariables.rda")
```

Este error es porque `R` sigue pensando que nuestro directorio es el escritorio y está buscando el archivo ahí sin hallarlo. Para encontrarlo hay que cambiar el directorio a través de `RStudio` (ya sea `Ctrl+Shift+H` o `Session >Set Working Directory > Choose Directory`) o bien a través de comandos en `R`: 

```{r, eval = FALSE}
#Si eres Mac/Linux
setwd("~/Desktop/Mi_curso_de_R") 

#Si eres Windows
setwd("C:\Users\Rodrigo\Desktop\Mi_curso_de_R") #Rodrigo = Mi usuario
```


```{r}
#Aquí sí se puede leer
load("MisVariables.rda")
```

### Ejercicio
Responde a las siguientes preguntas: 

1. ¿Qué es el directorio y por qué es necesario establecerlo? 

2. Si `R` me da el error `'No such file or directory'` ¿qué hice mal?

3. En `RStudio`, ¿qué hace `Session > Restart R`? ¿cuál es la diferencia con `rm(list = ls())`?

4. ¿Qué hace el comando `cat("\014")`? (_Ojo_ puede que no haga nada). Si funciona, ¿cuál es la diferencia con `rm(list = ls())` y con `Restart R`?

## Instalación de paquetes
Un paquete de `R` es un conjunto de funciones adicionales elaboradas por los usuarios, las cuales permiten hacer cosas adicionales en `R`. Para instalarlos requieres de una conexión a Internet (o bien puedes instalarlos a partir de un archivo, por ejemplo, mediante una `USB`). El comando de instalación es `install.packages` seguido del nombre del paquete. Por ejemplo (y por ocio) descarguemos el paquete `beepr` para hacer reproducir sonidos en la computadora^[En los siguientes capítulos descargaremos paquetes más interesantes; pero no desprecies la utilidad de `beepr` yo lo he usado en múltiples ocasiones para que la computadora me avise que ya terminó de correr un código.]. Para ello:  

```{r, eval = FALSE}
install.packages("beepr")
```

```
[...]
* DONE (beepr)

The downloaded source packages are in
	‘/algun/lugar/downloaded_packages’
```

Esto significa que el paquete ha sido instalado. Nos interesa usar la función `beep` que emite un sonido (`??beep` para ver la ayuda). Si la llamamos así tal cual, nos da error:
```{r, error = TRUE}
beep(3)
```

`R` es incapaz de hallar la función porque aún no le hemos dicho dónde se encuentra. Para ello podemos llamar al paquete mediante la función `library` y decirle a `R` que incluya las funciones que se encuentran dentro de `beepr`:

```{r}
library(beepr)
beep(3) #Esto produce un sonido
```

El comando `library` le dice a `R` ¡hey, voy a usar unas funciones que creó alguien más y que están dentro del paquete `beepr`! De esta manera, al correr `beep(3)`, `R` ya sabe dónde hallar la función y por eso no arroja error. 

### Ejercicios

**NIVEL 1**

1. Instala los paquetes `tidyverse` en `R`.
2. De `tidyverse` haz lo necesario para que el siguiente bloque de código te arroje una gráfica:

```{r}
#Aquí tienes que hacer algo
#
# RELLENA AQUÍ
#

#Esto genera un histograma
set.seed(1364752)
mis.datos <- data.frame(x = rnorm(1000))
ggplot(mis.datos, aes(x = x)) + 
  geom_histogram(bins = 50, fill = "deepskyblue3") +
  ggtitle("Histograma generado por el código")

```

**NIVEL 3**

1. Instala el paquete `devtools` (para hacerlo probablemente necesites instalar más cosas en tu computadora; averigua cuáles)
2. Usa `devtools` para instalar el paquete [`emoGG`](https://github.com/dill/emoGG) desde Github.
3. Verifica que tu instalación fue correcta haciendo la siguiente gráfica:
```{r}
library(emoGG)
ggplot(mtcars, aes(wt, mpg))+ geom_emoji(emoji="1f697")
```


## Comentarios adicionales sobre el formato
Así como en el español existen reglas de gramática para ponernos todos de acuerdo y entendernos entre todos, en `R` también existen _sugerencias_ a seguir para escribir tu código. Las sugerencias que aquí aparecen fueron adaptadas de las que [utiliza el equipo de `Google`](https://google.github.io/styleguide/Rguide.xml).

1. No escribas líneas de más de 80 caracteres (si se salió de tu pantalla, mejor continúa en el siguiente renglón).

2. Coloca espacios entre operadores `+,*,/,-,<-,=, <, <=, >, >=, ==` y usa paréntesis para agrupar: 
```{r, eval = FALSE}
#Esto no se ve muy bien
abs(3*5/(4-9)^2-60/100-888+0.1*8888-4/10*2) < 1.e-6

#Los espacios permiten distinguir el orden de las operaciones
abs( (3 * 5) / (4 - 9)^2 - 60 / 100 - 888 
      + (0.1 * 8888) - (4 / 10) * 2 ) < 1.e-6
```

3. Intenta alinear la asignación de variables para legibilidad:
```{r, eval = FALSE}
#Esto no tanto
altura <- 1.80
peso <- 80
edad <- 32

#Esto se ve bien
altura <- 1.80
peso   <- 80
edad   <- 32
```

4. Utiliza nombres que evoquen la variable que representas
```{r, eval=FALSE}
#Cuando regreses a esto no sabrás ni qué
x <- 10
y <- 2
z <- 3.14
W <- z * x^y #¿Qué calculé?

#Es mejor especificar la variable
radio        <- 10
potencia     <- 2
pi_aprox     <- 3.14
area_circulo <- pi_aprox * radio^potencia

```

5. No utilices un nombre demasiado similar para cosas diferentes. 
```{r}
#Aquí, seguro eventualmente te vas a equivocar
altura <- 10   #Altura del edificio
Altura <- 1.8  #Mi altura
ALTURA <- 2000 #La altitud de la CDMX

#Siempre elegir nombres claros, aunque largos
altura.edificio <- 10   #Altura del edificio
altura.Rodrigo  <- 1.8  #Mi altura
altura.CDMX     <- 2000 #La altitud de la CDMX
```

6. Comenta:
```{r, eval = FALSE}
#¿Qué hace esto?
x <- 168
x <- x/100
y <- 71.2
print(y/x^2) 
  
#Es mejor así
altura <- 168        #en centímetros
altura <- altura/100 #en metros
peso   <- 71.2       #peso en kg
print(peso/altura^2) #índice masa corporal
```

```{r,  fig.cap = "Trad: Un periodista se acerca a un programador a preguntarle ¿qué hace que un código sea malo? -Sin comentarios.", fig.width=3.5, fig.height=3.5, cache=TRUE, echo = FALSE,  message=FALSE}
knitr::include_graphics('images/tweet1.jpg')
```

7. Siempre pon las llamadas a los paquetes y el directorio al inicio de tu archivo para que otro usuario sepa qué necesita.

Código limpio y legible:
```{r, eval = FALSE}
#Asumiendo aquí inicia el archivo:
setwd("Mi directorio")

#Llamamos la librería
library(beepr)
library(tidyverse)

#Analizamos una base de datos de R
data(iris) #Base de datos de flores

#Agrupamos la base por especie
iris.agrupada <- group_by(iris, Species)

#Obtenemos la media por longitud de sépalo
iris.media    <- summarise(iris.agrupada, SL.mean = mean(Sepal.Length))

#Avisa que ya terminó
beep(5)
```
es siempre preferible a código escrito _con prisas_ :

```{r, fig.cap = "Yo, leyendo mi código no comentado y con mala edición 6 meses después de haberlo hecho.", fig.width=1.5, fig.height=1.5, cache=TRUE, echo = FALSE,   message=FALSE}
knitr::include_graphics('images/Grandma-Finds-The-Internet.jpg')
```

```{r, eval = FALSE}
data(iris);setwd("Mi directorio")
library(tidyverse);x<-group_by(iris,Species  )
#Aquí hacemos esto
iris.means=summarise( x,SL.mean=mean(Sepal.Length));library(beepr);beep(5)#FIN
```

Siempre escribe tu código pensando que alguien más ([y ese alguien más puedes ser tú](https://www.redaccionmedica.com/virico/noticias/el-gato-de-schrodinger-y-por-que-no-abrir-la-puerta-cerrada-de-la-consulta-5188)) va a leerlo. ¡No olvides comentar!


# Repaso de Proba




## Funciones indicadoras

Dado un conjunto $A$ definimos la función indicadora de $A$ como sigue:
$$
\mathbb{I}_A (x)= \begin{cases}
1 & \text{ si } x \in A \\
0 & \text{ si } x \not\in A
\end{cases}
$$

La función indicadora cumple las siguientes propiedades:

Sean $A,B$ conjuntos; luego:

1. $\mathbb{I}_{A \cap B}(x) =  \mathbb{I}_{A}(x) \cdot \mathbb{I}_{B}(x)$

2. $\mathbb{I}_{A \cup B}(x) =   \mathbb{I}_{A}(x) +  \mathbb{I}_{B}(x) - \mathbb{I}_{A}(x) \cdot \mathbb{I}_{B}(x)$

3. $\mathbb{E}_X[\mathbb{I}_A(X)] = \mathbb{P}(X\in A)$

_Demostración_:
1. Si $x\in A \cap B$ pasa que $\mathbb{I}_{A \cap B}(x) = 1$; además, por hipótesis $x\in A$ y $x \in B$ lo que implica que $\mathbb{I}_{A}(x) = 1$ y $\mathbb{I}_{B}(x) = 1$; en caso contrario $\mathbb{I}_{A \cap B}(x) = 1$ y como no está en el conjunto al menos uno $\mathbb{I}_{A}(x)$ ó $\mathbb{I}_{B}(x)$ es cero. Esto concluye la prueba.
2. Demostración es similar
3. Para cualquier variable aleatoria $X$, $\mathbb{I}_{A}(X)$ sólo toma dos valores: $0$ si $X\not\in A$ y $1$ si $X\in A$. Luego:

$$
\mathbb{E}_X[\mathbb{I}_A(X)] = 1 \cdot \mathbb{P}(X\in A) + 0 \cdot \mathbb{P}(X\not\in A) = \mathbb{P}(X\in A) 
$$


## Conteo


Intentemos resumir todas las formas de contar que tenemos con un ejemplo de @casella2002statistical. 

> En la lotería de Nueva York se eligen $6$ de $44$ números para un ticket. ¿Cuántos boletos de lotería posibles hay?

Veamos algunas formas posibles de solución^[¿Se te ocurre alguna que no esté aquí?]:

a. **Ordenado y sin reemplazo** Si sólo importa el orden y una vez que sale un número no se vuelve a meter a los posibles entonces tenemos:
$$
\frac{44!}{(44-6)!}
$$

b. **Ordenado y con reemplazo**  En cada uno de los $6$ lugares hay $44$ números posibles:
$$
44^6
$$

c. **Sin orden y sin reemplazo** Esto es una combinación por lo que la forma de extraerlo es:
$$
\binom{44}{6}
$$

d. **Sin orden y con reemplazo** Para resolver este caso podemos usar la técnica de las barras y los puntos. Coloquemos barras y los huecos entre ellas representan cada uno de los $44$ números. 
\begin{equation}\nonumber
|\underbrace{\_}_{1}|\underbrace{\_}_{2}|\underbrace{\_}_{3}|\cdots |\underbrace{\_}_{n}|
\end{equation}
Coloquemos puntos ($\circ$) donde estén los números seleccionados. Por ejemplo la siguiente representa la combinación $113555$
\begin{equation}\nonumber
|\underbrace{\circ \circ}_{1}|\underbrace{\_}_{2}|\underbrace{\circ}_{3}||\underbrace{\_}_{4}|\underbrace{\circ \circ \circ}_{5}|\cdots |\underbrace{\_}_{n}|
\end{equation}
Tenemos entonces que el problema se reduce a colocar $n - 1= 43$ barritas (son un total de $45$ pero la primera y la última no deben cambiar de lugar) y $k = 6$ círculos por tanto colocamos $49$ elementos en total. De estos, nos interesa poner $6$ por lo que tenemos:
$$
\binom{44 + 6  - 1}{6}
$$
formas distintas. Esto nos lleva a la tabla siguiente:

Para obtener una muestra de tamaño $k$ a partir de un conjunto de tamaño $n > 0$ éstas son las opciones: 


<table>
<tr>
<td></td>
<td></td>
<td>$\quad \text{Con Reemplazo}$</td>
<td></td>
<td>$\quad \text{Sin Reemplazo}$</td>
</tr>
<tr>
<td>$\quad \text{Con Orden}$ </td>
<td></td>
<td> $\quad n^k$</td>
<td></td>
<td> $\quad (n)_k$</td>
</tr>
<tr>
<td>$\quad \text{Sin Orden}$ </td>
<td></td>
<td> $\quad \binom{n+k-1}{k}$</td>
<td></td>
<td> $\quad \binom{n}{k}$</td>
</tr>
</table>


## Espacios de probabilidad

Los ingredientes para un modelo probabilístico son $3$:

1. Un conjunto $\Omega$ conocido como **espacio muestral** el cual es el conjunto de los resultados de interés. Por ejemplo, en el tiro de un dado $\Omega = \{1,2,3,4,5,6\}$, para el lanzamiento de una moneda $\Omega = \{\text{Águila},\text{Sol}\}$ o bien en seleccionar un número uniforme entre $0$ y $1$ tenemos que $\Omega = [0,1]$. 

2. Una colección $\mathcal{F}$ de subconjuntos de $\Omega$ conocida como **sigma-álgebra** o bien como **espacio de eventos** la cual cumple las siguientes características: 

  a. $\Omega \in \mathcal{F}$ 
  b. Si $A\in\mathcal{F}$ entonces $A^C \in \mathcal{F}$
  c. Si $A_1, A_2, \dots$ es una colección finita ó numerable de elementos de $\mathcal{F}$ entonces $\bigcup_{n} A_n \in \mathcal{F}$
  
Generalmente identificamos a la $\mathcal{F}$ con el potencia para conjuntos $\Omega$ finitos; para casos infinitos el teorema de Vitali nos dice que las cosas son más complicadas. 
  
3. Una función $\mathbb{P}:\mathcal{F} \to [0,1]$ que cumple que:

  a. $\mathbb{P}(\Omega) = 1$.
  b. $\mathbb{P}(A) \geq 0$ para todo $A \in \mathcal{F}$.
  c. Si $A_1, A_2, \dots$ es una colección finita ó numerable de conjuntos disjuntos ($A_i\cap A_j = \emptyset$ para $i \neq j$) entonces $\mathbb{P}(\bigcup_{n} A_n) = \sum\limits_{n}\mathbb{P}(A_n)$.
  
Estos últimos tres puntos se conocen como **Axiomas de Kolmogorov**. Una vez armados con los axiomas podíamos empezar a probar cosas con ellos; por ejemplo:

Sea $(\Omega,\mathcal{F},\mathbb{P})$ un espacio de probabilidad. Sea $A$ evento de $\mathcal{F}$. Luego:
$$
\mathbb{P}(A^C) = 1 - \mathbb{P}(A).
$$

Para verlo, podemos escribir $\Omega = A\cup A^C$ de donde se sigue que:
$$
1 = \mathbb{P}(\Omega) = \mathbb{P}(A \cup A^C) = \mathbb{P}(A) + \mathbb{P}(A^C);
$$
si despejamos obtenemos el resultado deseado.


También podemos probar, por ejemplo:
$$
\mathbb{P}(A\setminus B) = \mathbb{P}(A) - \mathbb{P}(A \cap B)
$$

si escribimos $A = (A\setminus B) \cup (A \cap B)$ de donde se sigue que:
$$
\mathbb{P}(A) = \mathbb{P}\big((A\setminus B) \cup (A \cap B) \big) =  \mathbb{P}(A\setminus B) + \mathbb{P} (A \cap B)
$$
y despejamos para tener el resultado deseado.


Una última cosa de importancia es tomar $A,B$ eventos de $\mathcal{F}$. Luego:
$$
\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B)
$$


Para verlo, escribimos $A\cup B$ como $A\cup B = (A\setminus B)\cup (A \cap B)\cup (B\setminus A)$ luego:
\begin{equation}\nonumber
\begin{aligned}
\mathbb{P}(A\cup B) & = \mathbb{P}(A\setminus B) + \mathbb{P} (A \cap B) + \mathbb{P}(B\setminus A) \\
& = \mathbb{P}(A) - \mathbb{P}(A \cap B) + \mathbb{P} (A \cap B) + \mathbb{P}(B) - \mathbb{P}(A \cap B)
\\ & = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B)
\end{aligned}
\end{equation}


## Probabilidad condicional 

Muchas veces la probabilidad cambia conforme obtenemos información extra. Por ejemplo, si consideramos los tiros de un dado $\Omega = \{1,2,3,4,5,6\}$ y se sabe que cayó par $B = \{2,4,6 \}$, la probabilidad de obtener $2$ ó $4$ (el evento) $A = \{ 2, 4\}$ cambia de probabilidad:
$$
\mathbb{P}(A | B) = \dfrac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
$$

En particular hay dos teoremas principales con probabilidad condicional: la ley de probabilidad total que te permite reconstruirlas probabilidades originales a partir de las condicionales y el de Bayes. 

El teorema de Bayes puede deducirse a partir de un simple despeje pues notamos que:
$$
\mathbb{P}(A | B) = \dfrac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
$$
y por otro lado:
$$
\mathbb{P}(B | A) = \dfrac{\mathbb{P}(A \cap B)}{\mathbb{P}(A)}
$$
Si despejamos del segundo, obtenemos: 
$$
\mathbb{P}(B | A)\mathbb{P}(A) = \mathbb{P}(A \cap B)
$$
Podemos sustituir la definición de intersección en $\mathbb{P}(A|B)$ y así obtener:
$$
\mathbb{P}(A | B) = \dfrac{\mathbb{P}(B | A)\mathbb{P}(A)}{\mathbb{P}(B)}
$$


Por otro lado, dada una partición $B_1, B_2, \dots$ finita o numerable de $\Omega$ podemos definir la probabilidad de $A$ en términos de cada uno de los pedazos:

$$
\mathbb{P}(A) = \sum\limits_{k} \mathbb{P}(A | B_k) \cdot \mathbb{P}(B_k)
$$

Esta identidad se sigue de que: 
$$
\mathbb{P}(A | B_k) = \dfrac{\mathbb{P}(A \cap B_k)}{\mathbb{P}(B_k)}
$$

de donde podemos sustituir arriba y obtener:
$$
\mathbb{P}(A) = \sum\limits_{k} \dfrac{\mathbb{P}(A \cap B_k)}{\mathbb{P}(B_k)} \cdot \mathbb{P}(B_k) = \sum\limits_{k} \mathbb{P}(A \cap B_k) =  \mathbb{P}\big(A \cap (\bigcup_k B_k) \big) =  \mathbb{P}\big(A \cap \Omega \big) 
$$

Tenemos entonces el teorema siguiente:

Sean $B_1, B_2, \dots $ eventos que forman una partición de $\Omega$; sea $A$ un evento cualquiera; luego:
$$
\mathbb{P}(A) = \sum\limits_{k} \mathbb{P}(A | B_k) \cdot \mathbb{P}(B_k)
$$


Usando probabilidad condicional podemos resolver problemas como el siguiente:


>Considera el conjunto $C = \{1,2,\dots, n\}$ para $n \geq 2$. Se extraen dos números $a$ y $b$ (primero el $a$ y luego el $b$) con probabilidad uniforme sin reemplazo. Determina la probabilidad de que $a > b$. 
 Podemos utilizar probabilidad condicional para representar el evento:
\begin{equation}\nonumber
\begin{aligned}
\mathbb{P}(a > b) &  = \sum\limits_{k = 1}^{n} \mathbb{P}(a > b \quad |  \quad  a = k) \mathbb{P}(a = k)
\end{aligned}
\end{equation}
Donde $\mathbb{P}(a = k) = \frac{1}{n}$ para todos los $k$ pues es uniforme (y es el primero en salir). Luego: 
\begin{equation}\nonumber
\begin{aligned}
\mathbb{P}(a > b) &  = \sum\limits_{k = 1}^{n} \mathbb{P}(a > b \quad |  \quad  a = k) \mathbb{P}(a = k) \\
& = \dfrac{1}{n} \sum\limits_{k = 1}^{n} \mathbb{P}(a > b \quad |  \quad  a = k) \\
& = \dfrac{1}{n} \sum\limits_{k = 1}^{n} \mathbb{P}(k > b \quad |  \quad  a = k) \\
\end{aligned}
\end{equation}
Notamos que cuando $k = 1$ no hay forma de que $k > b$; cuando $k = 2$ hay una única forma (que $b$ valga $1$); cuando $k = 3$ hay dos formas. En general para una $k$ genérica hay $k-1$ formas de seleccionar un $b$ menor a $k$ luego:
\begin{equation}\nonumber
\begin{aligned}
\mathbb{P}(a > b) &  = \dfrac{1}{n} \sum\limits_{k = 1}^{n} \dfrac{k-1}{n}
\\ & = \dfrac{1}{n^2} \sum\limits_{k = 1}^{n} k-1
\\ & = \dfrac{1}{n^2} \sum\limits_{k = 0}^{n-1} k
\\ & = \dfrac{1}{n^2} \dfrac{n(n-1)}{2}
\\ & = \dfrac{n-1}{2n} 
\end{aligned}
\end{equation}



## Independencia

Dos eventos $A,B$ son independientes si:
$$
\mathbb{P}(A \cap B) = \mathbb{P}(A) \mathbb{P}(B)
$$

Intuitivamente esto significa que saber $A$ no me dice nada de $B$ pues la independencia implica que:
$$
\mathbb{P}(A | B) = \mathbb{P}(A)
$$

## Variables aleatorias y función de distribución (acumulada)

Para hablar de probabilidad uno de los ingredientes principales eran las variables aleatorias. Éstas son funciones (**no son variables ni son aleatorias**) de tal manera que su imagen inversa pertenece a la sigma-álgebra $\mathcal{F}$:

Una función $X: \Omega \to \mathbb{R}$ es una variable aleatoria si:
$$
X^{-1}(A) = \{ \omega \in \Omega \quad : \quad X(\omega) \in A \} \in \mathcal{F}
$$
para todo $A\subseteq\textrm{Dom}_X$

En general la pregunta $\mathbb{P}(X \in A)$ la traducíamos a una pregunta sobre conjuntos:
$$
\mathbb{P}(X \in A) = \mathbb{P}\Big( \{ \omega \in \Omega \quad : \quad X(\omega) \in A \} \Big)
$$

y esto nos permitía hablar de probabilidades. En particular, construíamos la función de distribución acumulada como sigue:

> Definimos la función de distribución acumulada de una variable aleatoria $X: \Omega \to \mathbb{R}$ como:
$$
F_X(x) = \mathbb{P}(X \leq x)
$$
donde $X$ es la variable aleatoria y $x\in\mathbb{R}$ es un real. 

La función de distribución acumulada cumplía varias propiedades:

1. $\lim_{x \to \infty} F_X(x) = 1$
2. $\lim_{x \to -\infty} F_X(x) = 0$
3. $F_X$ es no decreciente. 
4. $F_X$ es continua por la derecha.
5. $F_X$ tiene límites por la izquierda. 

Los puntos 4 y 5 se resumen diciendo que la función es _càdlág_. 

Tener la acumulada nos permitía calcular probabilidades de intervalos; por ejemplo:
$$
\mathbb{P}(a < X \leq b) = F_X(b) - F_X(a)
$$
o bien:
$$
\mathbb{P}(X < x) = \lim_{z \to x^-} F(z)
$$

Las funciones de distribución acumulada más comunes se veían como en la imagen:

```{r}
x <- seq(-5,5,length.out = 100)
y <- pnorm(x)
dats <- data.frame(x = x, y = y)
plot1 <- ggplot(dats) + geom_line(aes(x = x, y = y), color = "deepskyblue3", size = 1) + theme_classic() + ggtitle("Distribución acumulada de la Normal(0,1)") +
  xlab("x") + ylab("F_X(x)")
x <- seq(0,5,length.out = 100)
y <- ppois(x, lambda = 1)
dats2 <- data.frame(x = x, y = y)
plot2 <- ggplot(dats2) + geom_step(aes(x = x, y = y), color = "firebrick", size = 1) + theme_classic() + ggtitle("Distribución acumulada de la Poisson(1)") +
  xlab("x") + ylab("F_X(x)")
grid.arrange(plot1,plot2, ncol = 1)
```

Si una función de distribución acumulada $F_X$ era continua entonces decíamos que la variable aleatoria asociada ($X$) es _continua_. En particular, la continuidad implica que:
$$
\mathbb{P}(X = k) = 0 \qquad \forall k 
$$


## Funciones de masa de probabilidad

Si una variable aleatoria $X$ tomaba una cantidad finita o numerable de valores decíamos que $X$ es una variable aleatoria discreta. Dentro de las variables aleatorias discretas teníamos varios modelos. Una cosa importante de las variables aleatorias es la función de masa de probabilidad que se define como:

Dada una variable aleatoria discreta $X$ definimos la función de masa de probabilidad de $X$ como la función $p:\mathbb{R} \to \mathbb{R}$ tal que:
$$
p(x) = \mathbb{P}(X = x)
$$
para todo $x \in\mathbb{R}$.

Algunos modelos importantes son:

Sea $A = \{ a_1, a_2, \dots, a_n \}$ un conjunto finito de $n$ elementos. Una variable aleatoria $X$ tiene una distribución uniforme discreta si:
$$
\mathbb{P}\big( X = a_k \big) = \dfrac{1}{n} \cdot \mathbb{I}_{A}(a_k) \qquad \forall k \in \{ 1, 2, \dots, n \} 
$$


```{r}
x <- 1:10
y <- rep(1/length(x), length(x))
dats <- data.frame(x = x, y = y)
ggplot(dats) + geom_point(aes(x = x, y = y), color = "deepskyblue3", size = 3) + theme_classic() + ggtitle("Función de masa de probabilidad\nuniforme discreta") +
  xlab("x") + ylab("p(x)")
```

Un modelo particular salía de considerar el siguiente problema:

> Tenemos una moneda que cae Águila con probabilidad $p$ y Sol con probabilidad $(1-p)$ (con $0 < p < 1$). Nos interesa saber cuál es la probabilidad de tener $k$ Águilas en $n$ tiros. 
 _Solución_ A fin de resolver este problema notamos que necesitamos acomodar las $k$ águilas en los $n$ tiros para ello hay $\binom{n}{k}$ formas de hacerlo; cada águila cae con probabilidad $p$ y hay $k$; como son independientes esto nos da $p^k$; por otro lado hay $n-k$ soles cada uno cayó con probabilidad $(1-p)$. Esta lógica da origen al modelo binomial: 

Una variable aleatoria $X$ tiene una distribución $\text{Binomial}(n,p)$ si:
$$
\mathbb{P}\big(X = k \big) = \binom{n}{k} p^k (1-p)^{n-k} \mathbb{I}_{\{0,1,2,\dots,n \}}(k)
$$


```{r}
x <- 0:10
y <- dbinom(x, 10, 1/3)
dats <- data.frame(x = x, y = y)
ggplot(dats) + geom_point(aes(x = x, y = y), color = "deepskyblue3", size = 3) + theme_classic() + ggtitle("Función de masa de probabilidad\nBinomial(10,1/3)") +
  xlab("x") + ylab("p(x)")
```

Una pregunta distinta que nos pudimos hacer fue:

> Tenemos una moneda que cae Águila con probabilidad $p$ y Sol con probabilidad $(1-p)$ (con $0 < p < 1$). Arrojamos la moneda hasta obtener $r$ Águilas y en ese momento nos detenemos. Determina la probabilidad de que se aviente la moneda $k$ veces.

Para ello notamos que la última Águila está fija por lo que sólo debemos poner las $r-1$ Águilas en los $k-1$ lugares restantes, $\binom{k-1}{r-1}$. Por otro lado, cada Águila tiene probabilidad $p$ y como son $k$ tiros independientes entonces tenemos $p^r$; para los soles tenemos $(1-p)^{k-r}$. Esto nos genera el modelo Binomial Negativo:

Una variable aleatoria $X$ tiene una distribución $\text{Binomial Negativa}(r,p)$ si:
$$
\mathbb{P}\big(X = k \big) = \binom{k-1}{r-1} p^r (1-p)^{k-r} \mathbb{I}_{\{r, r+1, r+2, \dots \}}(k)
$$


```{r}
x <- 0:100
y <- dnbinom(x, 10, 1/3)
dats <- data.frame(x = x, y = y)
ggplot(dats) + geom_point(aes(x = x, y = y), color = "deepskyblue3", size = 3) + theme_classic() + ggtitle("Función de masa de probabilidad\nBinomialNegativo(10,1/3)") +
  xlab("x") + ylab("p(x)")
```

Finalmente, otro modelo que pudimos hacer con monedas es un caso específico del _Binomial Negativo_ . Aquí la pregunta es, se tira una moneda que tiene probabilidad $p$ de salir Águila hasta que se obtiene el águila. Contamos cuántos tiros ocurrieron hasta que ocurriera el primer Águila y la pregunta de interés es la probabilidad de haber realizado específicamente $k$ tiros. Para ello necesitamos tener $(k-1)$ tiros que fueran sol: $(1-p)^{k-1}$ y un tiro que saliera águila $p$. Esto nos genera el modelo geométrico:

Una variable aleatoria $X$ tiene una distribución $\text{Geométrica}(p)$ si:
$$
\mathbb{P}\big(X = k \big) = (1-p)^k p \cdot \mathbb{I}_{\mathbb{N}}(k).
$$


```{r}
x <- 0:25
y <- dgeom(x, 1/3)
dats <- data.frame(x = x, y = y)
ggplot(dats) + geom_point(aes(x = x, y = y), color = "deepskyblue3", size = 3) + theme_classic() + ggtitle("Función de masa de probabilidad\nGeométrica(1/3)") +
  xlab("x") + ylab("p(x)")
```

Otro modelo de interés es el siguiente: 

> Se tiene una población de tamaño $M$ donde $N$ individuos pertenecen al partido político AZUL y $M-N$ pertenecen al VERDE Se toma una submuestra de tamaño $m$. Determina la probabilidad de que haya $n$ individuos del partido político AZUL. 

Para ello notamos que hay $\binom{M}{m}$ muestras totales. Por otro lado, necesitamos extraer de los $N$ azules a una submuestra de $n$: $\binom{N}{n}$; finalmente, de los $M$ verdes necesitamos extraer una submuestra de $m$, hay $\binom{M-N}{m-n}$ formas de hacerlo. Concluimos entonces con el modelo hipergeométrico:

Una variable aleatoria $X$ tiene una distribución $\text{Hipergeométrica}(M,N,m)$ si:
$$
\mathbb{P}\big(X = n \big) = \dfrac{\binom{M-N}{m-n} \binom{N}{n} }{\binom{M}{m}} \cdot \mathbb{I}_{\{0,1,\dots, \text{mín}\{m, N\} \}} (n)
$$


```{r}
x <- 0:50
y <- dhyper(x, 100, 150, 50)
dats <- data.frame(x = x, y = y)
ggplot(dats) + geom_point(aes(x = x, y = y), color = "deepskyblue3", size = 3) + theme_classic() + ggtitle("Función de masa de probabilidad\nHipergeométrica(250, 150, 50)") +
  xlab("x") + ylab("p(x)")
```

El modelo Poisson va a ser bastante útil. Para estudiarlo, consideremos un modelo. Vamos a pensar en un servidor de computación (piensa en una página de Internet) que recibe solicitudes de entrar a la página de manera independiente y aleatoria en un intervalo de tiempo entre $t = 0$ y $t = 1$.  Como primera aproximación podemos dividir el intervalo en $n$ pedazos cada uno de longitud $1/n$ y asumir que, a fuerza, sólo una conexión se puede realizar en cada uno de esos pedazos. Finalmente, asumamos que la probabilidad $p$ de que se haga una conexión es proporcional a la longitud del intervalo y sea $p = \lambda / n$. Con estas hipótesis, la probabilidad de tener $k$ conexiones ($k$ entero entre $0$ y $n$) está dada por un modelo binomial:
\begin{equation}\nonumber
\begin{aligned}
f_n(k) & = \binom{n}{k} \Big( \frac{\lambda}{n} \Big)^k \Big(1 - \frac{\lambda}{n} \Big)^k \
& = \frac{\lambda^k}{k!} \Big( 1 - \frac{\lambda}{n})^n \frac{n!}{n^k(n-k)!} \Big( 1 - \frac{\lambda}{n})^{-k}
\end{aligned}
\end{equation}
de donde concluimos que si continuamos partiendo el intervalo en pedazos cada vez más pequeños obtenemos:
\begin{equation}\nonumber
\begin{aligned}
\lim_{n \to \infty} f_n(k) & = \frac{e^{-\lambda} \lambda^k}{k!}
\end{aligned}
\end{equation}
Esto resulta en el modelo Poisson: 

Una variable aleatoria $X$ tiene una distribución $\text{Poisson}(\lambda)$ si:
$$
\mathbb{P}\big(X = k \big) = \dfrac{\lambda^k e^{-\lambda}}{k!} \mathbb{I}_{\mathbb{N}\cup \{ 0 \}}(k)
$$


```{r}
x <- 0:15
y <- dpois(x, 4)
dats <- data.frame(x = x, y = y)
ggplot(dats) + geom_point(aes(x = x, y = y), color = "deepskyblue3", size = 3) + theme_classic() + ggtitle("Función de masa de probabilidad\nPoisson(4)") +
  xlab("x") + ylab("p(x)")
```


## Funciones de densidad

Por construcción, las variables aleatorias continuas no tienen una función de masa de probabilidad (recuerda que $\mathbb{P}(X = k) = 0$ si $X$ es continua para todo $k$). Sin embargo, es posible definir, si $F_X$ es diferenciable algo _similar_, la función de densidad.

Para una variable aleatoria $X$ con función de distribución acumulada $F_X$ diferenciable, definimos la función de densidad como:
$$
f_X(x) = \dfrac{d}{dx} F_X(x)
$$

Notamos que una función de densidad no es una probabilidad y no necesariamente sigue las mismas reglas; lo único que se requiere es: 

1. $f_X(x) \geq 0$ para toda $x$.
2. $\int\limits_{-\infty}^{\infty} f_X(x) dx = 1$. 

La primer función de densidad es la que a un intervalo $[a,b]$ (ya sea abierto, cerrado o como sea) asigna a cada subintervalo una probabilidad proporcional a su longitud. Éste es el modelo uniforme:

Una variable aleatoria $X$ tiene una distribución $\text{Uniforme}(a,b)$ si:
$$
f_X(x) = \dfrac{1}{b-a}\mathbb{I}_{(a,b)}(x)
$$


```{r}
x <- seq(-2,2, length.out = 100)
y <- dunif(x, -1, 1)
dats <- data.frame(x = x, y = y)
ggplot(dats) + geom_step(aes(x = x, y = y), color = "deepskyblue3", size = 1) + theme_classic() + ggtitle("Función de densidad\nUniforme(-1,1)") +
  xlab("x") + ylab("f_X(x)")
```

Una generalización del modelo uniforme es el beta (eventualmente veremos de dónde sale):

Una variable aleatoria $X$ tiene una distribución $\text{Beta}(\alpha,\beta)$ si:
$$
f_X(x) = \dfrac{x^{\alpha - 1}(1-x)^{\beta - 1}}{B(\alpha, \beta)}\mathbb{I}_{(0,1)}(x)
$$
donde 
$$
B(\alpha, \beta) = \dfrac{\Gamma (\alpha) \Gamma (\beta)}{\Gamma (\alpha + \beta)}
$$


```{r}
x  <- seq(0,1, length.out = 100)
y1 <- dbeta(x, 2, 4)
y2 <- dbeta(x, 4, 2)
y3 <- dbeta(x, 2, 2)
dats <- data.frame(x = x, y1 = y1, y2 = y2, y3 = y3)
ggplot(dats) + 
  geom_line(aes(x = x, y = y1, color = "a = 2; b = 4"), size = 1) + 
  geom_line(aes(x = x, y = y2, color = "a = 4; b = 2"), size = 1) + 
  geom_line(aes(x = x, y = y3, color = "a = 2; b = 2"), size = 1) + 
  theme_classic() + ggtitle("Función de densidad\nBeta(a,b)") +
  xlab("x") + ylab("f_X(x)") + 
  scale_color_manual("Parámetros", values = c("deepskyblue3","firebrick","cyan"))
```

Podemos deducir el modelo exponencial a partir de la descripción del Poisson. Volvamos al mismo problema del $\textrm{Poisson}(\lambda)$ donde hay computadoras conectándose a un servidor. Sea $W$ la variable aleatoria que denota el tiempo de espera hasta el primer evento. Analicemos su distribución acumulada; notamos que 
$$
F_W(w) = \mathbb{P}(W \leq w) = 1 - \mathbb{P}(W > w)
$$
Ahora, para que $W > w$ eso significa que ningún evento tuvo que haber ocurrido en los primeros $w$ minutos (horas, lo que sea la unidad de tiempo). Y ese evento es equivalente a que nuestra variable aleatoria Poisson (tasa $\lambda w$)^[Recuerda que $\lambda$ era para un tiempo entre $0$ y $1$; $\lambda w$ es para un escalamiento del tiempo entre $0$ y $w$.] no tenga ningún arribo:
$$
\mathbb{P}(X = 0) = \dfrac{(\lambda w)^0 e^{-\lambda w}}{0!} = e^{-\lambda}
$$
De donde se obtiene la función de distribución acumulada:
$$
F_W(w) = 1 - e^{-\lambda w}
$$
De donde, al derivar respecto a $w$, se obtiene el modelo  exponencial:

Una variable aleatoria $X$ tiene una distribución $\text{Exponencial}(\lambda)$ si:
$$
f_X(x) = \lambda e^{-\lambda x} \mathbb{I}_{(0,\infty)}(x)
$$


Para deducir la distribución gamma, vamos a preguntarnos por exactamente el mismo proceso pero esta vez, en lugar de preguntarnos por el tiempo para la primer conexión nos preguntaremos por el tiempo para la $\alpha$-ésima conexión. Para ello, sea $W_{\alpha}$ el tiempo hasta la $\alpha$-ésima conexión. Usamos el mismo truco del complemento que la vez pasada: 
$$
F_{W_{\alpha}}(w) = \mathbb{P}(W_{\alpha} \leq w) = 1 - \mathbb{P}(W_{\alpha} > w)
$$
Y notamos que para que $W_{\alpha} > w$ entonces a lo más debieron haber $\alpha-1$ conexiones. Podemos reescribir:
$$
F_{W_{\alpha}}(w) =  1 - \mathbb{P}(W_{\alpha} > w) = 1 - \sum\limits_{k = 0}^{\alpha - 1} \dfrac{(\lambda w)^k e^{-\lambda w}}{k!} = 1 - e^{- \lambda w} - \sum\limits_{k = 1}^{\alpha - 1} \dfrac{(\lambda w)^k e^{-\lambda w}}{k!}
$$
Derivamos:
\begin{equation}\nonumber
\begin{aligned}
\dfrac{d}{dw}F_{W_{\alpha}}(w) & =  -\lambda e^{- \lambda w} - \sum\limits_{k = 1}^{\alpha - 1} \dfrac{k \lambda (\lambda w)^{k-1} e^{-\lambda w} - \lambda (\lambda w)^k e^{-\lambda w}}{k!} \
 & =  -\lambda e^{- \lambda w} - \lambda e^{- \lambda w} \sum\limits_{k = 1}^{\alpha - 1} \underbrace{\dfrac{(\lambda w)^{k-1}}{(k-1)!}  - \dfrac{(\lambda w)^k }{k!}}_{\text{Telescópica}} \
 & = -\lambda e^{- \lambda w} + \lambda e^{- \lambda w} \Bigg( \dfrac{(\lambda w)^{\alpha - 1} }{(\alpha - 1)!} - 1 \Bigg) \ 
 & = \lambda e^{- \lambda w} \dfrac{(\lambda w)^{\alpha - 1} }{(\alpha - 1)!} \
 & =  \dfrac{\beta^{\alpha} }{\Gamma (\alpha)} w^{\alpha - 1} e^{- \frac{w}{\beta}}  \
 \end{aligned}
\end{equation}
donde tomamos $\beta = \frac{1}{\lambda}$. Esto sugiere el modelo gamma:

Una variable aleatoria $W$ tiene una distribución $\text{Gamma}(\alpha,\beta)$ si:
$$
f_W(w) =  \dfrac{\beta^{\alpha} }{\Gamma (\alpha)} w^{\alpha - 1} e^{- \frac{w}{\beta}} \mathbb{I}_{(0,\infty)}
$$
para $\alpha,\beta > 0$.

Para deducir el modelo normal consideremos lo siguiente. Pensemos que estamos midiendo la posición de las estrellas en el cielo. Para ello hay dos formas. Bajo coordenadas cartesianas $(x,y)$ pensemos que el error de medición es independiente; es decir, si $f(x,y)$ es la densidad de los errores entonces:

$$
\rho (x,y) = f(x) f(y) 
$$

Por otro lado, asumamos que existe también una representación en coordenadas polares de la posición de la estrella: 
$$
g (r, \theta)  = g(r) 
$$
donde el error de medición depende sólo del radio (no del ángulo). Notamos entonces que:
$$
f(x) f(y) = g\Big( \sqrt{x^2 + y^2} \Big) 
$$
Si tomamos $y = 0$ tenemos que $f(x) f(0) = g(x)$ (asumo $x > 0$; los otros casos son similares). Podemos entonces sustituir:

$$
\dfrac{f(x) f(y)}{f(0)^2} = \dfrac{f\Big( \sqrt{x^2 + y^2} \Big) }{f(0)}
$$

Tomamos logaritmo:
$$
\ln \dfrac{f(x)}{f(0)} + \ln \dfrac{f(y)}{f(0)}  = \ln \dfrac{f\Big( \sqrt{x^2 + y^2} \Big) }{f(0)}
$$
Notamos que una solución es que:
$$
\ln \dfrac{f(x)}{f(0)} = \alpha x^2
$$
de donde despejamos y obtenemos:
$$
f(x) = \frac{1}{f(0)} e^{\alpha x^2}
$$
Finalmente sabemos que debe integrar a $1$ y por tanto esto fuerza a $\alpha$ a ser negativo. En particular tomaremos $\alpha = -\frac{1}{2}$ 
$$
f(x) = \frac{1}{f(0)} e^{-\frac{1}{2} x^2}
$$
Y para que integre a $1$:s
$$
f(x) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2} x^2}
$$

Por último, notamos que si $Z\sim \textrm{Normal}(0,1)$ entonces $X = \sigma Z + \mu$ tiene la densidad dada por^[Por teorema de cambio de variable.]:
$$
f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{1}{2\sigma^2} (x - \mu)^2}
$$

Una variable aleatoria $X$ tiene una distribución $\text{Normal}(\mu,\sigma)$ si:
$$
f_X(x) =  \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(x - \mu)^2}{2 \sigma^2}} 
$$


## Teorema de cambio de variable unidimensional

Supongamos que tenemos una variable aleatoria $X$ y nos interesa ver cómo se ve la $X$ después de aplicarle una función $\phi$. Por ejemplo, si $X\sim\textrm{Normal}(0,1)$ la función de densidad de $e^X$ está dada por:

$$
f_X(x) = \dfrac{1}{x \sqrt{2 \pi \sigma^2}}e^{-(\ln(x) - \mu)^2/2\sigma^2} \mathbb{I}_{(0,\infty)}(x).
$$

Lo cual cambia mucho la forma de la distribución:

```{r}
x <- seq(0,4, length.out = 1000)
y <- dlnorm(x, 0, 1)
dats <- data.frame(x = x, y = y)
ggplot(dats) + geom_line(aes(x = x, y = y), color = "deepskyblue3", size = 1) + theme_classic() + ggtitle("Función de densidad\nLognormal(0,1)") +
  xlab("x") + ylab("f_X(x)")
```

La pregunta es, cómo obtener la función de densidad de $X$ si se conoce la función $\phi$; el teorema de cambio de variable nos da una respuesta cuando $\phi$ es monótona estrictamente creciente o bien estrictamente decreciente y diferenciable.

Sea $X$ una variable aleatoria continua y $\phi$ una función estrictamente creciente ó estrictamente decreciente y diferenciable. Entonces:
$$
f_{\phi(X)}(t)  = f_X( \phi^{-1}(t) ) \cdot \left| \dfrac{d}{dt}  \phi^{-1}(t)  \right|
$$

**DEM: Caso estrictamente decreciente**
Como $\phi$ es estrictamente decreciente es invertible y por tanto:
\begin{equation}\nonumber
\begin{aligned}
F_{\phi(X)}(t) & =  \mathbb{P}(\phi(X) \leq t) \\
& = \mathbb{P}(X \geq \phi^{-1}(t) ) \\
& = 1 - \mathbb{P}(X \leq \phi^{-1}(t) ) \\
& = 1 - F_X( \phi^{-1}(t) )
\end{aligned}
\end{equation}
luego derivamos respecto a $t$:
\begin{equation}\nonumber
\begin{aligned}
f_{\phi(X)}(t) & = \dfrac{d}{dt} F_{\phi(X)}(t) \\
& = - \dfrac{d}{dt} F_X( \phi^{-1}(t) )  \\
& = - f_X( \phi^{-1}(t) ) \cdot \dfrac{d}{dt} \phi^{-1}(t)  \\
& = f_X( \phi^{-1}(t) ) \cdot \left| \dfrac{d}{dt}  \phi^{-1}(t)  \right|
\end{aligned}
\end{equation}
Donde el valor absoluto sale de que $\phi^{-1}(t) < 0$ por ser estrictamente decreciente la $\phi$. 

## Probabilidad Multivariada

De la misma manera que hablamos de una sola variable aleatoria podemos hablar de muchas como múltiples funciones de $\Omega \in \mathbb{R}$. Para una colección finita $\{ X_i \}_{i = 1}^n$ de variables aleatorias podemos hablar de su función de distribución acumulada conjunta como:
$$
F_{\vec{X}}(x_1, x_2, \dots, x_n) = \mathbb{P}\big( X_1 \leq x_1, X_2 \leq x_2, \dots, X_n \leq x_n)
$$
donde suponemos que $\vec{X} = (X_1, X_2, \dots, X_n)^T$ es un vector aleatorio cuyas entradas son las variables de la colección anterior. En el caso de que las $n$ variables sean discretas la función de masa conjunta está dada por:
$$
p_{\vec{X}}(x_1, x_2, \dots, x_n) = \mathbb{P}(X_1 = x_1, X_2 = x_2, \dots, X_n = x_n)
$$
En el caso de que sean continuas ($F_{\vec{X}}$ diferenciable en sus $n$ entradas) entonces la densidad está dada por:
$$
f_{\vec{X}}(x_1, x_2, \dots, x_n) = \dfrac{\partial^n}{\partial x_1 \partial x_2 \dots \partial x_n}F_{\vec{X}}\Bigg|_{(x_1, x_2, \dots, x_n)}
$$
En general la función de probabilidad conjunta siempre va a esta dada por:
$$
\mathbb{P}(X_1 \in A_1, X_2 \in A_2, \dots, X_n \in A_n) = \mathbb{P}\Big(\{ \omega \in \Omega :  X_1(\omega) \in A_1 \text{ y } X_2(\omega) \in A_2  \text{ y } \dots \text{ y } X_n(\omega) \in A_n \}\Big)
$$

para $A_1, A_2, \dots, A_n$ medibles (bajo $X_1, X_2, \dots, X_n$ respectivamente). 







Dos variables aleatorias $X_i$ y $X_j$ ($i \neq j$) son independientes si:
$$
\mathbb{P}(X_i \in A, X_i \in B) = \mathbb{P}(X_i \in A) \cdot \mathbb{P}(X_j \in B)
$$
para $A,B$ medibles. Una colección $\{ X_i \}_{i}$ de variables aleatorias es **completamente independiente** si para cualquier subcolección finita  $\{ X_{i_k} \}_{i_k}$ se tiene que:
$$
\mathbb{P}(X_{i_1} \in A_{i_1}, X_{i_2} \in A_{i_2}, \dots, X_{i_n} \in A_{i_n} ) = \prod_{k = 1}^n \mathbb{P}(X_{i_k} \in A) 
$$
en el contexto de estas notas, a menos que se indique lo contrario, las variables aleatorias que utilicemos serán **completamente independientes**. 

Un aspecto interesante de la independencia es que permite partir las funciones de masa, densidad y distribución acumulada en dos funciones independientes. Así, si $X,Y$ son independientes con masa conjunta $p$:
$$
p_{X,Y}(x,y) = \mathbb{P}(X = x, Y = y) = \mathbb{P}(X = x)\cdot\mathbb{P}(Y = y) = p_X(x)\cdot p_Y(y)
$$
El resultado se mantiene para distribuciones:
$$
F_{X,Y}(x,y) = \mathbb{P}(X \leq x, Y \leq y) = \mathbb{P}(X \leq x)\cdot\mathbb{P}(Y \leq y) = F_X(x)\cdot F_Y(y)
$$
y si derivamos (en caso de $F$ diferenciable), se mantiene para densidades:

$$
f_{X,Y}(x,y) = \dfrac{\partial^2}{\partial x\partial y} F_{X,Y}\Big|_{(x,y)} = \dfrac{\partial^2}{\partial x\partial y} F_X(x)\cdot F_Y(y)\Big|_{(x,y)} = f_X(x) f_Y(y)
$$




## Esperanza, varianza y covarianza
Para una función medible $g$ de una variable aleatoria $X$ definimos su valor esperado (si existe) como:
$$
\mathbb{E}\big[g(X)\big] = \begin{cases}
\sum\limits_{x \in \text{Supp}(X)} g(x) \cdot \mathbb{P}(X  = x) & \text{ si } X \text{ discreta.} \\
\int\limits_{-\infty}^{\infty} g(x) \cdot f_X(x) dx& \text{ si } X \text{ continua}
\end{cases}
$$
donde $f_X$ es la densidad de $X$ en el caso continuo y $\text{Supp}(X)$ es el conjunto imagen de $X$ (el soporte):
$$
\text{Supp}(X) = \{ x : X(\omega) = x \text{ para } \omega \in \Omega \}
$$
En el caso de conjuntos finitos de variables aleatorias la definción es similar:

Para una función $g:\mathbb{R}^n \to \mathbb{R}$ multivariada de $n$ variables aleatorias (sobre los reales) $X_1, X_2, \dots, X_n$ definimos su valor esperado (si existe y sin pérdida de generalidad suponiendo las primeras $j$ son discretas y las últimas $n - (j + 1)$ continuas) como:
\tiny
\begin{equation}\nonumber
\begin{aligned}
\mathbb{E}\big[  g(X_1, X_2, \dots, X_n) \big]  = 
\\ &   \int_{-\infty}^{\infty}  \dots  \int_{-\infty}^{\infty} \sum_{x_j \in \text{Supp}(X_j)}  \dots \sum_{x_1 \in \text{Supp}(X_1)} g(x_1, x_2, \dots, x_n) p(x_1)  \dots p(x_j) f_{X_{j+1}}(x_{j+1}) \dots  f_{X_{n}}(x_{n}) dx_{j+1} \dots  dx_{n}
\end{aligned}
\end{equation}

donde $p(x_j)$ es la masa de $X_j$ (es decir $p(x_j) = \mathbb{P}(X_j = x_j)$. En el caso particular de dos variables aleatorias $X_1$ y $X_2$ podemos escribir la expresión de manera más sencilla:


\begin{equation}\nonumber
\begin{aligned}
\mathbb{E}\big[ & g(X_1, X_2) \big]  = 
\begin{cases}
\int\limits_{-\infty}^{\infty} \int_{-\infty}^{\infty}  g(x_1, x_2) f_{X_1}(x_1) f_{X_2}(x_2) dx_1 dx_2  & \text{ ambas continuas,} \\ \\
\sum\limits_{x \in \text{Supp}(X_1)} \sum\limits_{x \in \text{Supp}(X_2)}   g(x_1, x_2) p(x_1) p(x_2)  & \text{ ambas discretas,} \\ \\
\int_{-\infty}^{\infty} \sum\limits_{x \in \text{Supp}(X_1)}    g(x_1, x_2) p(x_1) p(x_2)  & X_1 \text{ discreta, } X_2 \text{ continua.}  \\
\end{cases}
\end{aligned}
\end{equation}

En particular, en el espacio de las variables aleatorias definimos un producto interno, **la covarianza** la cual está dada por:
$$
\textrm{Cov}(X_1, X_2) = \mathbb{E}\Big[ \big(X_1 - \mathbb{E}[X_1]\big) \cdot \big(X_2 - \mathbb{E}[X_2]\big) \Big]
$$
La **varianza** es un caso particular de la covarianza: cuando $X_1 = X_2$:

$$
\textrm{Cov}(X_1, X_1) = \mathbb{E}\Big[ \big(X_1 - \mathbb{E}[X_1]\big)^2 \Big]
$$

### Propiedades de valor esperado, varianza y covarianza
El valor esperado al ser representable mediante sumas ó integrales cumple todas las propiedades de las sumas (resp integrales) en particular la linealidad:
$$
\mathbb{E}\Big[ a X + Y\Big] = a \mathbb{E}[X] + \mathbb{E}[Y]
$$
La demostración se hace exactamente igual en el caso de variables discretas, continuas (ó mezcla de una y una). Aquí muestro la de continuas con densidades $f_X$ y $f_Y$:

\begin{equation}
\begin{aligned}
\mathbb{E}\Big[ a X + Y\Big] & = \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} (a x + y) f_{X,Y}(x,y) dx dy \\
& = a \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} x  f_{X,Y}(x,y)  dx dy +  \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} y f_{X,Y}(x,y)  dx dy \\
& = a \Big[ \int\limits_{-\infty}^{\infty} x  f_X(x) dx \Big] +  \int\limits_{-\infty}^{\infty} y f_Y(y) dy \\
& =  a \mathbb{E}[X] + \mathbb{E}[Y]
\end{aligned}
\end{equation}

Otro resultado importante es que si dos variables aleatorias $X,Y$ son independientes entonces el valor esperado del producto se parte:
$$
\mathbb{E}[XY] = \mathbb{E}[X] \cdot \mathbb{E}[Y]
$$
La demostración se hace de manera idéntica en todos los casos. Aquí mostramos el caso de $X,Y$ discretas:

\begin{equation}
\begin{aligned}
\mathbb{E}\Big[XY\Big] & = \sum\limits_{y \in \text{Sup}(Y)} \sum\limits_{x \in \text{Sup}(X)} xy \mathbb{P}(X = x, Y = y) 
\\ & = \sum\limits_{y \in \text{Sup}(Y)} \sum\limits_{x \in \text{Sup}(X)} xy \mathbb{P}(X = x) \mathbb{P}(Y = y) 
\\ & = \Big[\sum\limits_{y \in \text{Sup}(Y)} y  \mathbb{P}(Y = y)\Big]  \Big[\sum\limits_{x \in \text{Sup}(X)} x \mathbb{P}(X = x)\Big]
\\ & = \mathbb{E}[X] \cdot \mathbb{E}[Y]
\end{aligned}
\end{equation}

La linealidad nos permite reescribir la covarianza:
\begin{equation}
\begin{aligned}
\textrm{Cov}(X_1, X_2) & = \mathbb{E}\Big[ \big(X_1 - \mathbb{E}[X_1]\big) \cdot \big(X_2 - \mathbb{E}[X_2]\big) \Big] \\
& =  \mathbb{E}\Big[ X_1 X_2 \Big]  - \mathbb{E}\Big[X_1\Big]\mathbb{E}\Big[X_2\Big]   - \mathbb{E}\Big[X_1\Big]\mathbb{E}\Big[X_2\Big]  +  \mathbb{E}\Big[X_1\Big]\mathbb{E}\Big[X_2\Big] 
\\ & = \mathbb{E}\Big[ X_1 X_2 \Big] - \mathbb{E}\Big[X_1\Big]\mathbb{E}\Big[X_2\Big] 
\end{aligned}
\end{equation}

de tal forma que es claro que si $X_1$ y $X_2$ son independientes entonces $\textrm{Cov}(X_1, X_2) = 0$ por la propiedad anterior del valor esperado. **OJO** De manera general covarianza $0$ no implica que las variables sean independientes como puede verse con las variables aleatorias siguientes:
$$
f_{X,Y}(x,y) = \begin{cases}
1/8 & \text{ si } (x,y) \in \{ (-1,-1), (-1,1), (1, -1), (1,1)\} \\
1/2 & \text{ si } (x,y)  = (0,0), \\
0 & \text{ en otro caso}
\end{cases}
$$
las cuales _no son independientes_ pues $\mathbb{P}(X = 0, Y = 0) = 1/2\neq 1/4 = \mathbb{P}(X = 0)\cdot \mathbb{P}(Y = 0)$; sin embargo (_ejercicio sugerido_) la covarianza es $0$. 

Una segunda propiedad de interés de la covarianza es que actúa como el producto interno (de hecho es uno):
$$
\text{Cov}(a X + bY, cW + dV) = ac \text{Cov}(X,W) + ad \text{Cov}(X,V) + bc \text{Cov}(Y,W) + bd \text{Cov}(Y,V) 
$$
la cual se demuestra igual mediante la linealidad:
\begin{equation}
\begin{aligned}
\textrm{Cov}(a & X + bY, cW + dV)  = \mathbb{E}\Big[ (a X + bY) (cW + dV) \Big] - \mathbb{E}\Big[a X + bY\Big]\mathbb{E}\Big[cW + dV\Big] 
\\ & = \mathbb{E}\Big[ ac XW + bc YW + ad XV + bd YV\Big] - \bigg( a \mathbb{E}\Big[ X \Big] + b\mathbb{E}\Big[ Y\Big]\bigg)\bigg( c\mathbb{E}\Big[W\Big] + d\mathbb{E}\Big[V\Big] \bigg)
\\ & =   ac \text{Cov}(X,W) + ad \text{Cov}(X,V) + bc \text{Cov}(Y,W) + bd \text{Cov}(Y,V) 
\end{aligned}
\end{equation}
donde la última igualdad se sigue de agrupar los términos idénticos tras sus constantes.


## Condicionamiento por otra variable aleatoria
A rellenarse pronto

## Funciones características
A rellenarse pronto

## Convergencias
A rellenarse pronto

### Teorema de continuidad de Lévy
A rellenarse pronto

## Ley de los grandes números
A rellenarse pronto

## Teorema del límite central
A rellenarse pronto




